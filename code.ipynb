{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a46d8e",
   "metadata": {},
   "source": [
    "\n",
    "# Co-Teaching on FashionMNIST (0.3 & 0.6) — **Fixed Loader v2**\n",
    "This version fixes the `Unexpected image shape (N, 784)` error by **automatically reshaping flattened images** to `(N, 1, 28, 28)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f20ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Working dir: /Users/wukris/5328\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, random, time\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Working dir:\", Path.cwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1271fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    dataset_path: str\n",
    "    n_classes: int = 3\n",
    "    batch_size: int = 256\n",
    "    epochs: int = 25\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 5e-4\n",
    "    num_workers: int = 0     # safer default on macOS\n",
    "    noise_rate: float = 0.3\n",
    "    num_gradual: int = 10\n",
    "    exponent: float = 1.0\n",
    "    n_trials: int = 10\n",
    "    out_dir: str = \"./results_coteaching\"\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7aa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_chw4(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Convert images to (N,C,H,W). Supports:\n",
    "    - (N, H, W)  -> (N, 1, H, W)\n",
    "    - (N, 784)   -> (N, 1, 28, 28)   # flattened FashionMNIST\n",
    "    - (N, 3072)  -> (N, 3, 32, 32)   # flattened CIFAR-style (not used here but safe)\n",
    "    - (N, C, H, W) with C in {1,3} -> return as-is\n",
    "    \"\"\"\n",
    "    if x.ndim == 4 and x.shape[1] in (1, 3):\n",
    "        return x\n",
    "    if x.ndim == 3:\n",
    "        # assume grayscale (N,H,W)\n",
    "        return x[:, None, :, :]\n",
    "    if x.ndim == 2:\n",
    "        N, D = x.shape\n",
    "        if D == 28*28:\n",
    "            return x.reshape(N, 1, 28, 28)\n",
    "        if D == 32*32*3:\n",
    "            return x.reshape(N, 3, 32, 32)\n",
    "        # fallback: try square grayscale\n",
    "        s = int(round(D ** 0.5))\n",
    "        if s*s == D:\n",
    "            return x.reshape(N, 1, s, s)\n",
    "    raise ValueError(f\"Unexpected image shape {x.shape}\")\n",
    "\n",
    "def load_npz_dataset(path: str):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {path.resolve()}\" )\n",
    "    data = np.load(path)\n",
    "    Xtr, Str = data['Xtr'], data['Str']\n",
    "    Xts, Yts = data['Xts'], data['Yts']\n",
    "\n",
    "    # float + scaling if needed\n",
    "    Xtr = Xtr.astype(np.float32); Xts = Xts.astype(np.float32)\n",
    "    if Xtr.max() > 1.0 or Xts.max() > 1.0:\n",
    "        Xtr /= 255.0; Xts /= 255.0\n",
    "\n",
    "    Xtr = _to_chw4(Xtr)\n",
    "    Xts = _to_chw4(Xts)\n",
    "\n",
    "    return Xtr, Str.astype(np.int64), Xts, Yts.astype(np.int64)\n",
    "\n",
    "def split_train_val(X, y, val_ratio=0.2, seed=0):\n",
    "    n = len(y); idx = np.arange(n)\n",
    "    rng = np.random.default_rng(seed); rng.shuffle(idx)\n",
    "    n_val = int(n * val_ratio)\n",
    "    return idx[n_val:], idx[:n_val]\n",
    "\n",
    "def make_loaders(Xtr, Str, Xts, Yts, tr_idx, val_idx, batch_size=256, num_workers=0):\n",
    "    Xtr_t = torch.from_numpy(Xtr[tr_idx]); Str_t = torch.from_numpy(Str[tr_idx])\n",
    "    Xval_t = torch.from_numpy(Xtr[val_idx]); Sval_t = torch.from_numpy(Str[val_idx])\n",
    "    Xts_t  = torch.from_numpy(Xts);          Yts_t  = torch.from_numpy(Yts)\n",
    "\n",
    "    tr_ds = TensorDataset(Xtr_t, Str_t)\n",
    "    val_ds= TensorDataset(Xval_t, Sval_t)\n",
    "    ts_ds = TensorDataset(Xts_t, Yts_t)\n",
    "\n",
    "    tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True,  drop_last=True,  num_workers=num_workers)\n",
    "    val_loader= DataLoader(val_ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers)\n",
    "    ts_loader = DataLoader(ts_ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=num_workers)\n",
    "    return tr_loader, val_loader, ts_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db573898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_dataset(path):\n",
    "    Xtr, Str, Xts, Yts = load_npz_dataset(path)\n",
    "    print(\"Train X shape:\", Xtr.shape, \"| noisy y:\", Str.shape)\n",
    "    print(\"Test  X shape:\", Xts.shape, \"| clean y:\", Yts.shape)\n",
    "    print(\"Train X stats: min\", float(Xtr.min()), \"max\", float(Xtr.max()), \"mean\", float(Xtr.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a62110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.AdaptiveAvgPool2d((7,7))\n",
    "        self.fc1 = nn.Linear(128*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = torch.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98189e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coteaching_schedule(epoch, noise_rate=0.3, num_gradual=10, exponent=1.0):\n",
    "    t = min(epoch / max(1, num_gradual), 1.0)\n",
    "    forget_rate = noise_rate * (t ** exponent)\n",
    "    return forget_rate, 1.0 - forget_rate\n",
    "\n",
    "def select_small_loss_idx(losses, remember_rate):\n",
    "    B = losses.shape[0]\n",
    "    k = max(int(remember_rate * B), 1)\n",
    "    _, idx = torch.topk(losses, k=k, largest=False)\n",
    "    return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval(); correct = 0; total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb).argmax(1)\n",
    "        correct += (pred==yb).sum().item(); total += yb.size(0)\n",
    "    return correct / max(total,1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ensemble(m1, m2, loader, device):\n",
    "    m1.eval(); m2.eval(); correct = 0; total = 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = (m1(xb)+m2(xb)).argmax(1)\n",
    "        correct += (pred==yb).sum().item(); total += yb.size(0)\n",
    "    return correct / max(total,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0227d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coteaching_one_trial(cfg: Config, seed: int):\n",
    "    set_seed(seed)\n",
    "    Xtr, Str, Xts, Yts = load_npz_dataset(cfg.dataset_path)\n",
    "    tr_idx, val_idx = split_train_val(Xtr, Str, val_ratio=0.2, seed=seed)\n",
    "    tr_loader, val_loader, ts_loader = make_loaders(\n",
    "        Xtr, Str, Xts, Yts, tr_idx, val_idx,\n",
    "        batch_size=cfg.batch_size, num_workers=cfg.num_workers\n",
    "    )\n",
    "\n",
    "    m1 = SmallCNN(cfg.n_classes).to(DEVICE)\n",
    "    m2 = SmallCNN(cfg.n_classes).to(DEVICE)\n",
    "    opt1 = torch.optim.AdamW(m1.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    opt2 = torch.optim.AdamW(m2.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    sch1 = torch.optim.lr_scheduler.CosineAnnealingLR(opt1, T_max=cfg.epochs)\n",
    "    sch2 = torch.optim.lr_scheduler.CosineAnnealingLR(opt2, T_max=cfg.epochs)\n",
    "    ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    hist = {\"seed\": seed, \"ts_acc_ens\": []}\n",
    "\n",
    "    for ep in range(cfg.epochs):\n",
    "        m1.train(); m2.train()\n",
    "        fr, rr = coteaching_schedule(ep, cfg.noise_rate, cfg.num_gradual, cfg.exponent)\n",
    "        for xb, yb in tr_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            l1 = ce(m1(xb), yb); l2 = ce(m2(xb), yb)\n",
    "            idx1 = select_small_loss_idx(l1.detach(), rr)\n",
    "            idx2 = select_small_loss_idx(l2.detach(), rr)\n",
    "\n",
    "            loss1 = ce(m1(xb)[idx2], yb[idx2]).mean()\n",
    "            loss2 = ce(m2(xb)[idx1], yb[idx1]).mean()\n",
    "\n",
    "            opt1.zero_grad(set_to_none=True); loss1.backward(); opt1.step()\n",
    "            opt2.zero_grad(set_to_none=True); loss2.backward(); opt2.step()\n",
    "\n",
    "        sch1.step(); sch2.step()\n",
    "        hist[\"ts_acc_ens\"].append(evaluate_ensemble(m1, m2, ts_loader, DEVICE))\n",
    "\n",
    "    return hist[\"ts_acc_ens\"][-1], hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03970a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials_for_dataset(cfg: Config, dataset_tag: str):\n",
    "    ensure_dir(cfg.out_dir)\n",
    "    out_dir = Path(cfg.out_dir) / f\"{dataset_tag}_coteaching\"\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    accs = []\n",
    "    for k in range(cfg.n_trials):\n",
    "        seed = 1234 + k\n",
    "        print(f\"\\n[Dataset={dataset_tag}] Trial {k+1}/{cfg.n_trials} (seed={seed})\")\n",
    "        acc, hist = train_coteaching_one_trial(cfg, seed)\n",
    "        accs.append(acc)\n",
    "        with open(out_dir / f\"trial_{k+1:02d}.json\", \"w\") as f:\n",
    "            json.dump(hist, f)\n",
    "\n",
    "    mean = float(np.mean(accs)); std = float(np.std(accs, ddof=1)) if len(accs)>1 else 0.0\n",
    "    summary = {\n",
    "        \"dataset\": dataset_tag, \"algo\": \"Co-Teaching\",\n",
    "        \"n_trials\": cfg.n_trials, \"epochs\": cfg.epochs,\n",
    "        \"batch_size\": cfg.batch_size, \"noise_rate\": cfg.noise_rate,\n",
    "        \"mean_test_acc\": mean, \"std_test_acc\": std,\n",
    "    }\n",
    "    with open(out_dir / \"summary.json\", \"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "    csv_path = Path(cfg.out_dir) / \"summary_all.csv\"\n",
    "    df_row = pd.DataFrame([summary])\n",
    "    if csv_path.exists():\n",
    "        df_old = pd.read_csv(csv_path)\n",
    "        df = pd.concat([df_old, df_row], ignore_index=True)\n",
    "    else:\n",
    "        df = df_row\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"[Done] {dataset_tag}: mean±std = {mean:.2%} ± {std:.2%}\")\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7c7510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(dataset_path='FashionMNIST0.3.npz', n_classes=3, batch_size=256, epochs=25, lr=0.001, weight_decay=0.0005, num_workers=0, noise_rate=0.3, num_gradual=10, exponent=1.0, n_trials=10, out_dir='./results_coteaching')\n",
      "Config(dataset_path='FashionMNIST0.6.npz', n_classes=3, batch_size=256, epochs=35, lr=0.001, weight_decay=0.0005, num_workers=0, noise_rate=0.6, num_gradual=10, exponent=1.0, n_trials=10, out_dir='./results_coteaching')\n",
      "Train X shape: (18000, 1, 28, 28) | noisy y: (18000,)\n",
      "Test  X shape: (3000, 1, 28, 28) | clean y: (3000,)\n",
      "Train X stats: min 0.0 max 1.0 mean 0.28434571623802185\n",
      "Train X shape: (18000, 1, 28, 28) | noisy y: (18000,)\n",
      "Test  X shape: (3000, 1, 28, 28) | clean y: (3000,)\n",
      "Train X stats: min 0.0 max 1.0 mean 0.28434571623802185\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 1/10 (seed=1234)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 2/10 (seed=1235)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 3/10 (seed=1236)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 4/10 (seed=1237)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 5/10 (seed=1238)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 6/10 (seed=1239)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 7/10 (seed=1240)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 8/10 (seed=1241)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 9/10 (seed=1242)\n",
      "\n",
      "[Dataset=FashionMNIST0.3] Trial 10/10 (seed=1243)\n",
      "[Done] FashionMNIST0.3: mean±std = 96.36% ± 0.30%\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 1/10 (seed=1234)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 2/10 (seed=1235)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 3/10 (seed=1236)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 4/10 (seed=1237)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 5/10 (seed=1238)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 6/10 (seed=1239)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 7/10 (seed=1240)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 8/10 (seed=1241)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 9/10 (seed=1242)\n",
      "\n",
      "[Dataset=FashionMNIST0.6] Trial 10/10 (seed=1243)\n",
      "[Done] FashionMNIST0.6: mean±std = 74.25% ± 20.11%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7425333333333333, 0.20105525313906825)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_03 = \"FashionMNIST0.3.npz\"\n",
    "PATH_06 = \"FashionMNIST0.6.npz\"\n",
    "\n",
    "cfg03 = Config(dataset_path=PATH_03, noise_rate=0.3, epochs=25, n_trials=10, num_workers=0, out_dir=\"./results_coteaching\")\n",
    "cfg06 = Config(dataset_path=PATH_06, noise_rate=0.6, epochs=35, n_trials=10, num_workers=0, out_dir=\"./results_coteaching\")\n",
    "\n",
    "print(cfg03)\n",
    "print(cfg06)\n",
    "\n",
    "preview_dataset(PATH_03)\n",
    "preview_dataset(PATH_06)\n",
    "\n",
    "run_trials_for_dataset(cfg03, dataset_tag=\"FashionMNIST0.3\")\n",
    "run_trials_for_dataset(cfg06, dataset_tag=\"FashionMNIST0.6\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
