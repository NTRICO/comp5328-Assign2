{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20673cd-3a00-42d2-b3d5-ebfb7a7e36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, csv, argparse, random\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from tqdm.notebook import tqdm\n",
    "import csv, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "#  Utils: seed & split\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 20251013):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def stratified_split(y, val_ratio=0.2, seed=20251013):\n",
    "    \"\"\"stratified split on noisy labels y (1D numpy array)\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = np.asarray(y)\n",
    "    idx = np.arange(len(y))\n",
    "    tr_idx, val_idx = [], []\n",
    "    for c in np.unique(y):\n",
    "        c_idx = idx[y == c]\n",
    "        rng.shuffle(c_idx)\n",
    "        n_val = int(round(len(c_idx) * val_ratio))\n",
    "        val_idx.append(c_idx[:n_val])\n",
    "        tr_idx.append(c_idx[n_val:])\n",
    "    return np.concatenate(tr_idx), np.concatenate(val_idx)\n",
    "\n",
    "# -------------------------\n",
    "#  Data helpers\n",
    "# -------------------------\n",
    "def infer_image_shape(x2d: np.ndarray) -> Tuple[int,int,int]:\n",
    "    D = x2d.shape[1]\n",
    "    if D == 28*28: return (1, 28, 28)\n",
    "    if D == 32*32*3: return (3, 32, 32)\n",
    "    s = int(round(math.sqrt(D)))\n",
    "    if s*s == D: return (1, s, s)\n",
    "    return (1, 1, D)\n",
    "\n",
    "def to_tensor_images(x: np.ndarray) -> torch.Tensor:\n",
    "    x = x.astype(np.float32)\n",
    "    if x.max() > 1.5: x = x/255.0  # [0,1]\n",
    "    if x.ndim == 2:\n",
    "        C,H,W = infer_image_shape(x); x = x.reshape((-1, C, H, W))\n",
    "    elif x.ndim == 3:  # (N,H,W)\n",
    "        x = x[:, None, ...]\n",
    "    elif x.ndim == 4 and x.shape[-1] in (1,3):  # (N,H,W,C) -> (N,C,H,W)\n",
    "        x = np.transpose(x, (0,3,1,2))\n",
    "    return torch.from_numpy(x)\n",
    "\n",
    "class NumpyTensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, aug: bool=False):\n",
    "        self.X = to_tensor_images(X)  # (N,C,H,W) float32 in [0,1]\n",
    "        self.y = torch.from_numpy(y.astype(np.int64))\n",
    "        self.aug = aug\n",
    "\n",
    "    def __len__(self): return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.X[i], self.y[i]\n",
    "        \n",
    "        if self.aug and x.shape[0]==3 and x.shape[-2:]==(32,32):\n",
    "            # random horizontal flip\n",
    "            if random.random() < 0.5:\n",
    "                x = torch.flip(x, dims=[2])  \n",
    "            # random crop 32 with padding=4（reflect padding）\n",
    "            pad = 4\n",
    "            x = F.pad(x, (pad,pad,pad,pad), mode='reflect')\n",
    "            i0 = random.randint(0, 2*pad)\n",
    "            j0 = random.randint(0, 2*pad)\n",
    "            x = x[:, j0:j0+32, i0:i0+32]\n",
    "        return x, y\n",
    "\n",
    "# -------------------------\n",
    "#  Models\n",
    "# -------------------------\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, in_ch=1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 32, 3, padding=1), nn.ReLU(True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),    nn.ReLU(True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),   nn.ReLU(True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.net(x).flatten(1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def make_resnet18_cifar(in_ch: int, num_classes: int):\n",
    "    m = resnet18(weights=None)\n",
    "    m.conv1  = nn.Conv2d(in_ch, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    m.fc = nn.Linear(512, num_classes)\n",
    "    return m\n",
    "\n",
    "# -------------------------\n",
    "#  (Optional) forward correction\n",
    "# -------------------------\n",
    "def row_normalize(T: torch.Tensor) -> torch.Tensor:\n",
    "    return T / (T.sum(dim=1, keepdim=True) + 1e-12)\n",
    "\n",
    "def forward_ce_loss(logits, y_noisy, T):\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    noisy_probs = probs @ T\n",
    "    log_noisy = torch.log(noisy_probs + 1e-12)\n",
    "    return F.nll_loss(log_noisy, y_noisy)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_T_anchor(model, loader, num_classes, device, topk=0.02, min_prob=0.0):\n",
    "    model.eval()\n",
    "    probs_all, pred_all, noisy_all = [], [], []\n",
    "    for x, s in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1).cpu()\n",
    "        probs_all.append(probs)\n",
    "        pred_all.append(probs.argmax(1))\n",
    "        noisy_all.append(s)\n",
    "    probs = torch.cat(probs_all); preds = torch.cat(pred_all); noisy = torch.cat(noisy_all)\n",
    "    T = torch.zeros((num_classes, num_classes), dtype=torch.float64)\n",
    "    for i in range(num_classes):\n",
    "        idx = (preds == i).nonzero(as_tuple=False).squeeze(1)\n",
    "        if idx.numel() == 0:\n",
    "            T[i] = torch.full((num_classes,), 1.0/num_classes, dtype=torch.float64); continue\n",
    "        keep = idx[probs[idx, i] >= min_prob] if min_prob>0 else torch.empty(0, dtype=torch.long)\n",
    "        use = keep if keep.numel()>0 else idx[probs[idx, i].argsort(descending=True)[:max(1, int(math.ceil(idx.numel()*topk)))]]\n",
    "        #hist = torch.bincount(noisy[use], minlength=num_classes).double()\n",
    "        #T[i] = hist / hist.sum().clamp_min(1.0)\n",
    "        hist = torch.bincount(noisy[use], minlength=num_classes).double()\n",
    "        eps = 1.0  \n",
    "        T[i] = (hist + eps) / (hist.sum() + eps * num_classes)\n",
    "    return T.float()\n",
    "\n",
    "# -------------------------\n",
    "#  Metrics\n",
    "# -------------------------\n",
    "def macro_f1_from_preds(y_true: np.ndarray, y_pred: np.ndarray, K: int) -> float:\n",
    "    cm = np.zeros((K, K), dtype=np.int64)\n",
    "    for t, p in zip(y_true, y_pred): cm[t, p] += 1\n",
    "    f1s = []\n",
    "    for k in range(K):\n",
    "        tp = cm[k,k]; fp = cm[:,k].sum()-tp; fn = cm[k,:].sum()-tp\n",
    "        prec = tp / (tp+fp+1e-12); reca = tp / (tp+fn+1e-12)\n",
    "        f1s.append(2*prec*reca/(prec+reca+1e-12))\n",
    "    return float(np.mean(f1s))\n",
    "\n",
    "def ece_score(probs: np.ndarray, y_true: np.ndarray, n_bins: int = 15) -> float:\n",
    "    conf = probs.max(1); pred = probs.argmax(1)\n",
    "    bins = np.linspace(0.,1.,n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        m = (conf > lo) & (conf <= hi)\n",
    "        if not np.any(m): continue\n",
    "        ece += (m.mean())*abs((pred[m]==y_true[m]).mean() - conf[m].mean())\n",
    "    return float(ece)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, num_classes, device):\n",
    "    model.eval()\n",
    "    tot, correct, nll_sum = 0, 0, 0.0\n",
    "    probs_all, y_all, p_all = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        nll_sum += F.nll_loss(torch.log(probs+1e-12), y, reduction='sum').item()\n",
    "        pred = probs.argmax(1)\n",
    "        correct += (pred==y).sum().item()\n",
    "        tot += y.size(0)\n",
    "        probs_all.append(probs.cpu()); y_all.append(y.cpu()); p_all.append(pred.cpu())\n",
    "    probs = torch.cat(probs_all).numpy()\n",
    "    y_true = torch.cat(y_all).numpy()\n",
    "    y_pred = torch.cat(p_all).numpy()\n",
    "    return dict(\n",
    "        acc=float(correct/tot),\n",
    "        macro_f1=float(macro_f1_from_preds(y_true, y_pred, num_classes)),\n",
    "        nll=float(nll_sum/tot),\n",
    "        ece=float(ece_score(probs, y_true, n_bins=15))\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "#  Train loop \n",
    "# -------------------------\n",
    "def train_one(dataset_name, Xtr, Str, Xval, Sval, Xte, Yte, T_given, C,\n",
    "              epochs=120, batch_size=256, lr=1e-3, weight_decay=1e-4,\n",
    "              warmup_epochs=0, topk=0.02, min_prob=0.0, mixT=0.0,\n",
    "              seed=20251013, device='cpu', arch='resnet18',\n",
    "              do_overfit_check=True):\n",
    "\n",
    "    set_seed(seed)\n",
    "    num_classes = int(C)\n",
    "\n",
    "    train_ds = NumpyTensorDataset(Xtr, Str, aug=True)\n",
    "    val_ds   = NumpyTensorDataset(Xval, Sval, aug=False)\n",
    "    test_ds  = NumpyTensorDataset(Xte,  Yte,  aug=False)\n",
    "\n",
    "    Xtmp = to_tensor_images(Xtr).float()\n",
    "    if Xtmp.max() > 1.5: Xtmp /= 255.0\n",
    "    train_mean = Xtmp.mean(dim=(0,2,3))\n",
    "    train_std  = Xtmp.std(dim=(0,2,3)).clamp_min(1e-6)\n",
    "    with torch.no_grad():\n",
    "        for ds in (train_ds, val_ds, test_ds):\n",
    "            ds.X.sub_(train_mean[None,:,None,None]).div_(train_std[None,:,None,None])\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Quick Print Self-Test\n",
    "    xb, yb = next(iter(train_loader))\n",
    "    print('num_classes C =', num_classes)\n",
    "    print('label min/max:', int(Str.min()), int(Str.max()))\n",
    "    print('label counts:', np.bincount(Str, minlength=num_classes)[:10], '...')\n",
    "    print('shape:', xb.shape)\n",
    "    print('range:', float(xb.min()), float(xb.max()))\n",
    "    ch_mean = xb.mean(dim=(0,2,3)); ch_std = xb.std(dim=(0,2,3))\n",
    "    print('per-channel mean:', [float(m) for m in ch_mean])\n",
    "    print('per-channel std :', [float(s) for s in ch_std])\n",
    "\n",
    "    # Model & Optimizer (ResNet18 on CIFAR-stem)\n",
    "    in_ch = train_ds.X.shape[1]\n",
    "    use_resnet = (arch == 'resnet18') or (arch == 'auto' and in_ch == 3)\n",
    "    print(\"[DEBUG] use_resnet =\", use_resnet, \"| arch passed =\", arch)\n",
    "\n",
    "    if use_resnet:\n",
    "        model = make_resnet18_cifar(in_ch, num_classes).to(device)\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    else:\n",
    "        model = SmallCNN(in_ch=in_ch, num_classes=num_classes).to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = None\n",
    "\n",
    "    \n",
    "    do_overfit_check=False\n",
    "    if do_overfit_check:\n",
    "        idx = torch.randperm(len(train_ds))[:256]\n",
    "        x_small = train_ds.X[idx].clone()\n",
    "        y_small = train_ds.y[idx].clone()\n",
    "        small_loader = DataLoader(TensorDataset(x_small, y_small), batch_size=256, shuffle=True)\n",
    "\n",
    "        state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        model.train()\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d): m.eval()\n",
    "\n",
    "        of_opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        best = 0.0\n",
    "        for t in range(1000):\n",
    "            for xb_, yb_ in small_loader:\n",
    "                xb_, yb_ = xb_.to(device), yb_.to(device)\n",
    "                logits = model(xb_); loss = F.cross_entropy(logits, yb_)\n",
    "                of_opt.zero_grad(); loss.backward(); of_opt.step()\n",
    "            with torch.no_grad():\n",
    "                pred = logits.argmax(1); acc = (pred==yb_).float().mean().item()\n",
    "            if (t+1) % 100 == 0:\n",
    "                print(f\"[overfit] step {t+1}: loss={loss.item():.4f}, acc={acc:.4f}\")\n",
    "            best = max(best, acc)\n",
    "            if best > 0.99: break\n",
    "        print(\"[overfit] best acc:\", best)\n",
    "        model.load_state_dict(state)\n",
    "\n",
    "  \n",
    "    warmup_eff = int(warmup_epochs)\n",
    "\n",
    "    T_used = None\n",
    "    if (T_given is not None) and (warmup_eff == 0):\n",
    "        T_used = row_normalize(T_given.clone().to(device))\n",
    "\n",
    "    # Training cycle (scheduler step after optimization in each epoch)\n",
    "    T_accum = None\n",
    "    cnt = 0\n",
    "\n",
    "    for ep in tqdm(range(epochs), desc=f\"Training (Seed {seed})\", unit=\"epoch\", leave=False):\n",
    "        model.train()\n",
    "        for x, y_noisy in train_loader:\n",
    "            x, y_noisy = x.to(device), y_noisy.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y_noisy) if (T_used is None or ep < warmup_eff) \\\n",
    "                  else forward_ce_loss(logits, y_noisy, T_used)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        \n",
    "        if T_given is None and warmup_eff > 0 and (ep+1) >= warmup_eff-2 and (ep+1) <= warmup_eff:\n",
    "            T_hat = estimate_T_anchor(model, val_loader, num_classes, device,\n",
    "                                      topk=topk, min_prob=min_prob)\n",
    "            T_hat = row_normalize(T_hat.to(device))\n",
    "            if mixT > 0.0:\n",
    "                I = torch.eye(num_classes, device=device)\n",
    "                T_hat = row_normalize((1.0 - mixT)*T_hat + mixT*I)\n",
    "            T_accum = T_hat if T_accum is None else (T_accum + T_hat)\n",
    "            cnt += 1\n",
    "\n",
    "       \n",
    "        if (ep+1) == warmup_eff and T_given is None and warmup_eff > 0:\n",
    "            T_used = row_normalize(T_accum / cnt)\n",
    "            Th = T_used.detach().cpu().numpy()\n",
    "            print('[T_hat(avg)] diag =', np.round(np.diag(Th), 3))\n",
    "            print('[T_hat(avg)] rowsum =', np.round(Th.sum(axis=1), 3))\n",
    "            print('[T_hat(avg)]\\n', np.round(Th, 3))\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    metrics = evaluate(model, test_loader, num_classes, device)\n",
    "    return metrics, (T_used.detach().cpu().numpy() if T_used is not None else None)\n",
    "\n",
    "# -------------------------\n",
    "#  NPZ loader\n",
    "# -------------------------\n",
    "def load_npz_dataset(path: str):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    keys_all = list(d.keys())\n",
    "\n",
    "    def pick(*cands):\n",
    "        for k in cands:\n",
    "            if k in d: return k\n",
    "        raise KeyError(f\"Missing any of {cands} in file. Found keys={keys_all}\")\n",
    "\n",
    "    k_Xtr = pick('Xtr','X_train','Xtrain','X_tr')\n",
    "    k_Str = pick('Str','ytr','S','y_train','Ytr','Y_tr')\n",
    "    k_Xte = pick('Xte','Xts','X_test','Xtest','X_te','X_ts')\n",
    "    k_Yte = pick('Yte','Yts','yte','Y_test','Ytest','Y_te','Y_ts')\n",
    "    k_T   = 'T' if 'T' in d else None\n",
    "\n",
    "    Xtr, Str = d[k_Xtr], d[k_Str]\n",
    "    Xte, Yte = d[k_Xte], d[k_Yte]\n",
    "    T = d[k_T].astype(np.float32) if k_T else None\n",
    "    C = int(max(Str.max(), Yte.max()) + 1)\n",
    "\n",
    "    print(f\"[keys] {keys_all}\")\n",
    "    print(f\"[mapping] Xtr={k_Xtr}, Str={k_Str}, Xte={k_Xte}, Yte={k_Yte}, T={'T' if k_T else 'None'}\")\n",
    "    return Xtr, Str, Xte, Yte, T, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685477b7-2214-4d4d-95c4-6a77bfab2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#  Configuration\n",
    "# -------------------------\n",
    "class Config:\n",
    "    data = '/content/FashionMNIST0.3.withT.npz'  \n",
    "    arch = 'cnn'                                  \n",
    "    runs = 10\n",
    "    epochs = 50          \n",
    "    warmup_epochs = 0   \n",
    "    topk = 0.02\n",
    "    min_prob = 0.0\n",
    "    mixT = 0.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    batch_size = 256\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    val_ratio = 0.2      \n",
    "    seed = 20251013\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# -------------------------\n",
    "#  Main Logic (from main())\n",
    "# -------------------------\n",
    "\n",
    "Xtr, Str, Xte, Yte, T, C = load_npz_dataset(args.data)\n",
    "dataset_name = os.path.splitext(os.path.basename(args.data))[0]\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "out_csv = os.path.join('results', f'{dataset_name}_forward_runs.csv')\n",
    "out_Tnpz = os.path.join('results', f'{dataset_name}_T_used_runs.npz')\n",
    "\n",
    "# CSV header\n",
    "with open(out_csv, 'w', newline='') as f:\n",
    "    csv.writer(f).writerow(['run', 'acc', 'macro_f1', 'nll', 'ece'])\n",
    "\n",
    "all_metrics = []\n",
    "T_collection = {}\n",
    "\n",
    "for r in range(args.runs):\n",
    "    seed = args.seed + r\n",
    "    print(f\"\\n--- Starting Run {r+1}/{args.runs} (Seed: {seed}) ---\")\n",
    "\n",
    "    # stratified split on noisy labels\n",
    "    tr_idx, val_idx = stratified_split(Str, val_ratio=args.val_ratio, seed=seed)\n",
    "    X_tr, S_tr = Xtr[tr_idx], Str[tr_idx]\n",
    "    X_val, S_val = Xtr[val_idx], Str[val_idx]\n",
    "\n",
    "    metrics, T_used = train_one(\n",
    "        dataset_name, X_tr, S_tr, X_val, S_val, Xte, Yte,\n",
    "        None if T is None else torch.from_numpy(T),\n",
    "        C=C,\n",
    "        epochs=args.epochs, batch_size=args.batch_size,\n",
    "        lr=args.lr, weight_decay=args.weight_decay,\n",
    "        warmup_epochs=args.warmup_epochs, topk=args.topk,\n",
    "        min_prob=args.min_prob, mixT=args.mixT,\n",
    "        seed=seed, device=device,\n",
    "        arch=args.arch\n",
    "    )\n",
    "\n",
    "    all_metrics.append(metrics)\n",
    "    with open(out_csv, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([r+1, metrics['acc'], metrics['macro_f1'], metrics['nll'], metrics['ece']])\n",
    "    print(f\"Run {r+1}/{args.runs}: acc={metrics['acc']:.4f}, macroF1={metrics['macro_f1']:.4f}, \"\n",
    "          f\"NLL={metrics['nll']:.4f}, ECE={metrics['ece']:.4f}\")\n",
    "\n",
    "    if T_used is not None:\n",
    "        T_collection[f'T_run{r+1}'] = T_used\n",
    "\n",
    "\n",
    "accs  = np.array([m['acc'] for m in all_metrics], dtype=np.float64)\n",
    "f1s   = np.array([m['macro_f1'] for m in all_metrics], dtype=np.float64)\n",
    "nlls  = np.array([m['nll'] for m in all_metrics], dtype=np.float64)\n",
    "eces  = np.array([m['ece'] for m in all_metrics], dtype=np.float64)\n",
    "\n",
    "mean_acc, std_acc = accs.mean(), accs.std()\n",
    "mean_f1,  std_f1  = f1s.mean(),  f1s.std()\n",
    "mean_nll, std_nll = nlls.mean(), nlls.std()\n",
    "mean_ece, std_ece = eces.mean(), eces.std()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"==> [{dataset_name}]  acc  = {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "print(f\"==> [{dataset_name}]  F1   = {mean_f1:.4f}\")\n",
    "print(f\"==> [{dataset_name}]  NLL  = {mean_nll:.4f}\")\n",
    "print(f\"==> [{dataset_name}]  ECE  = {mean_ece:.4f}\")\n",
    "print(f\"==>  Results saved to: {out_csv}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "\n",
    "with open(out_csv, 'a', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['---', '---', '---', '---', '---'])\n",
    "    w.writerow(['mean', mean_acc, mean_f1, mean_nll, mean_ece])\n",
    "    w.writerow(['std',  std_acc,  std_f1,  std_nll,  std_ece])\n",
    "\n",
    "if len(T_collection):\n",
    "    np.savez(out_Tnpz, **T_collection)\n",
    "    print(f\"==> T matrices saved to: {out_Tnpz}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a092777-8f41-492f-b73e-1bcd40304e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#  Configuration\n",
    "# -------------------------\n",
    "class Config:\n",
    "\n",
    "    data = '/content/FashionMNIST0.6.withT.npz'   \n",
    "    arch = 'cnn'                                 \n",
    "    runs = 10\n",
    "    epochs = 60           \n",
    "    warmup_epochs = 0     \n",
    "    topk = 0.02\n",
    "    min_prob = 0.0\n",
    "    mixT = 0.0\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    batch_size = 256\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    val_ratio = 0.2       \n",
    "    seed = 20251013\n",
    "\n",
    "args = Config()\n",
    "\n",
    "# -------------------------\n",
    "#  Main Logic (from main())\n",
    "# -------------------------\n",
    "\n",
    "Xtr, Str, Xte, Yte, T, C = load_npz_dataset(args.data)\n",
    "dataset_name = os.path.splitext(os.path.basename(args.data))[0]\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "out_csv = os.path.join('results', f'{dataset_name}_forward_runs.csv')\n",
    "out_Tnpz = os.path.join('results', f'{dataset_name}_T_used_runs.npz')\n",
    "\n",
    "# CSV header\n",
    "with open(out_csv, 'w', newline='') as f:\n",
    "    csv.writer(f).writerow(['run', 'acc', 'macro_f1', 'nll', 'ece'])\n",
    "\n",
    "all_metrics = []\n",
    "T_collection = {}\n",
    "\n",
    "for r in range(args.runs):\n",
    "    seed = args.seed + r\n",
    "    print(f\"\\n--- Starting Run {r+1}/{args.runs} (Seed: {seed}) ---\")\n",
    "\n",
    "    # stratified split on noisy labels\n",
    "    tr_idx, val_idx = stratified_split(Str, val_ratio=args.val_ratio, seed=seed)\n",
    "    X_tr, S_tr = Xtr[tr_idx], Str[tr_idx]\n",
    "    X_val, S_val = Xtr[val_idx], Str[val_idx]\n",
    "\n",
    "    metrics, T_used = train_one(\n",
    "        dataset_name, X_tr, S_tr, X_val, S_val, Xte, Yte,\n",
    "        None if T is None else torch.from_numpy(T),\n",
    "        C=C,\n",
    "        epochs=args.epochs, batch_size=args.batch_size,\n",
    "        lr=args.lr, weight_decay=args.weight_decay,\n",
    "        warmup_epochs=args.warmup_epochs, topk=args.topk,\n",
    "        min_prob=args.min_prob, mixT=args.mixT,\n",
    "        seed=seed, device=device,\n",
    "        arch=args.arch\n",
    "    )\n",
    "\n",
    "    all_metrics.append(metrics)\n",
    "    with open(out_csv, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([r+1, metrics['acc'], metrics['macro_f1'], metrics['nll'], metrics['ece']])\n",
    "    print(f\"Run {r+1}/{args.runs}: acc={metrics['acc']:.4f}, macroF1={metrics['macro_f1']:.4f}, \"\n",
    "          f\"NLL={metrics['nll']:.4f}, ECE={metrics['ece']:.4f}\")\n",
    "\n",
    "    if T_used is not None:\n",
    "        T_collection[f'T_run{r+1}'] = T_used\n",
    "\n",
    "\n",
    "accs  = np.array([m['acc'] for m in all_metrics], dtype=np.float64)\n",
    "f1s   = np.array([m['macro_f1'] for m in all_metrics], dtype=np.float64)\n",
    "nlls  = np.array([m['nll'] for m in all_metrics], dtype=np.float64)\n",
    "eces  = np.array([m['ece'] for m in all_metrics], dtype=np.float64)\n",
    "\n",
    "mean_acc, std_acc = accs.mean(), accs.std()\n",
    "mean_f1,  std_f1  = f1s.mean(),  f1s.std()\n",
    "mean_nll, std_nll = nlls.mean(), nlls.std()\n",
    "mean_ece, std_ece = eces.mean(), eces.std()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"==> [{dataset_name}]  acc  = {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "print(f\"==> [{dataset_name}]  F1   = {mean_f1:.4f}\")\n",
    "print(f\"==> [{dataset_name}]  NLL  = {mean_nll:.4f}\")\n",
    "print(f\"==> [{dataset_name}]  ECE  = {mean_ece:.4f}\")\n",
    "print(f\"==>  Results saved to: {out_csv}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "\n",
    "with open(out_csv, 'a', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['---', '---', '---', '---', '---'])\n",
    "    w.writerow(['mean', mean_acc, mean_f1, mean_nll, mean_ece])\n",
    "    w.writerow(['std',  std_acc,  std_f1,  std_nll,  std_ece])\n",
    "\n",
    "if len(T_collection):\n",
    "    np.savez(out_Tnpz, **T_collection)\n",
    "    print(f\"==> T matrices saved to: {out_Tnpz}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ad621-8ecd-4a35-868e-4d5fb90cc4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "#  Configuration\n",
    "# -------------------------\n",
    "class Config:\n",
    "    data = '/content/CIFAR.npz'\n",
    "    arch = 'resnet18'\n",
    "    runs = 10           \n",
    "    epochs = 120\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    batch_size = 256\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-4\n",
    "    seed = 20251013\n",
    "    val_ratio = 0.2\n",
    "    warmup_epochs = 40  \n",
    "    topk = 0.02       \n",
    "    min_prob = 0.8     \n",
    "    mixT = 0.15   \n",
    "\n",
    "args = Config()\n",
    "\n",
    "# -------------------------\n",
    "#  Main Logic\n",
    "# -------------------------\n",
    "Xtr, Str, Xte, Yte, T, C = load_npz_dataset(args.data)\n",
    "dataset_name = os.path.splitext(os.path.basename(args.data))[0]\n",
    "\n",
    "device = torch.device(args.device)\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "out_csv = os.path.join('results', f'{dataset_name}_forward_runs.csv')\n",
    "out_Tnpz = os.path.join('results', f'{dataset_name}_T_used_runs.npz')\n",
    "\n",
    "with open(out_csv, 'w', newline='') as f:\n",
    "    csv.writer(f).writerow(['run', 'acc', 'macro_f1', 'nll', 'ece'])\n",
    "\n",
    "all_metrics, T_collection = [], {}\n",
    "\n",
    "for r in range(args.runs):\n",
    "    seed = args.seed + r\n",
    "    print(f\"\\n--- Starting Run {r+1}/{args.runs} (Seed: {seed}) ---\")\n",
    "\n",
    "    tr_idx, val_idx = stratified_split(Str, val_ratio=args.val_ratio, seed=seed)\n",
    "    X_tr, S_tr = Xtr[tr_idx], Str[tr_idx]\n",
    "    X_val, S_val = Xtr[val_idx], Str[val_idx]\n",
    "\n",
    "    metrics, T_used = train_one(\n",
    "        dataset_name, X_tr, S_tr, X_val, S_val, Xte, Yte,\n",
    "        None if T is None else torch.from_numpy(T),\n",
    "        C=C,\n",
    "        epochs=args.epochs, batch_size=args.batch_size,\n",
    "        lr=args.lr, weight_decay=args.weight_decay,\n",
    "        warmup_epochs=args.warmup_epochs, topk=args.topk,\n",
    "        min_prob=args.min_prob, mixT=args.mixT,\n",
    "        seed=seed, device=device,\n",
    "        arch=args.arch,\n",
    "        do_overfit_check=False        \n",
    "    )\n",
    "\n",
    "    all_metrics.append(metrics)\n",
    "    with open(out_csv, 'a', newline='') as f:\n",
    "        csv.writer(f).writerow([r+1, metrics['acc'], metrics['macro_f1'], metrics['nll'], metrics['ece']])\n",
    "    print(f\"Run {r+1}/{args.runs}: acc={metrics['acc']:.4f}, macroF1={metrics['macro_f1']:.4f}, \"\n",
    "          f\"NLL={metrics['nll']:.4f}, ECE={metrics['ece']:.4f}\")\n",
    "\n",
    "    if T_used is not None:\n",
    "        T_collection[f'T_run{r+1}'] = T_used\n",
    "\n",
    "accs  = np.array([m['acc'] for m in all_metrics], dtype=np.float64)\n",
    "f1s   = np.array([m['macro_f1'] for m in all_metrics], dtype=np.float64)\n",
    "nlls  = np.array([m['nll'] for m in all_metrics], dtype=np.float64)\n",
    "eces  = np.array([m['ece'] for m in all_metrics], dtype=np.float64)\n",
    "\n",
    "mean_acc, std_acc = accs.mean(), accs.std()\n",
    "mean_f1,  std_f1  = f1s.mean(),  f1s.std()\n",
    "mean_nll, std_nll = nlls.mean(), nlls.std()\n",
    "mean_ece, std_ece = eces.mean(), eces.std()\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"==> {dataset_name}: acc = {mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "print(f\"==> {dataset_name}: F1  = {mean_f1:.4f}\")\n",
    "print(f\"==> {dataset_name}: NLL = {mean_nll:.4f}\")\n",
    "print(f\"==> {dataset_name}: ECE = {mean_ece:.4f}\")\n",
    "print(f\"==> Results saved to: {out_csv}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "with open(out_csv, 'a', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(['---', '---', '---', '---', '---'])\n",
    "    w.writerow(['mean', mean_acc, mean_f1, mean_nll, mean_ece])\n",
    "    w.writerow(['std',  std_acc,  std_f1,  std_nll,  std_ece])\n",
    "\n",
    "if len(T_collection):\n",
    "    np.savez(out_Tnpz, **T_collection)\n",
    "    print(f\"==> T matrices saved to: {out_Tnpz}\")\n",
    "\n",
    "print(\"\\n--- Training complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd2baa-5d1d-4c94-b935-86a5a7273166",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path('results/FashionMNIST0.3.withT_forward_runs.csv') \n",
    "\n",
    "acc = []\n",
    "with open(csv_path, newline='') as f:\n",
    "    rows = list(csv.reader(f))\n",
    "    for r in rows[1:11]:\n",
    "        try:\n",
    "            acc.append(float(r[1]))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\n",
    "acc = np.array(acc)\n",
    "runs = np.arange(1, len(acc)+1)\n",
    "m, s = acc.mean(), acc.std()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(runs, acc, marker='o', linewidth=1)\n",
    "plt.hlines(m, runs.min()-0.2, runs.max()+0.2, linestyles='dashed', linewidth=1.5)\n",
    "plt.fill_between([runs.min()-0.2, runs.max()+0.2], m-s, m+s, alpha=0.15)\n",
    "\n",
    "plt.title(f'FashionMNIST 0.3 — Accuracy over 10 runs  (mean={m:.3f},  std={s:.3f})')\n",
    "plt.xlabel('Run'); plt.ylabel('Accuracy')\n",
    "plt.xlim(runs.min()-0.2, runs.max()+0.2)\n",
    "ymin = max(0.0, acc.min()-0.05); ymax = min(1.0, acc.max()+0.05)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.grid(True, linestyle=':', linewidth=0.8)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987afbc8-d1a8-4b2c-aa11-ddea1ef92b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path('results/FashionMNIST0.6.withT_forward_runs.csv')  \n",
    "acc = []\n",
    "with open(csv_path, newline='') as f:\n",
    "    rows = list(csv.reader(f))\n",
    "    for r in rows[1:11]:\n",
    "        try:\n",
    "            acc.append(float(r[1]))\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\n",
    "acc = np.array(acc)\n",
    "runs = np.arange(1, len(acc)+1)\n",
    "m, s = acc.mean(), acc.std()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(runs, acc, marker='o', linewidth=1)\n",
    "plt.hlines(m, runs.min()-0.2, runs.max()+0.2, linestyles='dashed', linewidth=1.5)\n",
    "plt.fill_between([runs.min()-0.2, runs.max()+0.2], m-s, m+s, alpha=0.15)\n",
    "\n",
    "plt.title(f'FashionMNIST 0.6 — Accuracy over 10 runs  (mean={m:.3f},  std={s:.3f})')\n",
    "plt.xlabel('Run'); plt.ylabel('Accuracy')\n",
    "plt.xlim(runs.min()-0.2, runs.max()+0.2)\n",
    "ymin = max(0.0, acc.min()-0.05); ymax = min(1.0, acc.max()+0.05)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.grid(True, linestyle=':', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123ae60-b86b-498e-a08f-2453fce0ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path('results/CIFAR_forward_runs.csv')  \n",
    "\n",
    "acc = []\n",
    "with open(csv_path, newline='') as f:\n",
    "    rows = list(csv.reader(f))\n",
    "    for r in rows[1:11]:\n",
    "        try:\n",
    "            acc.append(float(r[1]))\n",
    "        except:\n",
    "            break\n",
    "acc = np.array(acc)\n",
    "runs = np.arange(1, len(acc)+1)\n",
    "\n",
    "m, s = acc.mean(), acc.std()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(runs, acc, marker='o', linewidth=1)             \n",
    "plt.hlines(m, runs.min()-0.2, runs.max()+0.2, linestyles='dashed', linewidth=1.5)  \n",
    "plt.fill_between([runs.min()-0.2, runs.max()+0.2], m-s, m+s, alpha=0.15)        \n",
    "\n",
    "plt.title(f'CIFAR — Accuracy over 10 runs (mean={m:.3f}, std={s:.3f})')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(runs.min()-0.2, runs.max()+0.2)\n",
    "ymin = max(0.0, acc.min()-0.05); ymax = min(1.0, acc.max()+0.05)\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.grid(True, linestyle=':', linewidth=0.8)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
