{
  "cells": [
    {
      "metadata": {
        "id": "331dd9a7608d7ffc"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Library"
      ],
      "id": "331dd9a7608d7ffc"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFgKZLmoKBAd",
        "outputId": "8de6c732-f241-44d1-8b61-977e8a40192d"
      },
      "id": "OFgKZLmoKBAd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:51:51.799802Z",
          "start_time": "2025-10-28T13:51:50.148947Z"
        },
        "id": "fa725d27b6efd992"
      },
      "cell_type": "code",
      "source": [
        "from ast import Import\n",
        "import os\n",
        "import gc\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from contextlib import redirect_stdout, redirect_stderr"
      ],
      "id": "fa725d27b6efd992",
      "outputs": [],
      "execution_count": 79
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "WUqswZKWJx2K"
      },
      "id": "WUqswZKWJx2K",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:21:43.303460Z",
          "start_time": "2025-10-28T13:21:40.781393Z"
        },
        "id": "981d521b3f5319a2"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "981d521b3f5319a2",
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "13da974a2c67651f"
      },
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "id": "13da974a2c67651f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:33:39.408058Z",
          "start_time": "2025-10-28T13:33:39.395046Z"
        },
        "id": "4a04d7f0c76021d2"
      },
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path, val_ratio=0.2, random_state=42):\n",
        "    dataset = np.load(file_path)\n",
        "    Xtr, Str = dataset['Xtr'], dataset['Str']\n",
        "    Xts, Yts = dataset['Xts'], dataset['Yts']\n",
        "\n",
        "    # Shuffle & split (80% train, 20% validation)\n",
        "    np.random.seed(random_state)\n",
        "    indices = np.arange(len(Str))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_idx = int(len(Str) * (1 - val_ratio))\n",
        "    train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n",
        "\n",
        "    X_train, y_train = Xtr[train_idx], Str[train_idx]\n",
        "    X_val, y_val = Xtr[val_idx], Str[val_idx]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, Xts, Yts\n",
        "\n",
        "def reshape_mnist(*arrays):\n",
        "    reshaped = []\n",
        "\n",
        "    for arr in arrays:\n",
        "        if arr.ndim == 1:\n",
        "            arr = torch.tensor(arr, dtype=torch.long)\n",
        "        elif arr.ndim >= 2:\n",
        "            arr = arr.reshape(-1, 1, 28, 28)\n",
        "            arr = torch.tensor(arr, dtype=torch.float32) / 255.0\n",
        "            mean = torch.tensor(0.1307, dtype=torch.float32).view(1, 1, 1)\n",
        "            std  = torch.tensor(0.3081, dtype=torch.float32).view(1, 1, 1)\n",
        "            arr = (arr - mean) / std\n",
        "        reshaped.append(arr)\n",
        "    return tuple(reshaped)\n",
        "\n",
        "def reshape_cifar(*arrays):\n",
        "    reshaped = []\n",
        "    mean = torch.tensor([0.4914, 0.4822, 0.4465], dtype=torch.float32).view(3, 1, 1)\n",
        "    std  = torch.tensor([0.2023, 0.1994, 0.2010], dtype=torch.float32).view(3, 1, 1)\n",
        "    for arr in arrays:\n",
        "        if arr.ndim == 1:\n",
        "            arr = torch.tensor(arr, dtype=torch.long)\n",
        "        elif arr.ndim >= 2:\n",
        "            arr = np.transpose(arr, (0, 3, 1, 2))\n",
        "            arr = torch.tensor(arr, dtype=torch.float32) / 255.0\n",
        "            arr = (arr - mean) / std\n",
        "        reshaped.append(arr)\n",
        "    return tuple(reshaped)\n"
      ],
      "id": "4a04d7f0c76021d2",
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset testing"
      ],
      "metadata": {
        "id": "115CliNQl3oF"
      },
      "id": "115CliNQl3oF"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:33:40.217263Z",
          "start_time": "2025-10-28T13:33:40.037515Z"
        },
        "id": "94753f7fe8e2ce2e"
      },
      "cell_type": "code",
      "source": [
        "Xtr_03, Str_03, Xval_03, Sval_03, Xts_03, Yts_03 = reshape_mnist(*load_dataset('datasets/FashionMNIST0.3.npz'))\n",
        "Xtr_06, Str_06, Xval_06, Sval_06, Xts_06, Yts_06 = reshape_mnist(*load_dataset('datasets/FashionMNIST0.6.npz'))\n",
        "Xtr_cifar, Str_cifar, Xval_cifar, Sval_cifar, Xts_cifar, Yts_cifar = reshape_cifar(*load_dataset('datasets/CIFAR.npz'))"
      ],
      "id": "94753f7fe8e2ce2e",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:33:40.781113Z",
          "start_time": "2025-10-28T13:33:40.768604Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd31fa31ebd84f7d",
        "outputId": "1e101947-89c6-49b8-c741-038e9141ff7c"
      },
      "cell_type": "code",
      "source": [
        "print(\"Xtr_03:\", Xtr_03.shape)\n",
        "print(\"Xtr_06:\", Xtr_06.shape)\n",
        "print(\"Xtr_cifar:\", Xtr_cifar.shape)"
      ],
      "id": "cd31fa31ebd84f7d",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xtr_03: torch.Size([14400, 1, 28, 28])\n",
            "Xtr_06: torch.Size([14400, 1, 28, 28])\n",
            "Xtr_cifar: torch.Size([12000, 3, 32, 32])\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:33:41.637497Z",
          "start_time": "2025-10-28T13:33:41.566282Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "635db7496d518a0e",
        "outputId": "541b04d5-b41e-471c-c2b3-19b8ad784faf"
      },
      "cell_type": "code",
      "source": [
        "# Mnist data checking\n",
        "plt.imshow(Xtr_03[114, -1, :, :], cmap='gray') #[pic number,_,_,_]\n",
        "plt.title(f\"Label: {Str_03[0]}\")\n",
        "plt.show()"
      ],
      "id": "635db7496d518a0e",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-28T13:33:42.777321Z",
          "start_time": "2025-10-28T13:33:42.710432Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "9483fb34c05f4089",
        "outputId": "60caa673-a27a-4f75-a432-0d25be7de553"
      },
      "cell_type": "code",
      "source": [
        "# CIFAR data checking\n",
        "mean_test = torch.tensor([0.4914, 0.4822, 0.4465], dtype=torch.float32).view(3, 1, 1)\n",
        "std_test  = torch.tensor([0.2023, 0.1994, 0.2010], dtype=torch.float32).view(3, 1, 1)\n",
        "test_cifar = Xtr_cifar[514] * std_test + mean_test\n",
        "plt.imshow(np.transpose(test_cifar, (1, -1, 0)))\n",
        "plt.title(f\"Label: {Str_cifar[0]}\")\n",
        "plt.show()"
      ],
      "id": "9483fb34c05f4089",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## True transition matrix reloading"
      ],
      "metadata": {
        "id": "eqWx_pidl9Gz"
      },
      "id": "eqWx_pidl9Gz"
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedTransition(nn.Module):\n",
        "\n",
        "    def __init__(self, T: torch.Tensor):\n",
        "        super().__init__()\n",
        "\n",
        "        T = T.detach().float()\n",
        "        self.register_buffer(\"T\", T)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.T\n",
        "def select_known_T(dataset_name: str):\n",
        "\n",
        "    mnist03_T = torch.tensor([\n",
        "        [0.7, 0.3, 0.0],\n",
        "        [0.0, 0.7, 0.3],\n",
        "        [0.3, 0.0, 0.7]\n",
        "    ], dtype=torch.float32, device=device)\n",
        "\n",
        "    mnist06_T = torch.tensor([\n",
        "        [0.4, 0.3, 0.3],\n",
        "        [0.3, 0.4, 0.3],\n",
        "        [0.3, 0.3, 0.4]\n",
        "    ], dtype=torch.float32, device=device)\n",
        "\n",
        "    name = dataset_name.lower()\n",
        "    if \"3\" in name or \"mnist03\" in name:\n",
        "        return FixedTransition(mnist03_T)\n",
        "    if \"6\" in name or \"mnist06\" in name:\n",
        "        return FixedTransition(mnist06_T)\n",
        "    return None"
      ],
      "metadata": {
        "id": "Ccyot4phEMqD"
      },
      "id": "Ccyot4phEMqD",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b81e59830c889b5"
      },
      "cell_type": "markdown",
      "source": [
        "# Main Program"
      ],
      "id": "b81e59830c889b5"
    },
    {
      "metadata": {
        "id": "dd2dc1c0a8571ecb"
      },
      "cell_type": "markdown",
      "source": [
        "## Classifier: ResNet-tiny"
      ],
      "id": "dd2dc1c0a8571ecb"
    },
    {
      "metadata": {
        "id": "e6a22e075ea1ad7e"
      },
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes,mode='cifar'):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        if mode == 'cifar':\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=0, bias=False)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        #self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        #self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(128*block.expansion, num_classes)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, revision=True):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        #out = self.layer3(out)\n",
        "        #out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        out = self.linear(out)\n",
        "\n",
        "        clean = F.softmax(out, 1)\n",
        "\n",
        "        return clean\n",
        "\n",
        "def ResNet_tiny(num_classes,mode):\n",
        "  return ResNet(BasicBlock, [1,1], num_classes,mode)"
      ],
      "id": "e6a22e075ea1ad7e",
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transition matrix initialization"
      ],
      "metadata": {
        "id": "-KwXA5meoDkK"
      },
      "id": "-KwXA5meoDkK"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-26T13:47:55.966800Z",
          "start_time": "2025-10-26T13:47:55.936051Z"
        },
        "id": "7f3edb74af3457c1"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class sig_t(nn.Module):\n",
        "    def __init__(self, device, num_classes, init=2):\n",
        "        super(sig_t, self).__init__()\n",
        "\n",
        "        C = num_classes\n",
        "        self.register_parameter(name='w', param=nn.parameter.Parameter(-init*torch.ones(num_classes, num_classes)))\n",
        "\n",
        "        co = torch.full((C, C), 0.3, device=device)\n",
        "        co.fill_diagonal_(0.4)\n",
        "        self.register_buffer(\"co\", co)\n",
        "        self.register_buffer(\"I\", torch.eye(C))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        sig = torch.sigmoid(self.w)\n",
        "\n",
        "        T = self.I + sig * self.co\n",
        "        return T\n",
        "\n",
        "\n",
        "def row_stoch(M, eps=1e-8):\n",
        "    M = M.clamp_min(Cfg.eps)\n",
        "    return M / (M.sum(dim=1, keepdim=True) + Cfg.eps)\n"
      ],
      "id": "7f3edb74af3457c1",
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main structrue"
      ],
      "metadata": {
        "id": "iQluEeluoI8P"
      },
      "id": "iQluEeluoI8P"
    },
    {
      "metadata": {
        "id": "9d601f0fc872489f"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 36,
      "source": [
        "def warmup(train_data, train_loader, model,optimizer_model, loss_func_ce):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        optimizer_model.zero_grad()\n",
        "\n",
        "        clean = model(batch_x)\n",
        "\n",
        "        ce_loss = loss_func_ce(clean.log(), batch_y.long())\n",
        "        loss = ce_loss\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_model.step()\n",
        "\n",
        "\n",
        "    print('Warmup Loss: {:.6f}'.format(train_loss / (len(train_data))*Cfg.batch_size))\n",
        "\n",
        "\n",
        "def train(train_data, train_loader,model,trans_for,trans_back,optimizer_es,optimizer_trans_for,optimizer_trans_back,scheduler1,scheduler2,scheduler3,loss_func_ce):\n",
        "    model.train()\n",
        "    trans_back.train()\n",
        "    trans_for.train()\n",
        "\n",
        "    if optimizer_trans_for is None:\n",
        "        trans_for.eval()\n",
        "    else:\n",
        "        trans_for.train()\n",
        "\n",
        "\n",
        "    loss_sum = 0.0\n",
        "    acc_sum  = 0\n",
        "    n_samp   = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y_idx = batch_y.to(device).long()\n",
        "\n",
        "        clean = model(batch_x)\n",
        "        noisy_1h = torch.zeros(batch_x.size(0), Cfg.num_classes, device=device).scatter_(1, batch_y_idx.view(-1,1), 1)\n",
        "\n",
        "        # ------- L1: forward-------\n",
        "        T_for = trans_for()\n",
        "        T_back = trans_back()\n",
        "\n",
        "        T_for  = row_stoch(T_for)\n",
        "        T_back = row_stoch(T_back)\n",
        "\n",
        "        p_noisy_pred = (clean @ T_for).clamp_min(Cfg.eps)\n",
        "        L1 = loss_func_ce(p_noisy_pred.log(), batch_y_idx)\n",
        "\n",
        "        # ------- L2: backward-------\n",
        "        p_clean_from_noisy = (noisy_1h @ T_back).clamp_min(Cfg.eps)\n",
        "        L2 = -(p_clean_from_noisy * clean.clamp_min(Cfg.eps).log()).sum(dim=1).mean()\n",
        "\n",
        "        # ------- L3: cycle-consistency -------\n",
        "        T_for_det  = T_for.detach()\n",
        "        T_back_det = T_back.detach()\n",
        "        p_cycle = ((clean @ T_for_det).clamp_min(Cfg.eps) @ T_back_det).clamp_min(Cfg.eps)\n",
        "        L3 = -(clean * p_cycle.log()).sum(dim=1).mean()\n",
        "\n",
        "        # ------- L4: reverse correction -------\n",
        "        I = torch.eye(Cfg.num_classes, device=device)\n",
        "        R_inv = (row_stoch(T_back) @ row_stoch(T_for).detach() - I).abs().mean()\n",
        "\n",
        "        loss = L1 + L2 + Cfg.w_cycle * L3 + Cfg.r_inv * R_inv\n",
        "\n",
        "        pred_noisy = p_noisy_pred.argmax(dim=1)\n",
        "        acc_sum += (pred_noisy == batch_y_idx).sum().item()\n",
        "\n",
        "\n",
        "        optimizer_es.zero_grad()\n",
        "        if optimizer_trans_for is not None:\n",
        "            optimizer_trans_for.zero_grad()\n",
        "        optimizer_trans_back.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_es.step()\n",
        "        if optimizer_trans_for is not None:\n",
        "            optimizer_trans_for.step()\n",
        "        optimizer_trans_back.step()\n",
        "\n",
        "        bs = batch_x.size(0)\n",
        "        loss_sum += loss.item() * bs\n",
        "        n_samp += bs\n",
        "\n",
        "    print('Train Loss: {:.6f},  Acc: {:.6f}'.format(loss_sum / max(1, n_samp), acc_sum / max(1, n_samp)))  # [FIX]\n",
        "\n",
        "    scheduler1.step()\n",
        "    if scheduler2 is not None:\n",
        "        scheduler2.step()\n",
        "    scheduler3.step()\n",
        "\n",
        "\n",
        "def val(val_data, val_loader, model, trans, loss_func_ce):\n",
        "    val_correct = 0\n",
        "    val_loss_sum = 0.0\n",
        "    total_n = 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        trans.eval()\n",
        "\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            clean = model(batch_x)\n",
        "            T_back = trans()\n",
        "            T_back = row_stoch(T_back)\n",
        "            #T_back = T_back / (T_back.sum(dim=0, keepdim=True) + 1e-8)\n",
        "\n",
        "            noisy_1h = torch.zeros(batch_x.size(0), Cfg.num_classes, device=device).scatter_(1, batch_y.view(-1,1), 1)\n",
        "\n",
        "\n",
        "            est_clean = torch.mm(noisy_1h, T_back).clamp_min(1e-8)\n",
        "            est_clean_y = est_clean.argmax(dim=1)\n",
        "\n",
        "\n",
        "            loss = -(est_clean * clean.clamp_min(1e-8).log()).sum(dim=1).mean()\n",
        "\n",
        "            pred_clean = clean.argmax(dim=1)\n",
        "            est_clean_y = est_clean.argmax(dim=1)\n",
        "            bs = batch_x.size(0)\n",
        "            val_correct += (pred_clean == est_clean_y).sum().item()\n",
        "            val_loss_sum += loss.item() * bs\n",
        "            total_n += bs\n",
        "\n",
        "    avg_val_loss = val_loss_sum / max(1, total_n)\n",
        "    avg_val_acc  = val_correct / max(1, total_n)\n",
        "    print('Val Loss: {:.6f}, Acc: {:.6f}'.format(avg_val_loss, avg_val_acc))\n",
        "    return avg_val_loss\n",
        "\n",
        "\n",
        "def test(test_data, test_loader, model, loss_func_ce):\n",
        "    eval_loss = 0.\n",
        "    eval_acc = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            clean = model(batch_x)\n",
        "\n",
        "            loss = loss_func_ce(clean.clamp_min(1e-8).log(), batch_y.long())\n",
        "            eval_loss += loss.item()\n",
        "            pred = torch.max(clean, 1)[1]\n",
        "            eval_correct = (pred == batch_y).sum()\n",
        "            eval_acc += eval_correct.item()\n",
        "\n",
        "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(test_data)) * Cfg.batch_size,\n",
        "                                                      eval_acc / (len(test_data))))\n",
        "    return eval_acc / (len(test_data))\n"
      ],
      "id": "9d601f0fc872489f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operation process"
      ],
      "metadata": {
        "id": "zXPznLPpoHWo"
      },
      "id": "zXPznLPpoHWo"
    },
    {
      "metadata": {
        "id": "8dbbdbdfbb271868"
      },
      "cell_type": "code",
      "source": [
        "def main(dataset_name):\n",
        "\n",
        "      try:\n",
        "          del model\n",
        "      except:\n",
        "          pass\n",
        "      try:\n",
        "          del trans; del trans_1\n",
        "      except:\n",
        "          pass\n",
        "      gc.collect()\n",
        "      if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "      best_Tf_np = None\n",
        "      best_Tb_np = None\n",
        "      best_epoch_tag = None\n",
        "\n",
        "\n",
        "\n",
        "      #init\n",
        "      criterion = torch.nn.NLLLoss(reduction=\"mean\")\n",
        "      model = ResNet_tiny(Cfg.num_classes, dataset_name)\n",
        "      milestones = Cfg.milestones\n",
        "\n",
        "      if dataset_name.lower() in (\"mnist03\"):\n",
        "          Xtr, ytr, Xva, yva, Xte, yte = reshape_mnist(*load_dataset(\"datasets/FashionMNIST0.3.npz\"))\n",
        "      elif dataset_name.lower() in (\"mnist06\"):\n",
        "          Xtr, ytr, Xva, yva, Xte, yte = reshape_mnist(*load_dataset(\"datasets/FashionMNIST0.6.npz\"))\n",
        "      elif dataset_name.lower() in (\"cifar\"):\n",
        "          Xtr, ytr, Xva, yva, Xte, yte = reshape_cifar(*load_dataset(\"datasets/CIFAR.npz\"))\n",
        "      else:\n",
        "          raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
        "\n",
        "      train_data = TensorDataset(Xtr, ytr)\n",
        "      val_data = TensorDataset(Xva, yva)\n",
        "      test_data = TensorDataset(Xte, yte)\n",
        "\n",
        "      trans_known = select_known_T(dataset_name) if Cfg.use_true_T_forward else None\n",
        "      has_clean_T = trans_known is not None\n",
        "\n",
        "      if has_clean_T:\n",
        "          print(f\"[Info] Using provided true transition matrix for {dataset_name}\")\n",
        "          trans = trans_known\n",
        "          optimizer_trans = None\n",
        "          scheduler2 = None\n",
        "      else:\n",
        "          trans = sig_t(device, Cfg.num_classes)\n",
        "          optimizer_trans = optim.AdamW(trans.parameters(), lr=Cfg.lr, weight_decay=0)\n",
        "          scheduler2 = MultiStepLR(optimizer_trans, milestones=milestones, gamma=0.1)\n",
        "\n",
        "\n",
        "      trans_1 = sig_t(device, Cfg.num_classes)\n",
        "      optimizer_trans_1 = optim.AdamW(trans_1.parameters(), lr=Cfg.lr, weight_decay=0)\n",
        "\n",
        "\n",
        "      #optimizer and StepLR\n",
        "      optimizer_es = optim.AdamW(model.parameters(), lr=Cfg.lr, weight_decay=Cfg.weight_decay)\n",
        "      scheduler1 = MultiStepLR(optimizer_es, milestones=milestones, gamma=0.1)\n",
        "      scheduler3 = MultiStepLR(optimizer_trans_1, milestones=milestones, gamma=0.1)\n",
        "\n",
        "\n",
        "      #data_loader\n",
        "      train_loader = DataLoader(dataset=train_data,\n",
        "                                batch_size=Cfg.batch_size,\n",
        "                                shuffle=True,\n",
        "                                num_workers=4,\n",
        "                                drop_last=False)\n",
        "\n",
        "      val_loader = DataLoader(dataset=val_data,\n",
        "                              batch_size=Cfg.batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=4,\n",
        "                              drop_last=False)\n",
        "\n",
        "      test_loader = DataLoader(dataset=test_data,\n",
        "                              batch_size=Cfg.batch_size,\n",
        "                              num_workers=4,\n",
        "                              drop_last=False)\n",
        "\n",
        "      #cuda\n",
        "      if torch.cuda.is_available():\n",
        "          model = model.to(device)\n",
        "          trans = trans.to(device)\n",
        "          trans_1 = trans_1.to(device)\n",
        "\n",
        "\n",
        "      best_acc = 0\n",
        "      best_warm_up_acc = 0\n",
        "      best_acc_back = 0\n",
        "      #warmup\n",
        "      for epoch in range(Cfg.warmup_epoch):\n",
        "          print('epoch[{}], Warmup'.format(epoch + 1))\n",
        "          warmup(train_data, train_loader, model,optimizer_es, criterion)\n",
        "          val(val_data, val_loader, model, trans_1, criterion)\n",
        "          acc = test(test_data, test_loader, model,criterion)\n",
        "          if acc> best_warm_up_acc:\n",
        "              best_warm_up_acc = acc\n",
        "          print('Best_acc: {:.6f}'.format(best_warm_up_acc))\n",
        "\n",
        "      acc_list = []\n",
        "      loss_list = []\n",
        "      early_stop = 0\n",
        "      best_val_loss = float('inf')\n",
        "      best_Tf_np, best_Tb_np, best_epoch_tag = None, None, None\n",
        "\n",
        "      #train\n",
        "      for epoch in range(Cfg.n_epoch):\n",
        "          print('epoch[{}], Train'.format(epoch+1))\n",
        "          train(train_data,train_loader,model,trans,trans_1,optimizer_es,optimizer_trans,optimizer_trans_1,scheduler1,scheduler2,scheduler3,criterion)\n",
        "          val_loss = val(val_data, val_loader, model, trans_1,criterion)\n",
        "          acc = test(test_data, test_loader, model,criterion)\n",
        "\n",
        "          acc_list.append(acc)\n",
        "          loss_list.append(val_loss)\n",
        "\n",
        "          improved = val_loss < best_val_loss - 1e-6\n",
        "          best_acc = max(best_acc, acc)\n",
        "\n",
        "          if improved:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch_tag = epoch + 1\n",
        "            with torch.no_grad():\n",
        "              Tf_use = row_stoch(trans().detach().float().cpu()).numpy()\n",
        "              Tb_use = row_stoch(trans_1().detach().float().cpu()).numpy()\n",
        "              best_Tf_np, best_Tb_np = Tf_use, Tb_use\n",
        "            early_stop = 0\n",
        "            print('Best_acc: {:.6f}'.format(best_acc))\n",
        "          else:\n",
        "            early_stop += 1\n",
        "            print('Best_acc: {:.6f}'.format(best_acc))\n",
        "            if early_stop >= Cfg.es_epoch:\n",
        "              print(f\" Early stopping at epoch {epoch+1} (best val_loss={best_val_loss:.6f}, best acc={best_acc:.6f})\")\n",
        "              break\n",
        "\n",
        "      if best_Tf_np is not None:\n",
        "        Tf_np, Tb_np = best_Tf_np, best_Tb_np\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          Tf_np = row_stoch(trans().detach().float().cpu()).numpy()\n",
        "          Tb_np = row_stoch(trans_1().detach().float().cpu()).numpy()\n",
        "\n",
        "      print('Best_acc: ', best_acc)\n",
        "\n",
        "      return best_acc,acc_list,loss_list,Tf_np, Tb_np\n"
      ],
      "id": "8dbbdbdfbb271868",
      "outputs": [],
      "execution_count": 56
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running"
      ],
      "metadata": {
        "id": "KgVDOhM2nJG6"
      },
      "id": "KgVDOhM2nJG6"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-26T05:55:11.786416Z",
          "start_time": "2025-10-26T05:55:11.770417Z"
        },
        "id": "fbd158abbbbfb37f"
      },
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "@dataclass\n",
        "class Cfg:\n",
        "    num_classes: int = 3\n",
        "    batch_size: int = 128\n",
        "    n_epoch: int = 120\n",
        "    warmup_epoch: int = 5\n",
        "    es_epoch: int = 40\n",
        "    eps: float = 1e-8\n",
        "    lr: float = 0.005\n",
        "    weight_decay: float = 1e-5\n",
        "    milestones: Tuple[int, int] = (20, 40)\n",
        "    w_cycle: float = 0.2\n",
        "    use_true_T_forward: bool = True\n",
        "    r_inv: float = 0\n",
        "    anchor: bool = False\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "id": "fbd158abbbbfb37f",
      "outputs": [],
      "execution_count": 86
    },
    {
      "cell_type": "code",
      "source": [
        "best_list = []\n",
        "dataset_list = ['mnist03','mnist06','cifar']\n",
        "os.makedirs('saved', exist_ok=True)\n",
        "\n",
        "for dataset_name in dataset_list:\n",
        "    Tf_list, Tb_list, best_sublist = [], [], []\n",
        "\n",
        "    for run_id in range(10):\n",
        "        with open(os.devnull, \"w\") as devnull:\n",
        "            with redirect_stdout(devnull), redirect_stderr(devnull):\n",
        "                best, acc_list, loss_list, Tf, Tb = main(dataset_name)\n",
        "        best_sublist.append(best)\n",
        "        Tf_list.append(np.array(Tf))\n",
        "        Tb_list.append(np.array(Tb))\n",
        "        best_list.append([dataset_name, run_id+1, best])\n",
        "\n",
        "    # ===  Tf/Tb ===\n",
        "    Tf_mean = np.mean(Tf_list, axis=0)\n",
        "    Tb_mean = np.mean(Tb_list, axis=0)\n",
        "    np.save(f'saved/{dataset_name}_Tf.npy', Tf_mean)\n",
        "    np.save(f'saved/{dataset_name}_Tb.npy', Tb_mean)\n",
        "\n",
        "    #  scsv\n",
        "    np.savetxt(f'saved/{dataset_name}_Tf.scsv', Tf_mean, delimiter=';')\n",
        "    np.savetxt(f'saved/{dataset_name}_Tb.scsv', Tb_mean, delimiter=';')\n",
        "\n",
        "# ===  run  best ===\n",
        "with open('saved/best_results.scsv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter=';')\n",
        "    writer.writerow(['name', 'run', 'best'])\n",
        "    writer.writerows(best_list)\n",
        "\n",
        "print('  bestTfTb .npy  .scsv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "U7LISbO23Ezy",
        "outputId": "1da0fcb7-c4fa-4923-c74a-084d52f7affc"
      },
      "id": "U7LISbO23Ezy",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2576532480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdevnull\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirect_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevnull\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbest_sublist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mTf_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1532858132.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch[{}], Train'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m           \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrans_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_es\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_trans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer_trans_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m           \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m           \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3696459444.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, train_loader, model, trans_for, trans_back, optimizer_es, optimizer_trans_for, optimizer_trans_back, scheduler1, scheduler2, scheduler3, loss_func_ce)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mT_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mT_for\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrow_stoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_for\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mT_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_stoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1832865544.py\u001b[0m in \u001b[0;36mrow_stoch\u001b[0;34m(M, eps)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrow_stoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-10-26T14:07:58.556750Z",
          "start_time": "2025-10-26T14:07:58.542750Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2470ffbd9b77e4b0",
        "outputId": "1f81c913-727c-498b-be77-70fc54e01ab0"
      },
      "cell_type": "code",
      "source": [
        "dataset_list = ['mnist03','mnist06','cifar']\n",
        "best, acc_list, loss_list, Tf, Tb = main(dataset_name)"
      ],
      "id": "2470ffbd9b77e4b0",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.118515\n",
            "Val Loss: 1.098326, Acc: 0.365333\n",
            "Test Loss: 1.089330, Acc: 0.439667\n",
            "Best_acc: 0.439667\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.102265\n",
            "Val Loss: 1.096343, Acc: 0.357000\n",
            "Test Loss: 1.072486, Acc: 0.464000\n",
            "Best_acc: 0.464000\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.099917\n",
            "Val Loss: 1.097188, Acc: 0.376000\n",
            "Test Loss: 1.063416, Acc: 0.538000\n",
            "Best_acc: 0.538000\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.101200\n",
            "Val Loss: 1.096633, Acc: 0.374667\n",
            "Test Loss: 1.094055, Acc: 0.522000\n",
            "Best_acc: 0.538000\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.097971\n",
            "Val Loss: 1.096580, Acc: 0.370667\n",
            "Test Loss: 1.077343, Acc: 0.536000\n",
            "Best_acc: 0.538000\n",
            "epoch[1], Train\n",
            "Train Loss: 2.408884,  Acc: 0.373417\n",
            "Val Loss: 1.095944, Acc: 0.372333\n",
            "Test Loss: 1.050186, Acc: 0.571667\n",
            "Best_acc: 0.571667\n",
            "epoch[2], Train\n",
            "Train Loss: 2.407580,  Acc: 0.372500\n",
            "Val Loss: 1.096197, Acc: 0.372333\n",
            "Test Loss: 1.063365, Acc: 0.627333\n",
            "Best_acc: 0.627333\n",
            "epoch[3], Train\n",
            "Train Loss: 2.405378,  Acc: 0.373333\n",
            "Val Loss: 1.095099, Acc: 0.363333\n",
            "Test Loss: 1.092268, Acc: 0.498667\n",
            "Best_acc: 0.627333\n",
            "epoch[4], Train\n",
            "Train Loss: 2.405818,  Acc: 0.370333\n",
            "Val Loss: 1.093520, Acc: 0.379333\n",
            "Test Loss: 1.045705, Acc: 0.593333\n",
            "Best_acc: 0.627333\n",
            "epoch[5], Train\n",
            "Train Loss: 2.402486,  Acc: 0.377500\n",
            "Val Loss: 1.094874, Acc: 0.375000\n",
            "Test Loss: 1.061076, Acc: 0.540667\n",
            "Best_acc: 0.627333\n",
            "epoch[6], Train\n",
            "Train Loss: 2.402362,  Acc: 0.370667\n",
            "Val Loss: 1.094065, Acc: 0.366333\n",
            "Test Loss: 1.047755, Acc: 0.549000\n",
            "Best_acc: 0.627333\n",
            "epoch[7], Train\n",
            "Train Loss: 2.400386,  Acc: 0.375333\n",
            "Val Loss: 1.089765, Acc: 0.378667\n",
            "Test Loss: 1.058369, Acc: 0.537667\n",
            "Best_acc: 0.627333\n",
            "epoch[8], Train\n",
            "Train Loss: 2.395355,  Acc: 0.379000\n",
            "Val Loss: 1.083440, Acc: 0.376667\n",
            "Test Loss: 1.056985, Acc: 0.519333\n",
            "Best_acc: 0.627333\n",
            "epoch[9], Train\n",
            "Train Loss: 2.386507,  Acc: 0.378750\n",
            "Val Loss: 1.079844, Acc: 0.360667\n",
            "Test Loss: 1.061247, Acc: 0.436000\n",
            "Best_acc: 0.627333\n",
            "epoch[10], Train\n",
            "Train Loss: 2.376564,  Acc: 0.376333\n",
            "Val Loss: 1.068090, Acc: 0.343333\n",
            "Test Loss: 1.070733, Acc: 0.394000\n",
            "Best_acc: 0.627333\n",
            "epoch[11], Train\n",
            "Train Loss: 2.366158,  Acc: 0.372833\n",
            "Val Loss: 1.062089, Acc: 0.337333\n",
            "Test Loss: 1.078655, Acc: 0.377333\n",
            "Best_acc: 0.627333\n",
            "epoch[12], Train\n",
            "Train Loss: 2.353303,  Acc: 0.377500\n",
            "Val Loss: 1.055479, Acc: 0.334000\n",
            "Test Loss: 1.071275, Acc: 0.378000\n",
            "Best_acc: 0.627333\n",
            "epoch[13], Train\n",
            "Train Loss: 2.347891,  Acc: 0.380250\n",
            "Val Loss: 1.052663, Acc: 0.323667\n",
            "Test Loss: 1.110403, Acc: 0.340667\n",
            "Best_acc: 0.627333\n",
            "epoch[14], Train\n",
            "Train Loss: 2.344682,  Acc: 0.381333\n",
            "Val Loss: 1.051999, Acc: 0.321333\n",
            "Test Loss: 1.115319, Acc: 0.335000\n",
            "Best_acc: 0.627333\n",
            "epoch[15], Train\n",
            "Train Loss: 2.341792,  Acc: 0.381333\n",
            "Val Loss: 1.051769, Acc: 0.323333\n",
            "Test Loss: 1.101753, Acc: 0.338000\n",
            "Best_acc: 0.627333\n",
            "epoch[16], Train\n",
            "Train Loss: 2.339747,  Acc: 0.386583\n",
            "Val Loss: 1.049620, Acc: 0.330333\n",
            "Test Loss: 1.097122, Acc: 0.347000\n",
            "Best_acc: 0.627333\n",
            "epoch[17], Train\n",
            "Train Loss: 2.337565,  Acc: 0.381583\n",
            "Val Loss: 1.052716, Acc: 0.327000\n",
            "Test Loss: 1.094060, Acc: 0.339000\n",
            "Best_acc: 0.627333\n",
            "epoch[18], Train\n",
            "Train Loss: 2.336276,  Acc: 0.386583\n",
            "Val Loss: 1.055575, Acc: 0.321667\n",
            "Test Loss: 1.124999, Acc: 0.334000\n",
            "Best_acc: 0.627333\n",
            "epoch[19], Train\n",
            "Train Loss: 2.334432,  Acc: 0.385167\n",
            "Val Loss: 1.047624, Acc: 0.336333\n",
            "Test Loss: 1.099048, Acc: 0.376667\n",
            "Best_acc: 0.627333\n",
            "epoch[20], Train\n",
            "Train Loss: 2.334885,  Acc: 0.389417\n",
            "Val Loss: 1.054776, Acc: 0.330000\n",
            "Test Loss: 1.101879, Acc: 0.356667\n",
            "Best_acc: 0.627333\n",
            "epoch[21], Train\n",
            "Train Loss: 2.330268,  Acc: 0.391333\n",
            "Val Loss: 1.050600, Acc: 0.326333\n",
            "Test Loss: 1.099793, Acc: 0.346667\n",
            "Best_acc: 0.627333\n",
            "epoch[22], Train\n",
            "Train Loss: 2.328433,  Acc: 0.394917\n",
            "Val Loss: 1.050029, Acc: 0.327667\n",
            "Test Loss: 1.100346, Acc: 0.350000\n",
            "Best_acc: 0.627333\n",
            "epoch[23], Train\n",
            "Train Loss: 2.327599,  Acc: 0.392917\n",
            "Val Loss: 1.049992, Acc: 0.331333\n",
            "Test Loss: 1.093914, Acc: 0.358333\n",
            "Best_acc: 0.627333\n",
            "epoch[24], Train\n",
            "Train Loss: 2.326061,  Acc: 0.395250\n",
            "Val Loss: 1.049520, Acc: 0.330667\n",
            "Test Loss: 1.093318, Acc: 0.355667\n",
            "Best_acc: 0.627333\n",
            "epoch[25], Train\n",
            "Train Loss: 2.325322,  Acc: 0.394667\n",
            "Val Loss: 1.050659, Acc: 0.333000\n",
            "Test Loss: 1.092936, Acc: 0.361333\n",
            "Best_acc: 0.627333\n",
            "epoch[26], Train\n",
            "Train Loss: 2.324849,  Acc: 0.396667\n",
            "Val Loss: 1.050252, Acc: 0.332333\n",
            "Test Loss: 1.090087, Acc: 0.361000\n",
            "Best_acc: 0.627333\n",
            "epoch[27], Train\n",
            "Train Loss: 2.323969,  Acc: 0.397333\n",
            "Val Loss: 1.049511, Acc: 0.334667\n",
            "Test Loss: 1.090193, Acc: 0.370333\n",
            "Best_acc: 0.627333\n",
            "epoch[28], Train\n",
            "Train Loss: 2.322825,  Acc: 0.403250\n",
            "Val Loss: 1.052377, Acc: 0.334667\n",
            "Test Loss: 1.097367, Acc: 0.366333\n",
            "Best_acc: 0.627333\n",
            "epoch[29], Train\n",
            "Train Loss: 2.322010,  Acc: 0.403583\n",
            "Val Loss: 1.050250, Acc: 0.327667\n",
            "Test Loss: 1.098700, Acc: 0.353667\n",
            "Best_acc: 0.627333\n",
            "epoch[30], Train\n",
            "Train Loss: 2.320713,  Acc: 0.405667\n",
            "Val Loss: 1.052045, Acc: 0.335000\n",
            "Test Loss: 1.083904, Acc: 0.373667\n",
            "Best_acc: 0.627333\n",
            "epoch[31], Train\n",
            "Train Loss: 2.319843,  Acc: 0.402417\n",
            "Val Loss: 1.056391, Acc: 0.335333\n",
            "Test Loss: 1.096476, Acc: 0.374333\n",
            "Best_acc: 0.627333\n",
            "epoch[32], Train\n",
            "Train Loss: 2.319621,  Acc: 0.405667\n",
            "Val Loss: 1.052489, Acc: 0.329667\n",
            "Test Loss: 1.090449, Acc: 0.363667\n",
            "Best_acc: 0.627333\n",
            "epoch[33], Train\n",
            "Train Loss: 2.318565,  Acc: 0.403583\n",
            "Val Loss: 1.054903, Acc: 0.338333\n",
            "Test Loss: 1.092121, Acc: 0.379000\n",
            "Best_acc: 0.627333\n",
            "epoch[34], Train\n",
            "Train Loss: 2.317669,  Acc: 0.406917\n",
            "Val Loss: 1.053941, Acc: 0.330667\n",
            "Test Loss: 1.102644, Acc: 0.363667\n",
            "Best_acc: 0.627333\n",
            "epoch[35], Train\n",
            "Train Loss: 2.316724,  Acc: 0.409250\n",
            "Val Loss: 1.052223, Acc: 0.331667\n",
            "Test Loss: 1.115682, Acc: 0.363000\n",
            "Best_acc: 0.627333\n",
            "epoch[36], Train\n",
            "Train Loss: 2.314975,  Acc: 0.406833\n",
            "Val Loss: 1.051900, Acc: 0.332000\n",
            "Test Loss: 1.116884, Acc: 0.362000\n",
            "Best_acc: 0.627333\n",
            "epoch[37], Train\n",
            "Train Loss: 2.312758,  Acc: 0.416333\n",
            "Val Loss: 1.057848, Acc: 0.333000\n",
            "Test Loss: 1.100573, Acc: 0.360333\n",
            "Best_acc: 0.627333\n",
            "epoch[38], Train\n",
            "Train Loss: 2.311600,  Acc: 0.408750\n",
            "Val Loss: 1.058396, Acc: 0.342333\n",
            "Test Loss: 1.102086, Acc: 0.394000\n",
            "Best_acc: 0.627333\n",
            "epoch[39], Train\n",
            "Train Loss: 2.310308,  Acc: 0.417083\n",
            "Val Loss: 1.057142, Acc: 0.353000\n",
            "Test Loss: 1.059522, Acc: 0.421333\n",
            "Best_acc: 0.627333\n",
            "epoch[40], Train\n",
            "Train Loss: 2.308865,  Acc: 0.419500\n",
            "Val Loss: 1.053465, Acc: 0.341333\n",
            "Test Loss: 1.109165, Acc: 0.370000\n",
            "Best_acc: 0.627333\n",
            "epoch[41], Train\n",
            "Train Loss: 2.301163,  Acc: 0.426833\n",
            "Val Loss: 1.054198, Acc: 0.348667\n",
            "Test Loss: 1.091317, Acc: 0.390000\n",
            "Best_acc: 0.627333\n",
            "epoch[42], Train\n",
            "Train Loss: 2.299735,  Acc: 0.425250\n",
            "Val Loss: 1.055026, Acc: 0.345000\n",
            "Test Loss: 1.098446, Acc: 0.386333\n",
            "Best_acc: 0.627333\n",
            "epoch[43], Train\n",
            "Train Loss: 2.297368,  Acc: 0.429417\n",
            "Val Loss: 1.055664, Acc: 0.345333\n",
            "Test Loss: 1.101083, Acc: 0.385667\n",
            "Best_acc: 0.627333\n",
            "epoch[44], Train\n",
            "Train Loss: 2.297794,  Acc: 0.428333\n",
            "Val Loss: 1.055745, Acc: 0.343333\n",
            "Test Loss: 1.098933, Acc: 0.381333\n",
            "Best_acc: 0.627333\n",
            "epoch[45], Train\n",
            "Train Loss: 2.297591,  Acc: 0.426500\n",
            "Val Loss: 1.055549, Acc: 0.350333\n",
            "Test Loss: 1.094267, Acc: 0.395667\n",
            "Best_acc: 0.627333\n",
            "epoch[46], Train\n",
            "Train Loss: 2.296831,  Acc: 0.427583\n",
            "Val Loss: 1.056268, Acc: 0.345333\n",
            "Test Loss: 1.100486, Acc: 0.382000\n",
            "Best_acc: 0.627333\n",
            "epoch[47], Train\n",
            "Train Loss: 2.296035,  Acc: 0.430583\n",
            "Val Loss: 1.056830, Acc: 0.343667\n",
            "Test Loss: 1.102218, Acc: 0.383000\n",
            "Best_acc: 0.627333\n",
            "epoch[48], Train\n",
            "Train Loss: 2.295440,  Acc: 0.428417\n",
            "Val Loss: 1.057907, Acc: 0.343000\n",
            "Test Loss: 1.103754, Acc: 0.381000\n",
            "Best_acc: 0.627333\n",
            "epoch[49], Train\n",
            "Train Loss: 2.295415,  Acc: 0.430583\n",
            "Val Loss: 1.056963, Acc: 0.346000\n",
            "Test Loss: 1.100792, Acc: 0.382333\n",
            "Best_acc: 0.627333\n",
            " Early stopping at epoch 49 (best val_loss=1.047624, best acc=0.627333)\n",
            "Best_acc:  0.6273333333333333\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.116543\n",
            "Val Loss: 1.096192, Acc: 0.377333\n",
            "Test Loss: 1.047411, Acc: 0.623000\n",
            "Best_acc: 0.623000\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.103345\n",
            "Val Loss: 1.094835, Acc: 0.384333\n",
            "Test Loss: 1.033169, Acc: 0.617000\n",
            "Best_acc: 0.623000\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.099790\n",
            "Val Loss: 1.103721, Acc: 0.380333\n",
            "Test Loss: 1.034377, Acc: 0.580000\n",
            "Best_acc: 0.623000\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.098906\n",
            "Val Loss: 1.099220, Acc: 0.377000\n",
            "Test Loss: 1.033666, Acc: 0.597667\n",
            "Best_acc: 0.623000\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.099836\n",
            "Val Loss: 1.094515, Acc: 0.373000\n",
            "Test Loss: 1.076511, Acc: 0.416000\n",
            "Best_acc: 0.623000\n",
            "epoch[1], Train\n",
            "Train Loss: 2.411376,  Acc: 0.365667\n",
            "Val Loss: 1.097368, Acc: 0.364333\n",
            "Test Loss: 1.057959, Acc: 0.594000\n",
            "Best_acc: 0.594000\n",
            "epoch[2], Train\n",
            "Train Loss: 2.407784,  Acc: 0.369417\n",
            "Val Loss: 1.092938, Acc: 0.365667\n",
            "Test Loss: 1.089139, Acc: 0.476333\n",
            "Best_acc: 0.594000\n",
            "epoch[3], Train\n",
            "Train Loss: 2.406028,  Acc: 0.369333\n",
            "Val Loss: 1.088619, Acc: 0.388333\n",
            "Test Loss: 1.029100, Acc: 0.618667\n",
            "Best_acc: 0.618667\n",
            "epoch[4], Train\n",
            "Train Loss: 2.402262,  Acc: 0.370917\n",
            "Val Loss: 1.090437, Acc: 0.380333\n",
            "Test Loss: 1.053147, Acc: 0.637000\n",
            "Best_acc: 0.637000\n",
            "epoch[5], Train\n",
            "Train Loss: 2.399845,  Acc: 0.378417\n",
            "Val Loss: 1.092220, Acc: 0.389667\n",
            "Test Loss: 1.017070, Acc: 0.642333\n",
            "Best_acc: 0.642333\n",
            "epoch[6], Train\n",
            "Train Loss: 2.399657,  Acc: 0.377583\n",
            "Val Loss: 1.092053, Acc: 0.377667\n",
            "Test Loss: 1.034804, Acc: 0.580000\n",
            "Best_acc: 0.642333\n",
            "epoch[7], Train\n",
            "Train Loss: 2.398257,  Acc: 0.381083\n",
            "Val Loss: 1.088082, Acc: 0.391000\n",
            "Test Loss: 1.026367, Acc: 0.663333\n",
            "Best_acc: 0.663333\n",
            "epoch[8], Train\n",
            "Train Loss: 2.396299,  Acc: 0.375583\n",
            "Val Loss: 1.085431, Acc: 0.377667\n",
            "Test Loss: 1.033281, Acc: 0.596333\n",
            "Best_acc: 0.663333\n",
            "epoch[9], Train\n",
            "Train Loss: 2.388413,  Acc: 0.379250\n",
            "Val Loss: 1.076241, Acc: 0.361000\n",
            "Test Loss: 1.042374, Acc: 0.478000\n",
            "Best_acc: 0.663333\n",
            "epoch[10], Train\n",
            "Train Loss: 2.373996,  Acc: 0.374083\n",
            "Val Loss: 1.065610, Acc: 0.333333\n",
            "Test Loss: 1.079964, Acc: 0.359333\n",
            "Best_acc: 0.663333\n",
            "epoch[11], Train\n",
            "Train Loss: 2.361800,  Acc: 0.371333\n",
            "Val Loss: 1.060576, Acc: 0.334333\n",
            "Test Loss: 1.051631, Acc: 0.389000\n",
            "Best_acc: 0.663333\n",
            "epoch[12], Train\n",
            "Train Loss: 2.352261,  Acc: 0.374917\n",
            "Val Loss: 1.063831, Acc: 0.328000\n",
            "Test Loss: 1.129250, Acc: 0.338000\n",
            "Best_acc: 0.663333\n",
            "epoch[13], Train\n",
            "Train Loss: 2.348528,  Acc: 0.376500\n",
            "Val Loss: 1.055684, Acc: 0.322333\n",
            "Test Loss: 1.091423, Acc: 0.335000\n",
            "Best_acc: 0.663333\n",
            "epoch[14], Train\n",
            "Train Loss: 2.341413,  Acc: 0.381333\n",
            "Val Loss: 1.064140, Acc: 0.326333\n",
            "Test Loss: 1.119121, Acc: 0.349333\n",
            "Best_acc: 0.663333\n",
            "epoch[15], Train\n",
            "Train Loss: 2.338655,  Acc: 0.380250\n",
            "Val Loss: 1.051982, Acc: 0.328333\n",
            "Test Loss: 1.085077, Acc: 0.357000\n",
            "Best_acc: 0.663333\n",
            "epoch[16], Train\n",
            "Train Loss: 2.336951,  Acc: 0.384000\n",
            "Val Loss: 1.051994, Acc: 0.323000\n",
            "Test Loss: 1.095095, Acc: 0.337667\n",
            "Best_acc: 0.663333\n",
            "epoch[17], Train\n",
            "Train Loss: 2.336876,  Acc: 0.386417\n",
            "Val Loss: 1.054692, Acc: 0.323667\n",
            "Test Loss: 1.100005, Acc: 0.338667\n",
            "Best_acc: 0.663333\n",
            "epoch[18], Train\n",
            "Train Loss: 2.334832,  Acc: 0.384750\n",
            "Val Loss: 1.057306, Acc: 0.323333\n",
            "Test Loss: 1.104050, Acc: 0.340333\n",
            "Best_acc: 0.663333\n",
            "epoch[19], Train\n",
            "Train Loss: 2.333434,  Acc: 0.388833\n",
            "Val Loss: 1.049293, Acc: 0.327333\n",
            "Test Loss: 1.088142, Acc: 0.349000\n",
            "Best_acc: 0.663333\n",
            "epoch[20], Train\n",
            "Train Loss: 2.332368,  Acc: 0.390000\n",
            "Val Loss: 1.052383, Acc: 0.322333\n",
            "Test Loss: 1.095460, Acc: 0.336000\n",
            "Best_acc: 0.663333\n",
            "epoch[21], Train\n",
            "Train Loss: 2.325177,  Acc: 0.400333\n",
            "Val Loss: 1.052283, Acc: 0.323667\n",
            "Test Loss: 1.078487, Acc: 0.340333\n",
            "Best_acc: 0.663333\n",
            "epoch[22], Train\n",
            "Train Loss: 2.323055,  Acc: 0.398417\n",
            "Val Loss: 1.051548, Acc: 0.329333\n",
            "Test Loss: 1.070150, Acc: 0.352333\n",
            "Best_acc: 0.663333\n",
            "epoch[23], Train\n",
            "Train Loss: 2.321339,  Acc: 0.401167\n",
            "Val Loss: 1.052672, Acc: 0.328000\n",
            "Test Loss: 1.071121, Acc: 0.350000\n",
            "Best_acc: 0.663333\n",
            "epoch[24], Train\n",
            "Train Loss: 2.320440,  Acc: 0.402167\n",
            "Val Loss: 1.054846, Acc: 0.328000\n",
            "Test Loss: 1.070382, Acc: 0.347333\n",
            "Best_acc: 0.663333\n",
            "epoch[25], Train\n",
            "Train Loss: 2.319116,  Acc: 0.403000\n",
            "Val Loss: 1.053859, Acc: 0.329000\n",
            "Test Loss: 1.074752, Acc: 0.350333\n",
            "Best_acc: 0.663333\n",
            "epoch[26], Train\n",
            "Train Loss: 2.318266,  Acc: 0.405167\n",
            "Val Loss: 1.053328, Acc: 0.329333\n",
            "Test Loss: 1.071153, Acc: 0.352667\n",
            "Best_acc: 0.663333\n",
            "epoch[27], Train\n",
            "Train Loss: 2.317207,  Acc: 0.404250\n",
            "Val Loss: 1.053683, Acc: 0.329333\n",
            "Test Loss: 1.073562, Acc: 0.356667\n",
            "Best_acc: 0.663333\n",
            "epoch[28], Train\n",
            "Train Loss: 2.316768,  Acc: 0.404083\n",
            "Val Loss: 1.055309, Acc: 0.329667\n",
            "Test Loss: 1.067098, Acc: 0.361000\n",
            "Best_acc: 0.663333\n",
            "epoch[29], Train\n",
            "Train Loss: 2.315234,  Acc: 0.408667\n",
            "Val Loss: 1.052320, Acc: 0.328333\n",
            "Test Loss: 1.057337, Acc: 0.364333\n",
            "Best_acc: 0.663333\n",
            "epoch[30], Train\n",
            "Train Loss: 2.314997,  Acc: 0.406083\n",
            "Val Loss: 1.057133, Acc: 0.331000\n",
            "Test Loss: 1.057633, Acc: 0.363000\n",
            "Best_acc: 0.663333\n",
            "epoch[31], Train\n",
            "Train Loss: 2.312998,  Acc: 0.408417\n",
            "Val Loss: 1.058376, Acc: 0.329000\n",
            "Test Loss: 1.073654, Acc: 0.359000\n",
            "Best_acc: 0.663333\n",
            "epoch[32], Train\n",
            "Train Loss: 2.312503,  Acc: 0.411250\n",
            "Val Loss: 1.055156, Acc: 0.324000\n",
            "Test Loss: 1.071997, Acc: 0.350667\n",
            "Best_acc: 0.663333\n",
            "epoch[33], Train\n",
            "Train Loss: 2.311126,  Acc: 0.411083\n",
            "Val Loss: 1.059724, Acc: 0.331000\n",
            "Test Loss: 1.075998, Acc: 0.357667\n",
            "Best_acc: 0.663333\n",
            "epoch[34], Train\n",
            "Train Loss: 2.308733,  Acc: 0.415917\n",
            "Val Loss: 1.058708, Acc: 0.322667\n",
            "Test Loss: 1.082435, Acc: 0.353667\n",
            "Best_acc: 0.663333\n",
            "epoch[35], Train\n",
            "Train Loss: 2.306053,  Acc: 0.417750\n",
            "Val Loss: 1.065263, Acc: 0.329667\n",
            "Test Loss: 1.081153, Acc: 0.379333\n",
            "Best_acc: 0.663333\n",
            "epoch[36], Train\n",
            "Train Loss: 2.306389,  Acc: 0.416500\n",
            "Val Loss: 1.059035, Acc: 0.328333\n",
            "Test Loss: 1.065635, Acc: 0.362667\n",
            "Best_acc: 0.663333\n",
            "epoch[37], Train\n",
            "Train Loss: 2.302159,  Acc: 0.418417\n",
            "Val Loss: 1.060724, Acc: 0.326333\n",
            "Test Loss: 1.086217, Acc: 0.362333\n",
            "Best_acc: 0.663333\n",
            "epoch[38], Train\n",
            "Train Loss: 2.301355,  Acc: 0.418417\n",
            "Val Loss: 1.065772, Acc: 0.330667\n",
            "Test Loss: 1.048400, Acc: 0.386000\n",
            "Best_acc: 0.663333\n",
            "epoch[39], Train\n",
            "Train Loss: 2.298857,  Acc: 0.425750\n",
            "Val Loss: 1.061928, Acc: 0.330667\n",
            "Test Loss: 1.054045, Acc: 0.392667\n",
            "Best_acc: 0.663333\n",
            "epoch[40], Train\n",
            "Train Loss: 2.296320,  Acc: 0.425750\n",
            "Val Loss: 1.065348, Acc: 0.326333\n",
            "Test Loss: 1.082521, Acc: 0.363000\n",
            "Best_acc: 0.663333\n",
            "epoch[41], Train\n",
            "Train Loss: 2.287185,  Acc: 0.433000\n",
            "Val Loss: 1.064044, Acc: 0.325000\n",
            "Test Loss: 1.079806, Acc: 0.369000\n",
            "Best_acc: 0.663333\n",
            "epoch[42], Train\n",
            "Train Loss: 2.283865,  Acc: 0.435167\n",
            "Val Loss: 1.065276, Acc: 0.327667\n",
            "Test Loss: 1.079898, Acc: 0.375333\n",
            "Best_acc: 0.663333\n",
            "epoch[43], Train\n",
            "Train Loss: 2.282587,  Acc: 0.437500\n",
            "Val Loss: 1.065828, Acc: 0.331333\n",
            "Test Loss: 1.074488, Acc: 0.379667\n",
            "Best_acc: 0.663333\n",
            "epoch[44], Train\n",
            "Train Loss: 2.281963,  Acc: 0.439417\n",
            "Val Loss: 1.066050, Acc: 0.330667\n",
            "Test Loss: 1.073761, Acc: 0.380667\n",
            "Best_acc: 0.663333\n",
            "epoch[45], Train\n",
            "Train Loss: 2.282218,  Acc: 0.439583\n",
            "Val Loss: 1.068013, Acc: 0.330000\n",
            "Test Loss: 1.078091, Acc: 0.375333\n",
            "Best_acc: 0.663333\n",
            "epoch[46], Train\n",
            "Train Loss: 2.280639,  Acc: 0.438667\n",
            "Val Loss: 1.066763, Acc: 0.331333\n",
            "Test Loss: 1.072221, Acc: 0.382000\n",
            "Best_acc: 0.663333\n",
            "epoch[47], Train\n",
            "Train Loss: 2.279554,  Acc: 0.435833\n",
            "Val Loss: 1.069335, Acc: 0.328000\n",
            "Test Loss: 1.083034, Acc: 0.374667\n",
            "Best_acc: 0.663333\n",
            "epoch[48], Train\n",
            "Train Loss: 2.278688,  Acc: 0.438750\n",
            "Val Loss: 1.068827, Acc: 0.329667\n",
            "Test Loss: 1.082273, Acc: 0.380667\n",
            "Best_acc: 0.663333\n",
            "epoch[49], Train\n",
            "Train Loss: 2.278364,  Acc: 0.441833\n",
            "Val Loss: 1.069787, Acc: 0.329667\n",
            "Test Loss: 1.082137, Acc: 0.380667\n",
            "Best_acc: 0.663333\n",
            " Early stopping at epoch 49 (best val_loss=1.049293, best acc=0.663333)\n",
            "Best_acc:  0.6633333333333333\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.123730\n",
            "Val Loss: 1.097999, Acc: 0.365333\n",
            "Test Loss: 1.084806, Acc: 0.515333\n",
            "Best_acc: 0.515333\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.102230\n",
            "Val Loss: 1.100000, Acc: 0.352667\n",
            "Test Loss: 1.081986, Acc: 0.479667\n",
            "Best_acc: 0.515333\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.100428\n",
            "Val Loss: 1.093659, Acc: 0.371000\n",
            "Test Loss: 1.051858, Acc: 0.562333\n",
            "Best_acc: 0.562333\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.098857\n",
            "Val Loss: 1.094924, Acc: 0.379333\n",
            "Test Loss: 1.068351, Acc: 0.622667\n",
            "Best_acc: 0.622667\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.097508\n",
            "Val Loss: 1.096169, Acc: 0.378667\n",
            "Test Loss: 1.047064, Acc: 0.560333\n",
            "Best_acc: 0.622667\n",
            "epoch[1], Train\n",
            "Train Loss: 2.409227,  Acc: 0.369000\n",
            "Val Loss: 1.094361, Acc: 0.370667\n",
            "Test Loss: 1.044254, Acc: 0.598333\n",
            "Best_acc: 0.598333\n",
            "epoch[2], Train\n",
            "Train Loss: 2.407435,  Acc: 0.370500\n",
            "Val Loss: 1.091748, Acc: 0.376667\n",
            "Test Loss: 1.073478, Acc: 0.533000\n",
            "Best_acc: 0.598333\n",
            "epoch[3], Train\n",
            "Train Loss: 2.402771,  Acc: 0.381167\n",
            "Val Loss: 1.091459, Acc: 0.372333\n",
            "Test Loss: 1.054298, Acc: 0.582667\n",
            "Best_acc: 0.598333\n",
            "epoch[4], Train\n",
            "Train Loss: 2.402418,  Acc: 0.375000\n",
            "Val Loss: 1.094983, Acc: 0.369333\n",
            "Test Loss: 1.072655, Acc: 0.458000\n",
            "Best_acc: 0.598333\n",
            "epoch[5], Train\n",
            "Train Loss: 2.400285,  Acc: 0.376000\n",
            "Val Loss: 1.089952, Acc: 0.385667\n",
            "Test Loss: 1.053173, Acc: 0.587000\n",
            "Best_acc: 0.598333\n",
            "epoch[6], Train\n",
            "Train Loss: 2.396923,  Acc: 0.377000\n",
            "Val Loss: 1.085883, Acc: 0.378333\n",
            "Test Loss: 1.026442, Acc: 0.568000\n",
            "Best_acc: 0.598333\n",
            "epoch[7], Train\n",
            "Train Loss: 2.390391,  Acc: 0.377167\n",
            "Val Loss: 1.079881, Acc: 0.380000\n",
            "Test Loss: 1.050026, Acc: 0.598000\n",
            "Best_acc: 0.598333\n",
            "epoch[8], Train\n",
            "Train Loss: 2.381344,  Acc: 0.372917\n",
            "Val Loss: 1.072377, Acc: 0.361333\n",
            "Test Loss: 1.069314, Acc: 0.436667\n",
            "Best_acc: 0.598333\n",
            "epoch[9], Train\n",
            "Train Loss: 2.371295,  Acc: 0.375417\n",
            "Val Loss: 1.064827, Acc: 0.370667\n",
            "Test Loss: 1.060797, Acc: 0.541000\n",
            "Best_acc: 0.598333\n",
            "epoch[10], Train\n",
            "Train Loss: 2.363309,  Acc: 0.380083\n",
            "Val Loss: 1.062965, Acc: 0.364667\n",
            "Test Loss: 1.080680, Acc: 0.481667\n",
            "Best_acc: 0.598333\n",
            "epoch[11], Train\n",
            "Train Loss: 2.359216,  Acc: 0.378667\n",
            "Val Loss: 1.063574, Acc: 0.362333\n",
            "Test Loss: 1.088871, Acc: 0.467000\n",
            "Best_acc: 0.598333\n",
            "epoch[12], Train\n",
            "Train Loss: 2.358240,  Acc: 0.377250\n",
            "Val Loss: 1.060277, Acc: 0.365333\n",
            "Test Loss: 1.082837, Acc: 0.535000\n",
            "Best_acc: 0.598333\n",
            "epoch[13], Train\n",
            "Train Loss: 2.354800,  Acc: 0.380917\n",
            "Val Loss: 1.060585, Acc: 0.365667\n",
            "Test Loss: 1.095674, Acc: 0.502667\n",
            "Best_acc: 0.598333\n",
            "epoch[14], Train\n",
            "Train Loss: 2.354372,  Acc: 0.380667\n",
            "Val Loss: 1.060468, Acc: 0.370000\n",
            "Test Loss: 1.082528, Acc: 0.545000\n",
            "Best_acc: 0.598333\n",
            "epoch[15], Train\n",
            "Train Loss: 2.352634,  Acc: 0.378833\n",
            "Val Loss: 1.061235, Acc: 0.356667\n",
            "Test Loss: 1.092573, Acc: 0.467000\n",
            "Best_acc: 0.598333\n",
            "epoch[16], Train\n",
            "Train Loss: 2.350737,  Acc: 0.384750\n",
            "Val Loss: 1.062604, Acc: 0.362000\n",
            "Test Loss: 1.108368, Acc: 0.465667\n",
            "Best_acc: 0.598333\n",
            "epoch[17], Train\n",
            "Train Loss: 2.348976,  Acc: 0.382250\n",
            "Val Loss: 1.062783, Acc: 0.369000\n",
            "Test Loss: 1.060325, Acc: 0.529000\n",
            "Best_acc: 0.598333\n",
            "epoch[18], Train\n",
            "Train Loss: 2.350053,  Acc: 0.389583\n",
            "Val Loss: 1.059985, Acc: 0.368000\n",
            "Test Loss: 1.080853, Acc: 0.514667\n",
            "Best_acc: 0.598333\n",
            "epoch[19], Train\n",
            "Train Loss: 2.348351,  Acc: 0.382167\n",
            "Val Loss: 1.061428, Acc: 0.368667\n",
            "Test Loss: 1.103977, Acc: 0.515667\n",
            "Best_acc: 0.598333\n",
            "epoch[20], Train\n",
            "Train Loss: 2.345621,  Acc: 0.394750\n",
            "Val Loss: 1.067936, Acc: 0.359333\n",
            "Test Loss: 1.125947, Acc: 0.440333\n",
            "Best_acc: 0.598333\n",
            "epoch[21], Train\n",
            "Train Loss: 2.344012,  Acc: 0.393583\n",
            "Val Loss: 1.059868, Acc: 0.369000\n",
            "Test Loss: 1.084214, Acc: 0.511667\n",
            "Best_acc: 0.598333\n",
            "epoch[22], Train\n",
            "Train Loss: 2.340338,  Acc: 0.395583\n",
            "Val Loss: 1.060443, Acc: 0.362667\n",
            "Test Loss: 1.073428, Acc: 0.498667\n",
            "Best_acc: 0.598333\n",
            "epoch[23], Train\n",
            "Train Loss: 2.338749,  Acc: 0.396250\n",
            "Val Loss: 1.062295, Acc: 0.366333\n",
            "Test Loss: 1.075047, Acc: 0.504333\n",
            "Best_acc: 0.598333\n",
            "epoch[24], Train\n",
            "Train Loss: 2.336749,  Acc: 0.398333\n",
            "Val Loss: 1.063454, Acc: 0.366000\n",
            "Test Loss: 1.071418, Acc: 0.514667\n",
            "Best_acc: 0.598333\n",
            "epoch[25], Train\n",
            "Train Loss: 2.336473,  Acc: 0.395750\n",
            "Val Loss: 1.062775, Acc: 0.370000\n",
            "Test Loss: 1.063390, Acc: 0.520000\n",
            "Best_acc: 0.598333\n",
            "epoch[26], Train\n",
            "Train Loss: 2.335247,  Acc: 0.396667\n",
            "Val Loss: 1.061390, Acc: 0.370667\n",
            "Test Loss: 1.065807, Acc: 0.518000\n",
            "Best_acc: 0.598333\n",
            "epoch[27], Train\n",
            "Train Loss: 2.334261,  Acc: 0.401833\n",
            "Val Loss: 1.063311, Acc: 0.366333\n",
            "Test Loss: 1.064739, Acc: 0.520333\n",
            "Best_acc: 0.598333\n",
            "epoch[28], Train\n",
            "Train Loss: 2.334147,  Acc: 0.400750\n",
            "Val Loss: 1.063551, Acc: 0.367667\n",
            "Test Loss: 1.069939, Acc: 0.502000\n",
            "Best_acc: 0.598333\n",
            "epoch[29], Train\n",
            "Train Loss: 2.332179,  Acc: 0.405583\n",
            "Val Loss: 1.064082, Acc: 0.359333\n",
            "Test Loss: 1.069238, Acc: 0.468000\n",
            "Best_acc: 0.598333\n",
            "epoch[30], Train\n",
            "Train Loss: 2.332425,  Acc: 0.406583\n",
            "Val Loss: 1.064617, Acc: 0.367000\n",
            "Test Loss: 1.070036, Acc: 0.488667\n",
            "Best_acc: 0.598333\n",
            "epoch[31], Train\n",
            "Train Loss: 2.330727,  Acc: 0.403667\n",
            "Val Loss: 1.064082, Acc: 0.372000\n",
            "Test Loss: 1.054198, Acc: 0.526000\n",
            "Best_acc: 0.598333\n",
            "epoch[32], Train\n",
            "Train Loss: 2.329499,  Acc: 0.405000\n",
            "Val Loss: 1.063837, Acc: 0.369667\n",
            "Test Loss: 1.062981, Acc: 0.503333\n",
            "Best_acc: 0.598333\n",
            "epoch[33], Train\n",
            "Train Loss: 2.328271,  Acc: 0.408583\n",
            "Val Loss: 1.065398, Acc: 0.371333\n",
            "Test Loss: 1.069202, Acc: 0.523333\n",
            "Best_acc: 0.598333\n",
            "epoch[34], Train\n",
            "Train Loss: 2.326064,  Acc: 0.409000\n",
            "Val Loss: 1.063967, Acc: 0.369333\n",
            "Test Loss: 1.043678, Acc: 0.517667\n",
            "Best_acc: 0.598333\n",
            "epoch[35], Train\n",
            "Train Loss: 2.325279,  Acc: 0.412083\n",
            "Val Loss: 1.066632, Acc: 0.371667\n",
            "Test Loss: 1.051824, Acc: 0.514667\n",
            "Best_acc: 0.598333\n",
            "epoch[36], Train\n",
            "Train Loss: 2.322910,  Acc: 0.414000\n",
            "Val Loss: 1.070298, Acc: 0.367000\n",
            "Test Loss: 1.092373, Acc: 0.488667\n",
            "Best_acc: 0.598333\n",
            "epoch[37], Train\n",
            "Train Loss: 2.324289,  Acc: 0.412417\n",
            "Val Loss: 1.065586, Acc: 0.367667\n",
            "Test Loss: 1.052231, Acc: 0.514333\n",
            "Best_acc: 0.598333\n",
            "epoch[38], Train\n",
            "Train Loss: 2.321187,  Acc: 0.413667\n",
            "Val Loss: 1.072006, Acc: 0.365000\n",
            "Test Loss: 1.078184, Acc: 0.461000\n",
            "Best_acc: 0.598333\n",
            "epoch[39], Train\n",
            "Train Loss: 2.317689,  Acc: 0.422750\n",
            "Val Loss: 1.068196, Acc: 0.378000\n",
            "Test Loss: 1.038470, Acc: 0.532667\n",
            "Best_acc: 0.598333\n",
            "epoch[40], Train\n",
            "Train Loss: 2.318126,  Acc: 0.412167\n",
            "Val Loss: 1.068556, Acc: 0.372333\n",
            "Test Loss: 1.047785, Acc: 0.521000\n",
            "Best_acc: 0.598333\n",
            "epoch[41], Train\n",
            "Train Loss: 2.309994,  Acc: 0.427417\n",
            "Val Loss: 1.067805, Acc: 0.372000\n",
            "Test Loss: 1.059004, Acc: 0.502333\n",
            "Best_acc: 0.598333\n",
            "epoch[42], Train\n",
            "Train Loss: 2.306661,  Acc: 0.425333\n",
            "Val Loss: 1.068746, Acc: 0.372667\n",
            "Test Loss: 1.063647, Acc: 0.491667\n",
            "Best_acc: 0.598333\n",
            "epoch[43], Train\n",
            "Train Loss: 2.305133,  Acc: 0.430000\n",
            "Val Loss: 1.070181, Acc: 0.372000\n",
            "Test Loss: 1.067369, Acc: 0.482333\n",
            "Best_acc: 0.598333\n",
            "epoch[44], Train\n",
            "Train Loss: 2.304638,  Acc: 0.428167\n",
            "Val Loss: 1.070656, Acc: 0.370333\n",
            "Test Loss: 1.067428, Acc: 0.486333\n",
            "Best_acc: 0.598333\n",
            "epoch[45], Train\n",
            "Train Loss: 2.302927,  Acc: 0.431917\n",
            "Val Loss: 1.071281, Acc: 0.372667\n",
            "Test Loss: 1.068948, Acc: 0.485667\n",
            "Best_acc: 0.598333\n",
            "epoch[46], Train\n",
            "Train Loss: 2.302665,  Acc: 0.433333\n",
            "Val Loss: 1.072093, Acc: 0.370667\n",
            "Test Loss: 1.071784, Acc: 0.477667\n",
            "Best_acc: 0.598333\n",
            "epoch[47], Train\n",
            "Train Loss: 2.301783,  Acc: 0.433750\n",
            "Val Loss: 1.072589, Acc: 0.372000\n",
            "Test Loss: 1.071169, Acc: 0.480333\n",
            "Best_acc: 0.598333\n",
            "epoch[48], Train\n",
            "Train Loss: 2.302044,  Acc: 0.432167\n",
            "Val Loss: 1.072406, Acc: 0.368333\n",
            "Test Loss: 1.074509, Acc: 0.478333\n",
            "Best_acc: 0.598333\n",
            "epoch[49], Train\n",
            "Train Loss: 2.301260,  Acc: 0.434417\n",
            "Val Loss: 1.073353, Acc: 0.367333\n",
            "Test Loss: 1.071295, Acc: 0.480667\n",
            "Best_acc: 0.598333\n",
            "epoch[50], Train\n",
            "Train Loss: 2.299980,  Acc: 0.431833\n",
            "Val Loss: 1.072930, Acc: 0.369667\n",
            "Test Loss: 1.069056, Acc: 0.484333\n",
            "Best_acc: 0.598333\n",
            "epoch[51], Train\n",
            "Train Loss: 2.299020,  Acc: 0.432500\n",
            "Val Loss: 1.073058, Acc: 0.367000\n",
            "Test Loss: 1.069917, Acc: 0.483667\n",
            "Best_acc: 0.598333\n",
            " Early stopping at epoch 51 (best val_loss=1.059868, best acc=0.598333)\n",
            "Best_acc:  0.5983333333333334\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.114422\n",
            "Val Loss: 1.096230, Acc: 0.373333\n",
            "Test Loss: 1.070981, Acc: 0.520667\n",
            "Best_acc: 0.520667\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.099452\n",
            "Val Loss: 1.095244, Acc: 0.370000\n",
            "Test Loss: 1.070696, Acc: 0.585667\n",
            "Best_acc: 0.585667\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.097791\n",
            "Val Loss: 1.092881, Acc: 0.374000\n",
            "Test Loss: 1.080576, Acc: 0.439333\n",
            "Best_acc: 0.585667\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.097851\n",
            "Val Loss: 1.093137, Acc: 0.365667\n",
            "Test Loss: 1.074296, Acc: 0.506333\n",
            "Best_acc: 0.585667\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.095749\n",
            "Val Loss: 1.093841, Acc: 0.373333\n",
            "Test Loss: 1.057073, Acc: 0.552333\n",
            "Best_acc: 0.585667\n",
            "epoch[1], Train\n",
            "Train Loss: 2.410431,  Acc: 0.361917\n",
            "Val Loss: 1.092344, Acc: 0.374667\n",
            "Test Loss: 1.055892, Acc: 0.633667\n",
            "Best_acc: 0.633667\n",
            "epoch[2], Train\n",
            "Train Loss: 2.406923,  Acc: 0.373750\n",
            "Val Loss: 1.091963, Acc: 0.380333\n",
            "Test Loss: 1.063234, Acc: 0.622667\n",
            "Best_acc: 0.633667\n",
            "epoch[3], Train\n",
            "Train Loss: 2.403812,  Acc: 0.375083\n",
            "Val Loss: 1.090340, Acc: 0.383000\n",
            "Test Loss: 1.032641, Acc: 0.630667\n",
            "Best_acc: 0.633667\n",
            "epoch[4], Train\n",
            "Train Loss: 2.401059,  Acc: 0.376083\n",
            "Val Loss: 1.092500, Acc: 0.383667\n",
            "Test Loss: 1.052890, Acc: 0.579000\n",
            "Best_acc: 0.633667\n",
            "epoch[5], Train\n",
            "Train Loss: 2.399633,  Acc: 0.377500\n",
            "Val Loss: 1.092320, Acc: 0.379000\n",
            "Test Loss: 1.013458, Acc: 0.636667\n",
            "Best_acc: 0.636667\n",
            "epoch[6], Train\n",
            "Train Loss: 2.394660,  Acc: 0.377333\n",
            "Val Loss: 1.086611, Acc: 0.352000\n",
            "Test Loss: 1.071237, Acc: 0.438667\n",
            "Best_acc: 0.636667\n",
            "epoch[7], Train\n",
            "Train Loss: 2.386812,  Acc: 0.373917\n",
            "Val Loss: 1.073663, Acc: 0.361667\n",
            "Test Loss: 1.058458, Acc: 0.464333\n",
            "Best_acc: 0.636667\n",
            "epoch[8], Train\n",
            "Train Loss: 2.373726,  Acc: 0.370167\n",
            "Val Loss: 1.068104, Acc: 0.325333\n",
            "Test Loss: 1.099477, Acc: 0.348000\n",
            "Best_acc: 0.636667\n",
            "epoch[9], Train\n",
            "Train Loss: 2.361388,  Acc: 0.370333\n",
            "Val Loss: 1.059795, Acc: 0.321667\n",
            "Test Loss: 1.076220, Acc: 0.334000\n",
            "Best_acc: 0.636667\n",
            "epoch[10], Train\n",
            "Train Loss: 2.354899,  Acc: 0.369250\n",
            "Val Loss: 1.054945, Acc: 0.323333\n",
            "Test Loss: 1.080912, Acc: 0.356667\n",
            "Best_acc: 0.636667\n",
            "epoch[11], Train\n",
            "Train Loss: 2.348481,  Acc: 0.377250\n",
            "Val Loss: 1.053005, Acc: 0.323000\n",
            "Test Loss: 1.099428, Acc: 0.338000\n",
            "Best_acc: 0.636667\n",
            "epoch[12], Train\n",
            "Train Loss: 2.344841,  Acc: 0.383917\n",
            "Val Loss: 1.051588, Acc: 0.324333\n",
            "Test Loss: 1.071389, Acc: 0.356000\n",
            "Best_acc: 0.636667\n",
            "epoch[13], Train\n",
            "Train Loss: 2.341154,  Acc: 0.381417\n",
            "Val Loss: 1.053581, Acc: 0.324333\n",
            "Test Loss: 1.108420, Acc: 0.343667\n",
            "Best_acc: 0.636667\n",
            "epoch[14], Train\n",
            "Train Loss: 2.340079,  Acc: 0.378000\n",
            "Val Loss: 1.049982, Acc: 0.322333\n",
            "Test Loss: 1.110499, Acc: 0.338000\n",
            "Best_acc: 0.636667\n",
            "epoch[15], Train\n",
            "Train Loss: 2.337964,  Acc: 0.381167\n",
            "Val Loss: 1.051959, Acc: 0.322333\n",
            "Test Loss: 1.093371, Acc: 0.339333\n",
            "Best_acc: 0.636667\n",
            "epoch[16], Train\n",
            "Train Loss: 2.336765,  Acc: 0.377833\n",
            "Val Loss: 1.055150, Acc: 0.321333\n",
            "Test Loss: 1.125639, Acc: 0.336000\n",
            "Best_acc: 0.636667\n",
            "epoch[17], Train\n",
            "Train Loss: 2.336520,  Acc: 0.384083\n",
            "Val Loss: 1.052303, Acc: 0.321667\n",
            "Test Loss: 1.124106, Acc: 0.333333\n",
            "Best_acc: 0.636667\n",
            "epoch[18], Train\n",
            "Train Loss: 2.335190,  Acc: 0.384083\n",
            "Val Loss: 1.049374, Acc: 0.320667\n",
            "Test Loss: 1.105713, Acc: 0.351333\n",
            "Best_acc: 0.636667\n",
            "epoch[19], Train\n",
            "Train Loss: 2.334024,  Acc: 0.386833\n",
            "Val Loss: 1.048706, Acc: 0.323667\n",
            "Test Loss: 1.090485, Acc: 0.339000\n",
            "Best_acc: 0.636667\n",
            "epoch[20], Train\n",
            "Train Loss: 2.332179,  Acc: 0.390917\n",
            "Val Loss: 1.051090, Acc: 0.323000\n",
            "Test Loss: 1.116131, Acc: 0.336667\n",
            "Best_acc: 0.636667\n",
            "epoch[21], Train\n",
            "Train Loss: 2.328970,  Acc: 0.392917\n",
            "Val Loss: 1.051868, Acc: 0.320333\n",
            "Test Loss: 1.111770, Acc: 0.342000\n",
            "Best_acc: 0.636667\n",
            "epoch[22], Train\n",
            "Train Loss: 2.326932,  Acc: 0.393917\n",
            "Val Loss: 1.051786, Acc: 0.324333\n",
            "Test Loss: 1.102435, Acc: 0.353000\n",
            "Best_acc: 0.636667\n",
            "epoch[23], Train\n",
            "Train Loss: 2.325323,  Acc: 0.394750\n",
            "Val Loss: 1.052645, Acc: 0.328000\n",
            "Test Loss: 1.099343, Acc: 0.356667\n",
            "Best_acc: 0.636667\n",
            "epoch[24], Train\n",
            "Train Loss: 2.324173,  Acc: 0.400167\n",
            "Val Loss: 1.053468, Acc: 0.326000\n",
            "Test Loss: 1.094105, Acc: 0.353333\n",
            "Best_acc: 0.636667\n",
            "epoch[25], Train\n",
            "Train Loss: 2.323447,  Acc: 0.396583\n",
            "Val Loss: 1.054415, Acc: 0.325667\n",
            "Test Loss: 1.098350, Acc: 0.350000\n",
            "Best_acc: 0.636667\n",
            "epoch[26], Train\n",
            "Train Loss: 2.322157,  Acc: 0.398417\n",
            "Val Loss: 1.053336, Acc: 0.327000\n",
            "Test Loss: 1.095591, Acc: 0.349667\n",
            "Best_acc: 0.636667\n",
            "epoch[27], Train\n",
            "Train Loss: 2.322200,  Acc: 0.396167\n",
            "Val Loss: 1.052157, Acc: 0.329333\n",
            "Test Loss: 1.098162, Acc: 0.363000\n",
            "Best_acc: 0.636667\n",
            "epoch[28], Train\n",
            "Train Loss: 2.320285,  Acc: 0.401583\n",
            "Val Loss: 1.055379, Acc: 0.329667\n",
            "Test Loss: 1.095994, Acc: 0.367000\n",
            "Best_acc: 0.636667\n",
            "epoch[29], Train\n",
            "Train Loss: 2.320013,  Acc: 0.402250\n",
            "Val Loss: 1.052088, Acc: 0.328333\n",
            "Test Loss: 1.091213, Acc: 0.368000\n",
            "Best_acc: 0.636667\n",
            "epoch[30], Train\n",
            "Train Loss: 2.318645,  Acc: 0.404417\n",
            "Val Loss: 1.057633, Acc: 0.326667\n",
            "Test Loss: 1.112354, Acc: 0.352000\n",
            "Best_acc: 0.636667\n",
            "epoch[31], Train\n",
            "Train Loss: 2.318276,  Acc: 0.401250\n",
            "Val Loss: 1.055875, Acc: 0.342333\n",
            "Test Loss: 1.081056, Acc: 0.399333\n",
            "Best_acc: 0.636667\n",
            "epoch[32], Train\n",
            "Train Loss: 2.316042,  Acc: 0.405667\n",
            "Val Loss: 1.056918, Acc: 0.331333\n",
            "Test Loss: 1.108451, Acc: 0.364333\n",
            "Best_acc: 0.636667\n",
            "epoch[33], Train\n",
            "Train Loss: 2.316089,  Acc: 0.405083\n",
            "Val Loss: 1.055254, Acc: 0.326333\n",
            "Test Loss: 1.079616, Acc: 0.368667\n",
            "Best_acc: 0.636667\n",
            "epoch[34], Train\n",
            "Train Loss: 2.315017,  Acc: 0.407083\n",
            "Val Loss: 1.058012, Acc: 0.328667\n",
            "Test Loss: 1.113951, Acc: 0.372333\n",
            "Best_acc: 0.636667\n",
            "epoch[35], Train\n",
            "Train Loss: 2.313160,  Acc: 0.406917\n",
            "Val Loss: 1.058726, Acc: 0.337000\n",
            "Test Loss: 1.077796, Acc: 0.396000\n",
            "Best_acc: 0.636667\n",
            "epoch[36], Train\n",
            "Train Loss: 2.311114,  Acc: 0.414583\n",
            "Val Loss: 1.061544, Acc: 0.326667\n",
            "Test Loss: 1.123839, Acc: 0.353667\n",
            "Best_acc: 0.636667\n",
            "epoch[37], Train\n",
            "Train Loss: 2.311017,  Acc: 0.416917\n",
            "Val Loss: 1.055338, Acc: 0.341333\n",
            "Test Loss: 1.069555, Acc: 0.393000\n",
            "Best_acc: 0.636667\n",
            "epoch[38], Train\n",
            "Train Loss: 2.306994,  Acc: 0.415917\n",
            "Val Loss: 1.059258, Acc: 0.335333\n",
            "Test Loss: 1.084988, Acc: 0.392000\n",
            "Best_acc: 0.636667\n",
            "epoch[39], Train\n",
            "Train Loss: 2.307329,  Acc: 0.419333\n",
            "Val Loss: 1.057523, Acc: 0.334000\n",
            "Test Loss: 1.072168, Acc: 0.388333\n",
            "Best_acc: 0.636667\n",
            "epoch[40], Train\n",
            "Train Loss: 2.305365,  Acc: 0.420917\n",
            "Val Loss: 1.059052, Acc: 0.342000\n",
            "Test Loss: 1.090502, Acc: 0.394333\n",
            "Best_acc: 0.636667\n",
            "epoch[41], Train\n",
            "Train Loss: 2.298070,  Acc: 0.432583\n",
            "Val Loss: 1.059897, Acc: 0.335667\n",
            "Test Loss: 1.098560, Acc: 0.380333\n",
            "Best_acc: 0.636667\n",
            "epoch[42], Train\n",
            "Train Loss: 2.296256,  Acc: 0.432500\n",
            "Val Loss: 1.060431, Acc: 0.336667\n",
            "Test Loss: 1.090501, Acc: 0.385667\n",
            "Best_acc: 0.636667\n",
            "epoch[43], Train\n",
            "Train Loss: 2.295002,  Acc: 0.432583\n",
            "Val Loss: 1.061313, Acc: 0.338000\n",
            "Test Loss: 1.091532, Acc: 0.391667\n",
            "Best_acc: 0.636667\n",
            "epoch[44], Train\n",
            "Train Loss: 2.293474,  Acc: 0.432417\n",
            "Val Loss: 1.062129, Acc: 0.337667\n",
            "Test Loss: 1.099120, Acc: 0.379667\n",
            "Best_acc: 0.636667\n",
            "epoch[45], Train\n",
            "Train Loss: 2.293691,  Acc: 0.431583\n",
            "Val Loss: 1.063290, Acc: 0.340000\n",
            "Test Loss: 1.096284, Acc: 0.384000\n",
            "Best_acc: 0.636667\n",
            "epoch[46], Train\n",
            "Train Loss: 2.293541,  Acc: 0.429500\n",
            "Val Loss: 1.063907, Acc: 0.339667\n",
            "Test Loss: 1.096152, Acc: 0.387333\n",
            "Best_acc: 0.636667\n",
            "epoch[47], Train\n",
            "Train Loss: 2.292468,  Acc: 0.433083\n",
            "Val Loss: 1.064440, Acc: 0.341000\n",
            "Test Loss: 1.103860, Acc: 0.385667\n",
            "Best_acc: 0.636667\n",
            "epoch[48], Train\n",
            "Train Loss: 2.292162,  Acc: 0.431500\n",
            "Val Loss: 1.064180, Acc: 0.339000\n",
            "Test Loss: 1.095422, Acc: 0.387333\n",
            "Best_acc: 0.636667\n",
            "epoch[49], Train\n",
            "Train Loss: 2.291452,  Acc: 0.432000\n",
            "Val Loss: 1.063466, Acc: 0.342000\n",
            "Test Loss: 1.095188, Acc: 0.396333\n",
            "Best_acc: 0.636667\n",
            " Early stopping at epoch 49 (best val_loss=1.048706, best acc=0.636667)\n",
            "Best_acc:  0.6366666666666667\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.119931\n",
            "Val Loss: 1.096451, Acc: 0.349000\n",
            "Test Loss: 1.090104, Acc: 0.443000\n",
            "Best_acc: 0.443000\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.100495\n",
            "Val Loss: 1.095604, Acc: 0.368333\n",
            "Test Loss: 1.076970, Acc: 0.543333\n",
            "Best_acc: 0.543333\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.096978\n",
            "Val Loss: 1.092541, Acc: 0.367667\n",
            "Test Loss: 1.047417, Acc: 0.596667\n",
            "Best_acc: 0.596667\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.097817\n",
            "Val Loss: 1.092480, Acc: 0.375667\n",
            "Test Loss: 1.096435, Acc: 0.409667\n",
            "Best_acc: 0.596667\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.096022\n",
            "Val Loss: 1.092540, Acc: 0.369000\n",
            "Test Loss: 1.064661, Acc: 0.589000\n",
            "Best_acc: 0.596667\n",
            "epoch[1], Train\n",
            "Train Loss: 2.406717,  Acc: 0.373167\n",
            "Val Loss: 1.094335, Acc: 0.362333\n",
            "Test Loss: 1.068605, Acc: 0.445333\n",
            "Best_acc: 0.445333\n",
            "epoch[2], Train\n",
            "Train Loss: 2.407594,  Acc: 0.371250\n",
            "Val Loss: 1.092405, Acc: 0.376000\n",
            "Test Loss: 1.071266, Acc: 0.571000\n",
            "Best_acc: 0.571000\n",
            "epoch[3], Train\n",
            "Train Loss: 2.403532,  Acc: 0.375250\n",
            "Val Loss: 1.091300, Acc: 0.385000\n",
            "Test Loss: 1.046012, Acc: 0.599333\n",
            "Best_acc: 0.599333\n",
            "epoch[4], Train\n",
            "Train Loss: 2.402011,  Acc: 0.378500\n",
            "Val Loss: 1.092026, Acc: 0.365333\n",
            "Test Loss: 1.055044, Acc: 0.513333\n",
            "Best_acc: 0.599333\n",
            "epoch[5], Train\n",
            "Train Loss: 2.399977,  Acc: 0.372000\n",
            "Val Loss: 1.092535, Acc: 0.360667\n",
            "Test Loss: 1.061700, Acc: 0.484333\n",
            "Best_acc: 0.599333\n",
            "epoch[6], Train\n",
            "Train Loss: 2.397204,  Acc: 0.378250\n",
            "Val Loss: 1.089575, Acc: 0.360000\n",
            "Test Loss: 1.057423, Acc: 0.480667\n",
            "Best_acc: 0.599333\n",
            "epoch[7], Train\n",
            "Train Loss: 2.391463,  Acc: 0.373500\n",
            "Val Loss: 1.081211, Acc: 0.371000\n",
            "Test Loss: 1.034710, Acc: 0.565000\n",
            "Best_acc: 0.599333\n",
            "epoch[8], Train\n",
            "Train Loss: 2.378998,  Acc: 0.374833\n",
            "Val Loss: 1.071822, Acc: 0.338333\n",
            "Test Loss: 1.072385, Acc: 0.406000\n",
            "Best_acc: 0.599333\n",
            "epoch[9], Train\n",
            "Train Loss: 2.367002,  Acc: 0.371750\n",
            "Val Loss: 1.062870, Acc: 0.327667\n",
            "Test Loss: 1.098469, Acc: 0.354667\n",
            "Best_acc: 0.599333\n",
            "epoch[10], Train\n",
            "Train Loss: 2.357048,  Acc: 0.373000\n",
            "Val Loss: 1.057606, Acc: 0.327333\n",
            "Test Loss: 1.087123, Acc: 0.358000\n",
            "Best_acc: 0.599333\n",
            "epoch[11], Train\n",
            "Train Loss: 2.352023,  Acc: 0.374083\n",
            "Val Loss: 1.054725, Acc: 0.322000\n",
            "Test Loss: 1.080813, Acc: 0.339333\n",
            "Best_acc: 0.599333\n",
            "epoch[12], Train\n",
            "Train Loss: 2.346278,  Acc: 0.375333\n",
            "Val Loss: 1.053912, Acc: 0.322333\n",
            "Test Loss: 1.092072, Acc: 0.336333\n",
            "Best_acc: 0.599333\n",
            "epoch[13], Train\n",
            "Train Loss: 2.344151,  Acc: 0.377667\n",
            "Val Loss: 1.052201, Acc: 0.325667\n",
            "Test Loss: 1.089277, Acc: 0.347667\n",
            "Best_acc: 0.599333\n",
            "epoch[14], Train\n",
            "Train Loss: 2.340188,  Acc: 0.384917\n",
            "Val Loss: 1.050781, Acc: 0.321333\n",
            "Test Loss: 1.105503, Acc: 0.343667\n",
            "Best_acc: 0.599333\n",
            "epoch[15], Train\n",
            "Train Loss: 2.339211,  Acc: 0.377417\n",
            "Val Loss: 1.056149, Acc: 0.322667\n",
            "Test Loss: 1.112869, Acc: 0.335000\n",
            "Best_acc: 0.599333\n",
            "epoch[16], Train\n",
            "Train Loss: 2.337115,  Acc: 0.385833\n",
            "Val Loss: 1.061095, Acc: 0.341667\n",
            "Test Loss: 1.126127, Acc: 0.401667\n",
            "Best_acc: 0.599333\n",
            "epoch[17], Train\n",
            "Train Loss: 2.335759,  Acc: 0.383417\n",
            "Val Loss: 1.051185, Acc: 0.321333\n",
            "Test Loss: 1.104768, Acc: 0.338000\n",
            "Best_acc: 0.599333\n",
            "epoch[18], Train\n",
            "Train Loss: 2.335263,  Acc: 0.383917\n",
            "Val Loss: 1.049034, Acc: 0.321333\n",
            "Test Loss: 1.101267, Acc: 0.335000\n",
            "Best_acc: 0.599333\n",
            "epoch[19], Train\n",
            "Train Loss: 2.332757,  Acc: 0.392417\n",
            "Val Loss: 1.053455, Acc: 0.323000\n",
            "Test Loss: 1.099838, Acc: 0.352333\n",
            "Best_acc: 0.599333\n",
            "epoch[20], Train\n",
            "Train Loss: 2.333113,  Acc: 0.387333\n",
            "Val Loss: 1.054819, Acc: 0.326000\n",
            "Test Loss: 1.087859, Acc: 0.348000\n",
            "Best_acc: 0.599333\n",
            "epoch[21], Train\n",
            "Train Loss: 2.329705,  Acc: 0.387833\n",
            "Val Loss: 1.053120, Acc: 0.323333\n",
            "Test Loss: 1.102915, Acc: 0.337667\n",
            "Best_acc: 0.599333\n",
            "epoch[22], Train\n",
            "Train Loss: 2.326687,  Acc: 0.392667\n",
            "Val Loss: 1.053299, Acc: 0.325667\n",
            "Test Loss: 1.110991, Acc: 0.340000\n",
            "Best_acc: 0.599333\n",
            "epoch[23], Train\n",
            "Train Loss: 2.325062,  Acc: 0.393833\n",
            "Val Loss: 1.052324, Acc: 0.325333\n",
            "Test Loss: 1.103088, Acc: 0.343667\n",
            "Best_acc: 0.599333\n",
            "epoch[24], Train\n",
            "Train Loss: 2.324073,  Acc: 0.396083\n",
            "Val Loss: 1.053065, Acc: 0.327000\n",
            "Test Loss: 1.095267, Acc: 0.352000\n",
            "Best_acc: 0.599333\n",
            "epoch[25], Train\n",
            "Train Loss: 2.322755,  Acc: 0.399417\n",
            "Val Loss: 1.052330, Acc: 0.327333\n",
            "Test Loss: 1.092282, Acc: 0.354000\n",
            "Best_acc: 0.599333\n",
            "epoch[26], Train\n",
            "Train Loss: 2.321835,  Acc: 0.398000\n",
            "Val Loss: 1.052348, Acc: 0.329000\n",
            "Test Loss: 1.089125, Acc: 0.354667\n",
            "Best_acc: 0.599333\n",
            "epoch[27], Train\n",
            "Train Loss: 2.320983,  Acc: 0.400917\n",
            "Val Loss: 1.052118, Acc: 0.332333\n",
            "Test Loss: 1.082754, Acc: 0.363667\n",
            "Best_acc: 0.599333\n",
            "epoch[28], Train\n",
            "Train Loss: 2.320290,  Acc: 0.400583\n",
            "Val Loss: 1.053792, Acc: 0.329667\n",
            "Test Loss: 1.097808, Acc: 0.355333\n",
            "Best_acc: 0.599333\n",
            "epoch[29], Train\n",
            "Train Loss: 2.318799,  Acc: 0.402083\n",
            "Val Loss: 1.056175, Acc: 0.324333\n",
            "Test Loss: 1.113439, Acc: 0.344000\n",
            "Best_acc: 0.599333\n",
            "epoch[30], Train\n",
            "Train Loss: 2.317300,  Acc: 0.405750\n",
            "Val Loss: 1.057336, Acc: 0.327000\n",
            "Test Loss: 1.099364, Acc: 0.351667\n",
            "Best_acc: 0.599333\n",
            "epoch[31], Train\n",
            "Train Loss: 2.316817,  Acc: 0.407250\n",
            "Val Loss: 1.052732, Acc: 0.336333\n",
            "Test Loss: 1.087157, Acc: 0.384000\n",
            "Best_acc: 0.599333\n",
            "epoch[32], Train\n",
            "Train Loss: 2.315740,  Acc: 0.405417\n",
            "Val Loss: 1.055208, Acc: 0.338333\n",
            "Test Loss: 1.093869, Acc: 0.373667\n",
            "Best_acc: 0.599333\n",
            "epoch[33], Train\n",
            "Train Loss: 2.314588,  Acc: 0.408250\n",
            "Val Loss: 1.059986, Acc: 0.335667\n",
            "Test Loss: 1.110049, Acc: 0.367000\n",
            "Best_acc: 0.599333\n",
            "epoch[34], Train\n",
            "Train Loss: 2.313730,  Acc: 0.410000\n",
            "Val Loss: 1.056570, Acc: 0.341333\n",
            "Test Loss: 1.071691, Acc: 0.390000\n",
            "Best_acc: 0.599333\n",
            "epoch[35], Train\n",
            "Train Loss: 2.312297,  Acc: 0.410333\n",
            "Val Loss: 1.058002, Acc: 0.337667\n",
            "Test Loss: 1.097934, Acc: 0.371333\n",
            "Best_acc: 0.599333\n",
            "epoch[36], Train\n",
            "Train Loss: 2.310543,  Acc: 0.410167\n",
            "Val Loss: 1.059979, Acc: 0.334333\n",
            "Test Loss: 1.109017, Acc: 0.361333\n",
            "Best_acc: 0.599333\n",
            "epoch[37], Train\n",
            "Train Loss: 2.308456,  Acc: 0.416917\n",
            "Val Loss: 1.060329, Acc: 0.350667\n",
            "Test Loss: 1.058487, Acc: 0.413000\n",
            "Best_acc: 0.599333\n",
            "epoch[38], Train\n",
            "Train Loss: 2.307357,  Acc: 0.417750\n",
            "Val Loss: 1.059002, Acc: 0.337333\n",
            "Test Loss: 1.069129, Acc: 0.407333\n",
            "Best_acc: 0.599333\n",
            "epoch[39], Train\n",
            "Train Loss: 2.306315,  Acc: 0.419583\n",
            "Val Loss: 1.061540, Acc: 0.337333\n",
            "Test Loss: 1.104121, Acc: 0.379667\n",
            "Best_acc: 0.599333\n",
            "epoch[40], Train\n",
            "Train Loss: 2.304667,  Acc: 0.418333\n",
            "Val Loss: 1.058651, Acc: 0.350333\n",
            "Test Loss: 1.081325, Acc: 0.408333\n",
            "Best_acc: 0.599333\n",
            "epoch[41], Train\n",
            "Train Loss: 2.297925,  Acc: 0.432333\n",
            "Val Loss: 1.061872, Acc: 0.339000\n",
            "Test Loss: 1.100718, Acc: 0.375667\n",
            "Best_acc: 0.599333\n",
            "epoch[42], Train\n",
            "Train Loss: 2.294906,  Acc: 0.432167\n",
            "Val Loss: 1.062867, Acc: 0.335333\n",
            "Test Loss: 1.103186, Acc: 0.370667\n",
            "Best_acc: 0.599333\n",
            "epoch[43], Train\n",
            "Train Loss: 2.294514,  Acc: 0.431500\n",
            "Val Loss: 1.063440, Acc: 0.335000\n",
            "Test Loss: 1.108078, Acc: 0.370667\n",
            "Best_acc: 0.599333\n",
            "epoch[44], Train\n",
            "Train Loss: 2.293609,  Acc: 0.427417\n",
            "Val Loss: 1.063251, Acc: 0.341000\n",
            "Test Loss: 1.098877, Acc: 0.381333\n",
            "Best_acc: 0.599333\n",
            "epoch[45], Train\n",
            "Train Loss: 2.293270,  Acc: 0.434417\n",
            "Val Loss: 1.063208, Acc: 0.340667\n",
            "Test Loss: 1.096121, Acc: 0.381000\n",
            "Best_acc: 0.599333\n",
            "epoch[46], Train\n",
            "Train Loss: 2.293202,  Acc: 0.431250\n",
            "Val Loss: 1.064105, Acc: 0.341333\n",
            "Test Loss: 1.103000, Acc: 0.382000\n",
            "Best_acc: 0.599333\n",
            "epoch[47], Train\n",
            "Train Loss: 2.290893,  Acc: 0.435333\n",
            "Val Loss: 1.065016, Acc: 0.339333\n",
            "Test Loss: 1.107098, Acc: 0.375333\n",
            "Best_acc: 0.599333\n",
            "epoch[48], Train\n",
            "Train Loss: 2.291789,  Acc: 0.437000\n",
            "Val Loss: 1.065453, Acc: 0.338333\n",
            "Test Loss: 1.109633, Acc: 0.372667\n",
            "Best_acc: 0.599333\n",
            " Early stopping at epoch 48 (best val_loss=1.049034, best acc=0.599333)\n",
            "Best_acc:  0.5993333333333334\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.118264\n",
            "Val Loss: 1.097315, Acc: 0.372667\n",
            "Test Loss: 1.097544, Acc: 0.470000\n",
            "Best_acc: 0.470000\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.101815\n",
            "Val Loss: 1.099159, Acc: 0.358667\n",
            "Test Loss: 1.096335, Acc: 0.453333\n",
            "Best_acc: 0.470000\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.097944\n",
            "Val Loss: 1.098576, Acc: 0.354000\n",
            "Test Loss: 1.070431, Acc: 0.506667\n",
            "Best_acc: 0.506667\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.097087\n",
            "Val Loss: 1.094796, Acc: 0.377000\n",
            "Test Loss: 1.068156, Acc: 0.551667\n",
            "Best_acc: 0.551667\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.097669\n",
            "Val Loss: 1.096040, Acc: 0.381333\n",
            "Test Loss: 1.053244, Acc: 0.524000\n",
            "Best_acc: 0.551667\n",
            "epoch[1], Train\n",
            "Train Loss: 2.408673,  Acc: 0.368917\n",
            "Val Loss: 1.103603, Acc: 0.382333\n",
            "Test Loss: 1.053961, Acc: 0.595333\n",
            "Best_acc: 0.595333\n",
            "epoch[2], Train\n",
            "Train Loss: 2.407155,  Acc: 0.368667\n",
            "Val Loss: 1.093832, Acc: 0.370333\n",
            "Test Loss: 1.053529, Acc: 0.557000\n",
            "Best_acc: 0.595333\n",
            "epoch[3], Train\n",
            "Train Loss: 2.403892,  Acc: 0.373333\n",
            "Val Loss: 1.092903, Acc: 0.381000\n",
            "Test Loss: 1.062432, Acc: 0.596333\n",
            "Best_acc: 0.596333\n",
            "epoch[4], Train\n",
            "Train Loss: 2.403937,  Acc: 0.366417\n",
            "Val Loss: 1.092399, Acc: 0.378333\n",
            "Test Loss: 1.053597, Acc: 0.582000\n",
            "Best_acc: 0.596333\n",
            "epoch[5], Train\n",
            "Train Loss: 2.401151,  Acc: 0.378500\n",
            "Val Loss: 1.092871, Acc: 0.390000\n",
            "Test Loss: 1.037637, Acc: 0.569000\n",
            "Best_acc: 0.596333\n",
            "epoch[6], Train\n",
            "Train Loss: 2.400120,  Acc: 0.377417\n",
            "Val Loss: 1.092640, Acc: 0.384333\n",
            "Test Loss: 1.012602, Acc: 0.635000\n",
            "Best_acc: 0.635000\n",
            "epoch[7], Train\n",
            "Train Loss: 2.396448,  Acc: 0.376500\n",
            "Val Loss: 1.085988, Acc: 0.378333\n",
            "Test Loss: 1.027595, Acc: 0.581000\n",
            "Best_acc: 0.635000\n",
            "epoch[8], Train\n",
            "Train Loss: 2.388246,  Acc: 0.378667\n",
            "Val Loss: 1.078634, Acc: 0.364667\n",
            "Test Loss: 1.037790, Acc: 0.534667\n",
            "Best_acc: 0.635000\n",
            "epoch[9], Train\n",
            "Train Loss: 2.379085,  Acc: 0.376583\n",
            "Val Loss: 1.073284, Acc: 0.363333\n",
            "Test Loss: 1.049326, Acc: 0.513000\n",
            "Best_acc: 0.635000\n",
            "epoch[10], Train\n",
            "Train Loss: 2.370421,  Acc: 0.376167\n",
            "Val Loss: 1.072593, Acc: 0.363000\n",
            "Test Loss: 1.058944, Acc: 0.433667\n",
            "Best_acc: 0.635000\n",
            "epoch[11], Train\n",
            "Train Loss: 2.363361,  Acc: 0.378417\n",
            "Val Loss: 1.065169, Acc: 0.354667\n",
            "Test Loss: 1.066006, Acc: 0.459667\n",
            "Best_acc: 0.635000\n",
            "epoch[12], Train\n",
            "Train Loss: 2.359357,  Acc: 0.382000\n",
            "Val Loss: 1.064858, Acc: 0.364000\n",
            "Test Loss: 1.081977, Acc: 0.510000\n",
            "Best_acc: 0.635000\n",
            "epoch[13], Train\n",
            "Train Loss: 2.356304,  Acc: 0.382167\n",
            "Val Loss: 1.062047, Acc: 0.359333\n",
            "Test Loss: 1.060207, Acc: 0.510000\n",
            "Best_acc: 0.635000\n",
            "epoch[14], Train\n",
            "Train Loss: 2.353795,  Acc: 0.380583\n",
            "Val Loss: 1.065828, Acc: 0.352333\n",
            "Test Loss: 1.064585, Acc: 0.452333\n",
            "Best_acc: 0.635000\n",
            "epoch[15], Train\n",
            "Train Loss: 2.350146,  Acc: 0.378250\n",
            "Val Loss: 1.059954, Acc: 0.336000\n",
            "Test Loss: 1.072527, Acc: 0.371667\n",
            "Best_acc: 0.635000\n",
            "epoch[16], Train\n",
            "Train Loss: 2.347572,  Acc: 0.376500\n",
            "Val Loss: 1.057590, Acc: 0.324667\n",
            "Test Loss: 1.109510, Acc: 0.341000\n",
            "Best_acc: 0.635000\n",
            "epoch[17], Train\n",
            "Train Loss: 2.339791,  Acc: 0.382750\n",
            "Val Loss: 1.058871, Acc: 0.336000\n",
            "Test Loss: 1.124332, Acc: 0.372000\n",
            "Best_acc: 0.635000\n",
            "epoch[18], Train\n",
            "Train Loss: 2.337586,  Acc: 0.386917\n",
            "Val Loss: 1.053041, Acc: 0.329667\n",
            "Test Loss: 1.081019, Acc: 0.363667\n",
            "Best_acc: 0.635000\n",
            "epoch[19], Train\n",
            "Train Loss: 2.336080,  Acc: 0.386667\n",
            "Val Loss: 1.053616, Acc: 0.324333\n",
            "Test Loss: 1.112551, Acc: 0.344333\n",
            "Best_acc: 0.635000\n",
            "epoch[20], Train\n",
            "Train Loss: 2.334111,  Acc: 0.384500\n",
            "Val Loss: 1.053414, Acc: 0.325667\n",
            "Test Loss: 1.084174, Acc: 0.345333\n",
            "Best_acc: 0.635000\n",
            "epoch[21], Train\n",
            "Train Loss: 2.327045,  Acc: 0.394583\n",
            "Val Loss: 1.052436, Acc: 0.326667\n",
            "Test Loss: 1.089048, Acc: 0.349000\n",
            "Best_acc: 0.635000\n",
            "epoch[22], Train\n",
            "Train Loss: 2.325289,  Acc: 0.395417\n",
            "Val Loss: 1.053660, Acc: 0.335667\n",
            "Test Loss: 1.092236, Acc: 0.367000\n",
            "Best_acc: 0.635000\n",
            "epoch[23], Train\n",
            "Train Loss: 2.324370,  Acc: 0.396083\n",
            "Val Loss: 1.052455, Acc: 0.334667\n",
            "Test Loss: 1.088978, Acc: 0.366000\n",
            "Best_acc: 0.635000\n",
            "epoch[24], Train\n",
            "Train Loss: 2.323040,  Acc: 0.397083\n",
            "Val Loss: 1.053581, Acc: 0.337333\n",
            "Test Loss: 1.093205, Acc: 0.369333\n",
            "Best_acc: 0.635000\n",
            "epoch[25], Train\n",
            "Train Loss: 2.322912,  Acc: 0.396417\n",
            "Val Loss: 1.054455, Acc: 0.331333\n",
            "Test Loss: 1.080539, Acc: 0.372000\n",
            "Best_acc: 0.635000\n",
            "epoch[26], Train\n",
            "Train Loss: 2.322624,  Acc: 0.398583\n",
            "Val Loss: 1.055188, Acc: 0.324333\n",
            "Test Loss: 1.102917, Acc: 0.345667\n",
            "Best_acc: 0.635000\n",
            "epoch[27], Train\n",
            "Train Loss: 2.321099,  Acc: 0.403833\n",
            "Val Loss: 1.053619, Acc: 0.327333\n",
            "Test Loss: 1.092182, Acc: 0.361667\n",
            "Best_acc: 0.635000\n",
            "epoch[28], Train\n",
            "Train Loss: 2.321019,  Acc: 0.402417\n",
            "Val Loss: 1.054361, Acc: 0.337667\n",
            "Test Loss: 1.087430, Acc: 0.379000\n",
            "Best_acc: 0.635000\n",
            "epoch[29], Train\n",
            "Train Loss: 2.319511,  Acc: 0.403750\n",
            "Val Loss: 1.054697, Acc: 0.322000\n",
            "Test Loss: 1.088060, Acc: 0.354667\n",
            "Best_acc: 0.635000\n",
            "epoch[30], Train\n",
            "Train Loss: 2.319135,  Acc: 0.402000\n",
            "Val Loss: 1.057890, Acc: 0.332000\n",
            "Test Loss: 1.107035, Acc: 0.360000\n",
            "Best_acc: 0.635000\n",
            "epoch[31], Train\n",
            "Train Loss: 2.318480,  Acc: 0.405250\n",
            "Val Loss: 1.055837, Acc: 0.328667\n",
            "Test Loss: 1.086740, Acc: 0.368000\n",
            "Best_acc: 0.635000\n",
            "epoch[32], Train\n",
            "Train Loss: 2.316695,  Acc: 0.408250\n",
            "Val Loss: 1.055241, Acc: 0.345000\n",
            "Test Loss: 1.075553, Acc: 0.400667\n",
            "Best_acc: 0.635000\n",
            "epoch[33], Train\n",
            "Train Loss: 2.316666,  Acc: 0.404583\n",
            "Val Loss: 1.055296, Acc: 0.334000\n",
            "Test Loss: 1.070097, Acc: 0.376333\n",
            "Best_acc: 0.635000\n",
            "epoch[34], Train\n",
            "Train Loss: 2.314486,  Acc: 0.412750\n",
            "Val Loss: 1.055391, Acc: 0.342333\n",
            "Test Loss: 1.091613, Acc: 0.395000\n",
            "Best_acc: 0.635000\n",
            "epoch[35], Train\n",
            "Train Loss: 2.312940,  Acc: 0.409917\n",
            "Val Loss: 1.058318, Acc: 0.330333\n",
            "Test Loss: 1.080447, Acc: 0.374000\n",
            "Best_acc: 0.635000\n",
            "epoch[36], Train\n",
            "Train Loss: 2.311087,  Acc: 0.410833\n",
            "Val Loss: 1.059910, Acc: 0.335667\n",
            "Test Loss: 1.099358, Acc: 0.372333\n",
            "Best_acc: 0.635000\n",
            "epoch[37], Train\n",
            "Train Loss: 2.311631,  Acc: 0.412750\n",
            "Val Loss: 1.063054, Acc: 0.336667\n",
            "Test Loss: 1.091721, Acc: 0.377667\n",
            "Best_acc: 0.635000\n",
            "epoch[38], Train\n",
            "Train Loss: 2.309043,  Acc: 0.412917\n",
            "Val Loss: 1.063212, Acc: 0.332667\n",
            "Test Loss: 1.097771, Acc: 0.366333\n",
            "Best_acc: 0.635000\n",
            "epoch[39], Train\n",
            "Train Loss: 2.307737,  Acc: 0.416000\n",
            "Val Loss: 1.069858, Acc: 0.333000\n",
            "Test Loss: 1.103866, Acc: 0.374000\n",
            "Best_acc: 0.635000\n",
            "epoch[40], Train\n",
            "Train Loss: 2.306315,  Acc: 0.418750\n",
            "Val Loss: 1.071153, Acc: 0.329667\n",
            "Test Loss: 1.126341, Acc: 0.359667\n",
            "Best_acc: 0.635000\n",
            "epoch[41], Train\n",
            "Train Loss: 2.298445,  Acc: 0.427000\n",
            "Val Loss: 1.060598, Acc: 0.341000\n",
            "Test Loss: 1.094522, Acc: 0.386667\n",
            "Best_acc: 0.635000\n",
            "epoch[42], Train\n",
            "Train Loss: 2.296163,  Acc: 0.428917\n",
            "Val Loss: 1.061536, Acc: 0.339000\n",
            "Test Loss: 1.098588, Acc: 0.378333\n",
            "Best_acc: 0.635000\n",
            "epoch[43], Train\n",
            "Train Loss: 2.296403,  Acc: 0.427583\n",
            "Val Loss: 1.062032, Acc: 0.339333\n",
            "Test Loss: 1.093260, Acc: 0.380000\n",
            "Best_acc: 0.635000\n",
            "epoch[44], Train\n",
            "Train Loss: 2.294738,  Acc: 0.432250\n",
            "Val Loss: 1.062328, Acc: 0.342000\n",
            "Test Loss: 1.093261, Acc: 0.380000\n",
            "Best_acc: 0.635000\n",
            "epoch[45], Train\n",
            "Train Loss: 2.294160,  Acc: 0.431250\n",
            "Val Loss: 1.061493, Acc: 0.342000\n",
            "Test Loss: 1.094510, Acc: 0.380333\n",
            "Best_acc: 0.635000\n",
            "epoch[46], Train\n",
            "Train Loss: 2.294362,  Acc: 0.429250\n",
            "Val Loss: 1.062470, Acc: 0.342667\n",
            "Test Loss: 1.094543, Acc: 0.383667\n",
            "Best_acc: 0.635000\n",
            "epoch[47], Train\n",
            "Train Loss: 2.293733,  Acc: 0.427667\n",
            "Val Loss: 1.064630, Acc: 0.340000\n",
            "Test Loss: 1.099206, Acc: 0.380667\n",
            "Best_acc: 0.635000\n",
            "epoch[48], Train\n",
            "Train Loss: 2.292354,  Acc: 0.432167\n",
            "Val Loss: 1.062505, Acc: 0.341667\n",
            "Test Loss: 1.093096, Acc: 0.387333\n",
            "Best_acc: 0.635000\n",
            "epoch[49], Train\n",
            "Train Loss: 2.293163,  Acc: 0.431000\n",
            "Val Loss: 1.063809, Acc: 0.340333\n",
            "Test Loss: 1.097559, Acc: 0.379667\n",
            "Best_acc: 0.635000\n",
            "epoch[50], Train\n",
            "Train Loss: 2.291345,  Acc: 0.435083\n",
            "Val Loss: 1.063045, Acc: 0.340667\n",
            "Test Loss: 1.097316, Acc: 0.380667\n",
            "Best_acc: 0.635000\n",
            "epoch[51], Train\n",
            "Train Loss: 2.291356,  Acc: 0.432167\n",
            "Val Loss: 1.064996, Acc: 0.341000\n",
            "Test Loss: 1.097361, Acc: 0.381667\n",
            "Best_acc: 0.635000\n",
            " Early stopping at epoch 51 (best val_loss=1.052436, best acc=0.635000)\n",
            "Best_acc:  0.635\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.116286\n",
            "Val Loss: 1.095665, Acc: 0.377333\n",
            "Test Loss: 1.038567, Acc: 0.597667\n",
            "Best_acc: 0.597667\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.103724\n",
            "Val Loss: 1.093538, Acc: 0.378333\n",
            "Test Loss: 1.054349, Acc: 0.592000\n",
            "Best_acc: 0.597667\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.097214\n",
            "Val Loss: 1.095139, Acc: 0.375000\n",
            "Test Loss: 1.075384, Acc: 0.577667\n",
            "Best_acc: 0.597667\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.096761\n",
            "Val Loss: 1.092412, Acc: 0.382000\n",
            "Test Loss: 1.039921, Acc: 0.628667\n",
            "Best_acc: 0.628667\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.095401\n",
            "Val Loss: 1.091206, Acc: 0.386333\n",
            "Test Loss: 1.047887, Acc: 0.605333\n",
            "Best_acc: 0.628667\n",
            "epoch[1], Train\n",
            "Train Loss: 2.406920,  Acc: 0.364500\n",
            "Val Loss: 1.095806, Acc: 0.366333\n",
            "Test Loss: 1.071717, Acc: 0.427333\n",
            "Best_acc: 0.427333\n",
            "epoch[2], Train\n",
            "Train Loss: 2.403847,  Acc: 0.379000\n",
            "Val Loss: 1.091422, Acc: 0.385000\n",
            "Test Loss: 1.055992, Acc: 0.587667\n",
            "Best_acc: 0.587667\n",
            "epoch[3], Train\n",
            "Train Loss: 2.402668,  Acc: 0.375083\n",
            "Val Loss: 1.091302, Acc: 0.379667\n",
            "Test Loss: 1.065055, Acc: 0.541333\n",
            "Best_acc: 0.587667\n",
            "epoch[4], Train\n",
            "Train Loss: 2.400970,  Acc: 0.380083\n",
            "Val Loss: 1.090558, Acc: 0.377333\n",
            "Test Loss: 1.026536, Acc: 0.630333\n",
            "Best_acc: 0.630333\n",
            "epoch[5], Train\n",
            "Train Loss: 2.399091,  Acc: 0.375417\n",
            "Val Loss: 1.090119, Acc: 0.378333\n",
            "Test Loss: 1.049622, Acc: 0.613333\n",
            "Best_acc: 0.630333\n",
            "epoch[6], Train\n",
            "Train Loss: 2.394198,  Acc: 0.377750\n",
            "Val Loss: 1.084036, Acc: 0.374333\n",
            "Test Loss: 1.047821, Acc: 0.526000\n",
            "Best_acc: 0.630333\n",
            "epoch[7], Train\n",
            "Train Loss: 2.385559,  Acc: 0.382000\n",
            "Val Loss: 1.074743, Acc: 0.351333\n",
            "Test Loss: 1.072060, Acc: 0.415667\n",
            "Best_acc: 0.630333\n",
            "epoch[8], Train\n",
            "Train Loss: 2.372457,  Acc: 0.381167\n",
            "Val Loss: 1.065074, Acc: 0.343333\n",
            "Test Loss: 1.060189, Acc: 0.415333\n",
            "Best_acc: 0.630333\n",
            "epoch[9], Train\n",
            "Train Loss: 2.361760,  Acc: 0.374583\n",
            "Val Loss: 1.059352, Acc: 0.328667\n",
            "Test Loss: 1.086840, Acc: 0.350000\n",
            "Best_acc: 0.630333\n",
            "epoch[10], Train\n",
            "Train Loss: 2.351481,  Acc: 0.375833\n",
            "Val Loss: 1.059167, Acc: 0.352667\n",
            "Test Loss: 1.093386, Acc: 0.423000\n",
            "Best_acc: 0.630333\n",
            "epoch[11], Train\n",
            "Train Loss: 2.346270,  Acc: 0.378750\n",
            "Val Loss: 1.061439, Acc: 0.321333\n",
            "Test Loss: 1.108022, Acc: 0.339667\n",
            "Best_acc: 0.630333\n",
            "epoch[12], Train\n",
            "Train Loss: 2.342097,  Acc: 0.381917\n",
            "Val Loss: 1.051918, Acc: 0.322667\n",
            "Test Loss: 1.084664, Acc: 0.337000\n",
            "Best_acc: 0.630333\n",
            "epoch[13], Train\n",
            "Train Loss: 2.338692,  Acc: 0.388750\n",
            "Val Loss: 1.053175, Acc: 0.330667\n",
            "Test Loss: 1.082010, Acc: 0.366667\n",
            "Best_acc: 0.630333\n",
            "epoch[14], Train\n",
            "Train Loss: 2.336760,  Acc: 0.380750\n",
            "Val Loss: 1.056798, Acc: 0.320667\n",
            "Test Loss: 1.110873, Acc: 0.334333\n",
            "Best_acc: 0.630333\n",
            "epoch[15], Train\n",
            "Train Loss: 2.336157,  Acc: 0.388167\n",
            "Val Loss: 1.053823, Acc: 0.326333\n",
            "Test Loss: 1.088262, Acc: 0.369000\n",
            "Best_acc: 0.630333\n",
            "epoch[16], Train\n",
            "Train Loss: 2.334922,  Acc: 0.387917\n",
            "Val Loss: 1.050778, Acc: 0.321667\n",
            "Test Loss: 1.087793, Acc: 0.341000\n",
            "Best_acc: 0.630333\n",
            "epoch[17], Train\n",
            "Train Loss: 2.332392,  Acc: 0.390750\n",
            "Val Loss: 1.062300, Acc: 0.324000\n",
            "Test Loss: 1.077713, Acc: 0.338667\n",
            "Best_acc: 0.630333\n",
            "epoch[18], Train\n",
            "Train Loss: 2.331603,  Acc: 0.388083\n",
            "Val Loss: 1.051324, Acc: 0.322667\n",
            "Test Loss: 1.084616, Acc: 0.333667\n",
            "Best_acc: 0.630333\n",
            "epoch[19], Train\n",
            "Train Loss: 2.331089,  Acc: 0.388833\n",
            "Val Loss: 1.054931, Acc: 0.321000\n",
            "Test Loss: 1.090919, Acc: 0.335333\n",
            "Best_acc: 0.630333\n",
            "epoch[20], Train\n",
            "Train Loss: 2.329818,  Acc: 0.388417\n",
            "Val Loss: 1.067669, Acc: 0.321333\n",
            "Test Loss: 1.127166, Acc: 0.333333\n",
            "Best_acc: 0.630333\n",
            "epoch[21], Train\n",
            "Train Loss: 2.323774,  Acc: 0.394667\n",
            "Val Loss: 1.053652, Acc: 0.321000\n",
            "Test Loss: 1.098875, Acc: 0.339667\n",
            "Best_acc: 0.630333\n",
            "epoch[22], Train\n",
            "Train Loss: 2.320652,  Acc: 0.401833\n",
            "Val Loss: 1.054494, Acc: 0.322667\n",
            "Test Loss: 1.090981, Acc: 0.341667\n",
            "Best_acc: 0.630333\n",
            "epoch[23], Train\n",
            "Train Loss: 2.318699,  Acc: 0.406417\n",
            "Val Loss: 1.055012, Acc: 0.324000\n",
            "Test Loss: 1.098794, Acc: 0.344333\n",
            "Best_acc: 0.630333\n",
            "epoch[24], Train\n",
            "Train Loss: 2.317979,  Acc: 0.406833\n",
            "Val Loss: 1.057986, Acc: 0.324667\n",
            "Test Loss: 1.115716, Acc: 0.336333\n",
            "Best_acc: 0.630333\n",
            "epoch[25], Train\n",
            "Train Loss: 2.315861,  Acc: 0.405917\n",
            "Val Loss: 1.055429, Acc: 0.329000\n",
            "Test Loss: 1.094638, Acc: 0.361667\n",
            "Best_acc: 0.630333\n",
            "epoch[26], Train\n",
            "Train Loss: 2.314701,  Acc: 0.408667\n",
            "Val Loss: 1.056428, Acc: 0.327000\n",
            "Test Loss: 1.096701, Acc: 0.353333\n",
            "Best_acc: 0.630333\n",
            "epoch[27], Train\n",
            "Train Loss: 2.313489,  Acc: 0.411083\n",
            "Val Loss: 1.056765, Acc: 0.329000\n",
            "Test Loss: 1.102089, Acc: 0.356000\n",
            "Best_acc: 0.630333\n",
            "epoch[28], Train\n",
            "Train Loss: 2.312458,  Acc: 0.413667\n",
            "Val Loss: 1.059861, Acc: 0.328333\n",
            "Test Loss: 1.101270, Acc: 0.358000\n",
            "Best_acc: 0.630333\n",
            "epoch[29], Train\n",
            "Train Loss: 2.310406,  Acc: 0.412250\n",
            "Val Loss: 1.056596, Acc: 0.329667\n",
            "Test Loss: 1.088028, Acc: 0.368667\n",
            "Best_acc: 0.630333\n",
            "epoch[30], Train\n",
            "Train Loss: 2.309902,  Acc: 0.416583\n",
            "Val Loss: 1.059839, Acc: 0.328667\n",
            "Test Loss: 1.086986, Acc: 0.374333\n",
            "Best_acc: 0.630333\n",
            "epoch[31], Train\n",
            "Train Loss: 2.307278,  Acc: 0.416333\n",
            "Val Loss: 1.060342, Acc: 0.323667\n",
            "Test Loss: 1.115502, Acc: 0.346000\n",
            "Best_acc: 0.630333\n",
            "epoch[32], Train\n",
            "Train Loss: 2.304884,  Acc: 0.417583\n",
            "Val Loss: 1.066688, Acc: 0.332000\n",
            "Test Loss: 1.072226, Acc: 0.381000\n",
            "Best_acc: 0.630333\n",
            "epoch[33], Train\n",
            "Train Loss: 2.304782,  Acc: 0.422167\n",
            "Val Loss: 1.060459, Acc: 0.334667\n",
            "Test Loss: 1.107641, Acc: 0.375333\n",
            "Best_acc: 0.630333\n",
            "epoch[34], Train\n",
            "Train Loss: 2.301933,  Acc: 0.427083\n",
            "Val Loss: 1.067401, Acc: 0.335333\n",
            "Test Loss: 1.108414, Acc: 0.373000\n",
            "Best_acc: 0.630333\n",
            "epoch[35], Train\n",
            "Train Loss: 2.298680,  Acc: 0.426083\n",
            "Val Loss: 1.065488, Acc: 0.337667\n",
            "Test Loss: 1.082460, Acc: 0.405000\n",
            "Best_acc: 0.630333\n",
            "epoch[36], Train\n",
            "Train Loss: 2.297567,  Acc: 0.431083\n",
            "Val Loss: 1.063120, Acc: 0.342667\n",
            "Test Loss: 1.083508, Acc: 0.418000\n",
            "Best_acc: 0.630333\n",
            "epoch[37], Train\n",
            "Train Loss: 2.294195,  Acc: 0.427667\n",
            "Val Loss: 1.066140, Acc: 0.331000\n",
            "Test Loss: 1.108472, Acc: 0.376333\n",
            "Best_acc: 0.630333\n",
            "epoch[38], Train\n",
            "Train Loss: 2.290655,  Acc: 0.438167\n",
            "Val Loss: 1.068226, Acc: 0.344000\n",
            "Test Loss: 1.096919, Acc: 0.397333\n",
            "Best_acc: 0.630333\n",
            "epoch[39], Train\n",
            "Train Loss: 2.288824,  Acc: 0.435750\n",
            "Val Loss: 1.071933, Acc: 0.346667\n",
            "Test Loss: 1.089347, Acc: 0.424000\n",
            "Best_acc: 0.630333\n",
            "epoch[40], Train\n",
            "Train Loss: 2.285284,  Acc: 0.439333\n",
            "Val Loss: 1.072270, Acc: 0.334667\n",
            "Test Loss: 1.140614, Acc: 0.362000\n",
            "Best_acc: 0.630333\n",
            "epoch[41], Train\n",
            "Train Loss: 2.274470,  Acc: 0.455250\n",
            "Val Loss: 1.069162, Acc: 0.338667\n",
            "Test Loss: 1.100168, Acc: 0.398333\n",
            "Best_acc: 0.630333\n",
            "epoch[42], Train\n",
            "Train Loss: 2.271989,  Acc: 0.454750\n",
            "Val Loss: 1.071808, Acc: 0.339000\n",
            "Test Loss: 1.100055, Acc: 0.398000\n",
            "Best_acc: 0.630333\n",
            "epoch[43], Train\n",
            "Train Loss: 2.270390,  Acc: 0.455417\n",
            "Val Loss: 1.073343, Acc: 0.340000\n",
            "Test Loss: 1.102357, Acc: 0.397667\n",
            "Best_acc: 0.630333\n",
            "epoch[44], Train\n",
            "Train Loss: 2.269193,  Acc: 0.453333\n",
            "Val Loss: 1.073288, Acc: 0.338333\n",
            "Test Loss: 1.103784, Acc: 0.396667\n",
            "Best_acc: 0.630333\n",
            "epoch[45], Train\n",
            "Train Loss: 2.267637,  Acc: 0.455583\n",
            "Val Loss: 1.073076, Acc: 0.339000\n",
            "Test Loss: 1.102402, Acc: 0.396333\n",
            "Best_acc: 0.630333\n",
            "epoch[46], Train\n",
            "Train Loss: 2.267877,  Acc: 0.457167\n",
            "Val Loss: 1.074967, Acc: 0.338667\n",
            "Test Loss: 1.103934, Acc: 0.398000\n",
            "Best_acc: 0.630333\n",
            " Early stopping at epoch 46 (best val_loss=1.050778, best acc=0.630333)\n",
            "Best_acc:  0.6303333333333333\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.117817\n",
            "Val Loss: 1.104342, Acc: 0.372667\n",
            "Test Loss: 1.052802, Acc: 0.508000\n",
            "Best_acc: 0.508000\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.099736\n",
            "Val Loss: 1.098546, Acc: 0.366000\n",
            "Test Loss: 1.067225, Acc: 0.517333\n",
            "Best_acc: 0.517333\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.100029\n",
            "Val Loss: 1.100793, Acc: 0.386667\n",
            "Test Loss: 1.055375, Acc: 0.572000\n",
            "Best_acc: 0.572000\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.098732\n",
            "Val Loss: 1.092826, Acc: 0.379000\n",
            "Test Loss: 1.073081, Acc: 0.529000\n",
            "Best_acc: 0.572000\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.096644\n",
            "Val Loss: 1.096052, Acc: 0.365667\n",
            "Test Loss: 1.076146, Acc: 0.499667\n",
            "Best_acc: 0.572000\n",
            "epoch[1], Train\n",
            "Train Loss: 2.409717,  Acc: 0.363417\n",
            "Val Loss: 1.097256, Acc: 0.382000\n",
            "Test Loss: 1.051869, Acc: 0.600667\n",
            "Best_acc: 0.600667\n",
            "epoch[2], Train\n",
            "Train Loss: 2.406570,  Acc: 0.370167\n",
            "Val Loss: 1.092691, Acc: 0.382333\n",
            "Test Loss: 1.064059, Acc: 0.554333\n",
            "Best_acc: 0.600667\n",
            "epoch[3], Train\n",
            "Train Loss: 2.406172,  Acc: 0.371083\n",
            "Val Loss: 1.092187, Acc: 0.375000\n",
            "Test Loss: 1.070091, Acc: 0.563333\n",
            "Best_acc: 0.600667\n",
            "epoch[4], Train\n",
            "Train Loss: 2.404868,  Acc: 0.369667\n",
            "Val Loss: 1.090396, Acc: 0.378333\n",
            "Test Loss: 1.044616, Acc: 0.588667\n",
            "Best_acc: 0.600667\n",
            "epoch[5], Train\n",
            "Train Loss: 2.400429,  Acc: 0.374333\n",
            "Val Loss: 1.100771, Acc: 0.366333\n",
            "Test Loss: 1.110752, Acc: 0.450000\n",
            "Best_acc: 0.600667\n",
            "epoch[6], Train\n",
            "Train Loss: 2.399460,  Acc: 0.371500\n",
            "Val Loss: 1.087498, Acc: 0.369667\n",
            "Test Loss: 1.061244, Acc: 0.494333\n",
            "Best_acc: 0.600667\n",
            "epoch[7], Train\n",
            "Train Loss: 2.387610,  Acc: 0.372250\n",
            "Val Loss: 1.075539, Acc: 0.342333\n",
            "Test Loss: 1.061735, Acc: 0.410667\n",
            "Best_acc: 0.600667\n",
            "epoch[8], Train\n",
            "Train Loss: 2.375759,  Acc: 0.365417\n",
            "Val Loss: 1.067345, Acc: 0.332667\n",
            "Test Loss: 1.066975, Acc: 0.372333\n",
            "Best_acc: 0.600667\n",
            "epoch[9], Train\n",
            "Train Loss: 2.363334,  Acc: 0.369417\n",
            "Val Loss: 1.060124, Acc: 0.324667\n",
            "Test Loss: 1.074379, Acc: 0.345000\n",
            "Best_acc: 0.600667\n",
            "epoch[10], Train\n",
            "Train Loss: 2.354429,  Acc: 0.371917\n",
            "Val Loss: 1.055375, Acc: 0.321333\n",
            "Test Loss: 1.105807, Acc: 0.333667\n",
            "Best_acc: 0.600667\n",
            "epoch[11], Train\n",
            "Train Loss: 2.347751,  Acc: 0.379583\n",
            "Val Loss: 1.053137, Acc: 0.322000\n",
            "Test Loss: 1.099191, Acc: 0.334000\n",
            "Best_acc: 0.600667\n",
            "epoch[12], Train\n",
            "Train Loss: 2.344674,  Acc: 0.383000\n",
            "Val Loss: 1.051745, Acc: 0.322000\n",
            "Test Loss: 1.092592, Acc: 0.333667\n",
            "Best_acc: 0.600667\n",
            "epoch[13], Train\n",
            "Train Loss: 2.341419,  Acc: 0.379083\n",
            "Val Loss: 1.052733, Acc: 0.327000\n",
            "Test Loss: 1.073675, Acc: 0.354333\n",
            "Best_acc: 0.600667\n",
            "epoch[14], Train\n",
            "Train Loss: 2.339936,  Acc: 0.382167\n",
            "Val Loss: 1.054372, Acc: 0.322333\n",
            "Test Loss: 1.093035, Acc: 0.335000\n",
            "Best_acc: 0.600667\n",
            "epoch[15], Train\n",
            "Train Loss: 2.337716,  Acc: 0.384333\n",
            "Val Loss: 1.053346, Acc: 0.322000\n",
            "Test Loss: 1.075035, Acc: 0.335000\n",
            "Best_acc: 0.600667\n",
            "epoch[16], Train\n",
            "Train Loss: 2.337461,  Acc: 0.383000\n",
            "Val Loss: 1.057659, Acc: 0.321667\n",
            "Test Loss: 1.134767, Acc: 0.333333\n",
            "Best_acc: 0.600667\n",
            "epoch[17], Train\n",
            "Train Loss: 2.335777,  Acc: 0.387583\n",
            "Val Loss: 1.050806, Acc: 0.322000\n",
            "Test Loss: 1.081455, Acc: 0.334000\n",
            "Best_acc: 0.600667\n",
            "epoch[18], Train\n",
            "Train Loss: 2.334999,  Acc: 0.384750\n",
            "Val Loss: 1.051050, Acc: 0.322333\n",
            "Test Loss: 1.072928, Acc: 0.335333\n",
            "Best_acc: 0.600667\n",
            "epoch[19], Train\n",
            "Train Loss: 2.332740,  Acc: 0.388500\n",
            "Val Loss: 1.055753, Acc: 0.324000\n",
            "Test Loss: 1.087836, Acc: 0.340000\n",
            "Best_acc: 0.600667\n",
            "epoch[20], Train\n",
            "Train Loss: 2.332607,  Acc: 0.386000\n",
            "Val Loss: 1.050284, Acc: 0.325667\n",
            "Test Loss: 1.073925, Acc: 0.346333\n",
            "Best_acc: 0.600667\n",
            "epoch[21], Train\n",
            "Train Loss: 2.324455,  Acc: 0.398833\n",
            "Val Loss: 1.054648, Acc: 0.322667\n",
            "Test Loss: 1.080588, Acc: 0.340667\n",
            "Best_acc: 0.600667\n",
            "epoch[22], Train\n",
            "Train Loss: 2.321794,  Acc: 0.401417\n",
            "Val Loss: 1.053049, Acc: 0.325333\n",
            "Test Loss: 1.068500, Acc: 0.351000\n",
            "Best_acc: 0.600667\n",
            "epoch[23], Train\n",
            "Train Loss: 2.320616,  Acc: 0.401750\n",
            "Val Loss: 1.054490, Acc: 0.326333\n",
            "Test Loss: 1.072513, Acc: 0.346667\n",
            "Best_acc: 0.600667\n",
            "epoch[24], Train\n",
            "Train Loss: 2.319693,  Acc: 0.401000\n",
            "Val Loss: 1.054350, Acc: 0.324000\n",
            "Test Loss: 1.065882, Acc: 0.349667\n",
            "Best_acc: 0.600667\n",
            "epoch[25], Train\n",
            "Train Loss: 2.317897,  Acc: 0.405500\n",
            "Val Loss: 1.056234, Acc: 0.325667\n",
            "Test Loss: 1.070935, Acc: 0.350667\n",
            "Best_acc: 0.600667\n",
            "epoch[26], Train\n",
            "Train Loss: 2.317868,  Acc: 0.409417\n",
            "Val Loss: 1.055559, Acc: 0.325667\n",
            "Test Loss: 1.063348, Acc: 0.355667\n",
            "Best_acc: 0.600667\n",
            "epoch[27], Train\n",
            "Train Loss: 2.315951,  Acc: 0.411000\n",
            "Val Loss: 1.056639, Acc: 0.327667\n",
            "Test Loss: 1.068694, Acc: 0.360667\n",
            "Best_acc: 0.600667\n",
            "epoch[28], Train\n",
            "Train Loss: 2.315667,  Acc: 0.412500\n",
            "Val Loss: 1.056575, Acc: 0.324000\n",
            "Test Loss: 1.066329, Acc: 0.354333\n",
            "Best_acc: 0.600667\n",
            "epoch[29], Train\n",
            "Train Loss: 2.314633,  Acc: 0.409667\n",
            "Val Loss: 1.057685, Acc: 0.328667\n",
            "Test Loss: 1.065942, Acc: 0.354000\n",
            "Best_acc: 0.600667\n",
            "epoch[30], Train\n",
            "Train Loss: 2.313312,  Acc: 0.415167\n",
            "Val Loss: 1.059037, Acc: 0.324333\n",
            "Test Loss: 1.054837, Acc: 0.360667\n",
            "Best_acc: 0.600667\n",
            "epoch[31], Train\n",
            "Train Loss: 2.312310,  Acc: 0.416000\n",
            "Val Loss: 1.056953, Acc: 0.325000\n",
            "Test Loss: 1.062248, Acc: 0.361333\n",
            "Best_acc: 0.600667\n",
            "epoch[32], Train\n",
            "Train Loss: 2.311632,  Acc: 0.414333\n",
            "Val Loss: 1.057907, Acc: 0.326333\n",
            "Test Loss: 1.071704, Acc: 0.353667\n",
            "Best_acc: 0.600667\n",
            "epoch[33], Train\n",
            "Train Loss: 2.309930,  Acc: 0.416583\n",
            "Val Loss: 1.059247, Acc: 0.325000\n",
            "Test Loss: 1.084871, Acc: 0.352667\n",
            "Best_acc: 0.600667\n",
            "epoch[34], Train\n",
            "Train Loss: 2.307610,  Acc: 0.416000\n",
            "Val Loss: 1.059772, Acc: 0.325333\n",
            "Test Loss: 1.084441, Acc: 0.348333\n",
            "Best_acc: 0.600667\n",
            "epoch[35], Train\n",
            "Train Loss: 2.307465,  Acc: 0.418750\n",
            "Val Loss: 1.060500, Acc: 0.324667\n",
            "Test Loss: 1.090231, Acc: 0.350333\n",
            "Best_acc: 0.600667\n",
            "epoch[36], Train\n",
            "Train Loss: 2.306130,  Acc: 0.420083\n",
            "Val Loss: 1.065695, Acc: 0.323667\n",
            "Test Loss: 1.071344, Acc: 0.352000\n",
            "Best_acc: 0.600667\n",
            "epoch[37], Train\n",
            "Train Loss: 2.304299,  Acc: 0.414083\n",
            "Val Loss: 1.062631, Acc: 0.327000\n",
            "Test Loss: 1.079482, Acc: 0.362333\n",
            "Best_acc: 0.600667\n",
            "epoch[38], Train\n",
            "Train Loss: 2.300889,  Acc: 0.426000\n",
            "Val Loss: 1.062087, Acc: 0.326667\n",
            "Test Loss: 1.062414, Acc: 0.361667\n",
            "Best_acc: 0.600667\n",
            "epoch[39], Train\n",
            "Train Loss: 2.300275,  Acc: 0.418667\n",
            "Val Loss: 1.064568, Acc: 0.324000\n",
            "Test Loss: 1.093727, Acc: 0.352333\n",
            "Best_acc: 0.600667\n",
            "epoch[40], Train\n",
            "Train Loss: 2.297817,  Acc: 0.425750\n",
            "Val Loss: 1.065163, Acc: 0.326333\n",
            "Test Loss: 1.096189, Acc: 0.359000\n",
            "Best_acc: 0.600667\n",
            "epoch[41], Train\n",
            "Train Loss: 2.289231,  Acc: 0.433667\n",
            "Val Loss: 1.063258, Acc: 0.329333\n",
            "Test Loss: 1.076991, Acc: 0.371000\n",
            "Best_acc: 0.600667\n",
            "epoch[42], Train\n",
            "Train Loss: 2.288326,  Acc: 0.434000\n",
            "Val Loss: 1.065475, Acc: 0.329667\n",
            "Test Loss: 1.080378, Acc: 0.366667\n",
            "Best_acc: 0.600667\n",
            "epoch[43], Train\n",
            "Train Loss: 2.285575,  Acc: 0.435000\n",
            "Val Loss: 1.065210, Acc: 0.333000\n",
            "Test Loss: 1.074935, Acc: 0.375000\n",
            "Best_acc: 0.600667\n",
            "epoch[44], Train\n",
            "Train Loss: 2.284224,  Acc: 0.438917\n",
            "Val Loss: 1.067024, Acc: 0.332000\n",
            "Test Loss: 1.080126, Acc: 0.373000\n",
            "Best_acc: 0.600667\n",
            "epoch[45], Train\n",
            "Train Loss: 2.284620,  Acc: 0.434333\n",
            "Val Loss: 1.066623, Acc: 0.333333\n",
            "Test Loss: 1.073363, Acc: 0.381667\n",
            "Best_acc: 0.600667\n",
            "epoch[46], Train\n",
            "Train Loss: 2.283727,  Acc: 0.437083\n",
            "Val Loss: 1.068446, Acc: 0.331000\n",
            "Test Loss: 1.081217, Acc: 0.371000\n",
            "Best_acc: 0.600667\n",
            "epoch[47], Train\n",
            "Train Loss: 2.283993,  Acc: 0.436917\n",
            "Val Loss: 1.068494, Acc: 0.335000\n",
            "Test Loss: 1.076378, Acc: 0.379333\n",
            "Best_acc: 0.600667\n",
            "epoch[48], Train\n",
            "Train Loss: 2.283049,  Acc: 0.434833\n",
            "Val Loss: 1.069026, Acc: 0.333333\n",
            "Test Loss: 1.078614, Acc: 0.379333\n",
            "Best_acc: 0.600667\n",
            "epoch[49], Train\n",
            "Train Loss: 2.280958,  Acc: 0.438500\n",
            "Val Loss: 1.069589, Acc: 0.335000\n",
            "Test Loss: 1.079665, Acc: 0.379333\n",
            "Best_acc: 0.600667\n",
            "epoch[50], Train\n",
            "Train Loss: 2.281143,  Acc: 0.437833\n",
            "Val Loss: 1.070415, Acc: 0.329667\n",
            "Test Loss: 1.086156, Acc: 0.375000\n",
            "Best_acc: 0.600667\n",
            " Early stopping at epoch 50 (best val_loss=1.050284, best acc=0.600667)\n",
            "Best_acc:  0.6006666666666667\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.118001\n",
            "Val Loss: 1.100245, Acc: 0.369333\n",
            "Test Loss: 1.049802, Acc: 0.548667\n",
            "Best_acc: 0.548667\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.100097\n",
            "Val Loss: 1.097501, Acc: 0.373667\n",
            "Test Loss: 1.049633, Acc: 0.557000\n",
            "Best_acc: 0.557000\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.098254\n",
            "Val Loss: 1.092006, Acc: 0.374667\n",
            "Test Loss: 1.067232, Acc: 0.556667\n",
            "Best_acc: 0.557000\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.096296\n",
            "Val Loss: 1.093254, Acc: 0.369667\n",
            "Test Loss: 1.070099, Acc: 0.599333\n",
            "Best_acc: 0.599333\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.096060\n",
            "Val Loss: 1.098662, Acc: 0.359333\n",
            "Test Loss: 1.053509, Acc: 0.518667\n",
            "Best_acc: 0.599333\n",
            "epoch[1], Train\n",
            "Train Loss: 2.409001,  Acc: 0.363667\n",
            "Val Loss: 1.094123, Acc: 0.367000\n",
            "Test Loss: 1.078711, Acc: 0.497000\n",
            "Best_acc: 0.497000\n",
            "epoch[2], Train\n",
            "Train Loss: 2.404960,  Acc: 0.374583\n",
            "Val Loss: 1.096684, Acc: 0.360333\n",
            "Test Loss: 1.045342, Acc: 0.539333\n",
            "Best_acc: 0.539333\n",
            "epoch[3], Train\n",
            "Train Loss: 2.403227,  Acc: 0.374750\n",
            "Val Loss: 1.091443, Acc: 0.382000\n",
            "Test Loss: 1.022245, Acc: 0.638000\n",
            "Best_acc: 0.638000\n",
            "epoch[4], Train\n",
            "Train Loss: 2.402502,  Acc: 0.376667\n",
            "Val Loss: 1.090120, Acc: 0.376667\n",
            "Test Loss: 1.059610, Acc: 0.578000\n",
            "Best_acc: 0.638000\n",
            "epoch[5], Train\n",
            "Train Loss: 2.400359,  Acc: 0.373417\n",
            "Val Loss: 1.093079, Acc: 0.365333\n",
            "Test Loss: 1.050349, Acc: 0.533667\n",
            "Best_acc: 0.638000\n",
            "epoch[6], Train\n",
            "Train Loss: 2.398276,  Acc: 0.375250\n",
            "Val Loss: 1.090832, Acc: 0.348667\n",
            "Test Loss: 1.065637, Acc: 0.458000\n",
            "Best_acc: 0.638000\n",
            "epoch[7], Train\n",
            "Train Loss: 2.391446,  Acc: 0.370167\n",
            "Val Loss: 1.080108, Acc: 0.354333\n",
            "Test Loss: 1.057789, Acc: 0.469000\n",
            "Best_acc: 0.638000\n",
            "epoch[8], Train\n",
            "Train Loss: 2.379083,  Acc: 0.373750\n",
            "Val Loss: 1.067073, Acc: 0.321333\n",
            "Test Loss: 1.101435, Acc: 0.334333\n",
            "Best_acc: 0.638000\n",
            "epoch[9], Train\n",
            "Train Loss: 2.365647,  Acc: 0.369250\n",
            "Val Loss: 1.062255, Acc: 0.322667\n",
            "Test Loss: 1.099713, Acc: 0.338333\n",
            "Best_acc: 0.638000\n",
            "epoch[10], Train\n",
            "Train Loss: 2.355097,  Acc: 0.377167\n",
            "Val Loss: 1.055107, Acc: 0.331333\n",
            "Test Loss: 1.075934, Acc: 0.370000\n",
            "Best_acc: 0.638000\n",
            "epoch[11], Train\n",
            "Train Loss: 2.348055,  Acc: 0.374500\n",
            "Val Loss: 1.059491, Acc: 0.323000\n",
            "Test Loss: 1.107561, Acc: 0.333667\n",
            "Best_acc: 0.638000\n",
            "epoch[12], Train\n",
            "Train Loss: 2.345501,  Acc: 0.382167\n",
            "Val Loss: 1.056495, Acc: 0.324000\n",
            "Test Loss: 1.077714, Acc: 0.337667\n",
            "Best_acc: 0.638000\n",
            "epoch[13], Train\n",
            "Train Loss: 2.343094,  Acc: 0.380500\n",
            "Val Loss: 1.054089, Acc: 0.321667\n",
            "Test Loss: 1.052880, Acc: 0.337333\n",
            "Best_acc: 0.638000\n",
            "epoch[14], Train\n",
            "Train Loss: 2.339467,  Acc: 0.385083\n",
            "Val Loss: 1.061264, Acc: 0.321667\n",
            "Test Loss: 1.107775, Acc: 0.334667\n",
            "Best_acc: 0.638000\n",
            "epoch[15], Train\n",
            "Train Loss: 2.338869,  Acc: 0.383250\n",
            "Val Loss: 1.058720, Acc: 0.321667\n",
            "Test Loss: 1.103886, Acc: 0.333333\n",
            "Best_acc: 0.638000\n",
            "epoch[16], Train\n",
            "Train Loss: 2.338379,  Acc: 0.384000\n",
            "Val Loss: 1.054897, Acc: 0.322333\n",
            "Test Loss: 1.100974, Acc: 0.338000\n",
            "Best_acc: 0.638000\n",
            "epoch[17], Train\n",
            "Train Loss: 2.336859,  Acc: 0.385250\n",
            "Val Loss: 1.055799, Acc: 0.322667\n",
            "Test Loss: 1.096862, Acc: 0.337000\n",
            "Best_acc: 0.638000\n",
            "epoch[18], Train\n",
            "Train Loss: 2.335301,  Acc: 0.387083\n",
            "Val Loss: 1.058856, Acc: 0.325000\n",
            "Test Loss: 1.090122, Acc: 0.347000\n",
            "Best_acc: 0.638000\n",
            "epoch[19], Train\n",
            "Train Loss: 2.334330,  Acc: 0.385667\n",
            "Val Loss: 1.052923, Acc: 0.322333\n",
            "Test Loss: 1.082152, Acc: 0.334667\n",
            "Best_acc: 0.638000\n",
            "epoch[20], Train\n",
            "Train Loss: 2.333551,  Acc: 0.389917\n",
            "Val Loss: 1.051173, Acc: 0.322333\n",
            "Test Loss: 1.087357, Acc: 0.334000\n",
            "Best_acc: 0.638000\n",
            "epoch[21], Train\n",
            "Train Loss: 2.327088,  Acc: 0.394750\n",
            "Val Loss: 1.051927, Acc: 0.322000\n",
            "Test Loss: 1.084012, Acc: 0.334333\n",
            "Best_acc: 0.638000\n",
            "epoch[22], Train\n",
            "Train Loss: 2.324892,  Acc: 0.396333\n",
            "Val Loss: 1.050784, Acc: 0.322667\n",
            "Test Loss: 1.075537, Acc: 0.337333\n",
            "Best_acc: 0.638000\n",
            "epoch[23], Train\n",
            "Train Loss: 2.323108,  Acc: 0.402417\n",
            "Val Loss: 1.052816, Acc: 0.322000\n",
            "Test Loss: 1.077496, Acc: 0.340000\n",
            "Best_acc: 0.638000\n",
            "epoch[24], Train\n",
            "Train Loss: 2.321582,  Acc: 0.402250\n",
            "Val Loss: 1.052085, Acc: 0.324000\n",
            "Test Loss: 1.071660, Acc: 0.342667\n",
            "Best_acc: 0.638000\n",
            "epoch[25], Train\n",
            "Train Loss: 2.320142,  Acc: 0.407500\n",
            "Val Loss: 1.052483, Acc: 0.326000\n",
            "Test Loss: 1.065340, Acc: 0.353000\n",
            "Best_acc: 0.638000\n",
            "epoch[26], Train\n",
            "Train Loss: 2.320082,  Acc: 0.403583\n",
            "Val Loss: 1.052615, Acc: 0.326000\n",
            "Test Loss: 1.073643, Acc: 0.345000\n",
            "Best_acc: 0.638000\n",
            "epoch[27], Train\n",
            "Train Loss: 2.318407,  Acc: 0.407417\n",
            "Val Loss: 1.051411, Acc: 0.328667\n",
            "Test Loss: 1.053615, Acc: 0.357333\n",
            "Best_acc: 0.638000\n",
            "epoch[28], Train\n",
            "Train Loss: 2.316812,  Acc: 0.406250\n",
            "Val Loss: 1.053665, Acc: 0.328000\n",
            "Test Loss: 1.063544, Acc: 0.355333\n",
            "Best_acc: 0.638000\n",
            "epoch[29], Train\n",
            "Train Loss: 2.315809,  Acc: 0.407917\n",
            "Val Loss: 1.054974, Acc: 0.329333\n",
            "Test Loss: 1.064255, Acc: 0.360667\n",
            "Best_acc: 0.638000\n",
            "epoch[30], Train\n",
            "Train Loss: 2.314285,  Acc: 0.405583\n",
            "Val Loss: 1.055886, Acc: 0.326667\n",
            "Test Loss: 1.066567, Acc: 0.356333\n",
            "Best_acc: 0.638000\n",
            "epoch[31], Train\n",
            "Train Loss: 2.313380,  Acc: 0.409250\n",
            "Val Loss: 1.057275, Acc: 0.327000\n",
            "Test Loss: 1.072799, Acc: 0.356000\n",
            "Best_acc: 0.638000\n",
            "epoch[32], Train\n",
            "Train Loss: 2.312403,  Acc: 0.411833\n",
            "Val Loss: 1.058930, Acc: 0.326667\n",
            "Test Loss: 1.069922, Acc: 0.359000\n",
            "Best_acc: 0.638000\n",
            "epoch[33], Train\n",
            "Train Loss: 2.310708,  Acc: 0.411500\n",
            "Val Loss: 1.055471, Acc: 0.325000\n",
            "Test Loss: 1.075150, Acc: 0.349667\n",
            "Best_acc: 0.638000\n",
            "epoch[34], Train\n",
            "Train Loss: 2.309219,  Acc: 0.412667\n",
            "Val Loss: 1.058697, Acc: 0.329667\n",
            "Test Loss: 1.066541, Acc: 0.371667\n",
            "Best_acc: 0.638000\n",
            "epoch[35], Train\n",
            "Train Loss: 2.307887,  Acc: 0.411667\n",
            "Val Loss: 1.058154, Acc: 0.327333\n",
            "Test Loss: 1.078204, Acc: 0.361333\n",
            "Best_acc: 0.638000\n",
            "epoch[36], Train\n",
            "Train Loss: 2.305566,  Acc: 0.418250\n",
            "Val Loss: 1.060128, Acc: 0.331333\n",
            "Test Loss: 1.064591, Acc: 0.371333\n",
            "Best_acc: 0.638000\n",
            "epoch[37], Train\n",
            "Train Loss: 2.303281,  Acc: 0.423917\n",
            "Val Loss: 1.068471, Acc: 0.330333\n",
            "Test Loss: 1.111758, Acc: 0.360667\n",
            "Best_acc: 0.638000\n",
            "epoch[38], Train\n",
            "Train Loss: 2.302268,  Acc: 0.419417\n",
            "Val Loss: 1.065431, Acc: 0.326333\n",
            "Test Loss: 1.085978, Acc: 0.354000\n",
            "Best_acc: 0.638000\n",
            "epoch[39], Train\n",
            "Train Loss: 2.297786,  Acc: 0.426417\n",
            "Val Loss: 1.066574, Acc: 0.332000\n",
            "Test Loss: 1.069227, Acc: 0.379333\n",
            "Best_acc: 0.638000\n",
            "epoch[40], Train\n",
            "Train Loss: 2.298570,  Acc: 0.426667\n",
            "Val Loss: 1.062203, Acc: 0.332333\n",
            "Test Loss: 1.058901, Acc: 0.389333\n",
            "Best_acc: 0.638000\n",
            "epoch[41], Train\n",
            "Train Loss: 2.290196,  Acc: 0.430750\n",
            "Val Loss: 1.063590, Acc: 0.330667\n",
            "Test Loss: 1.076322, Acc: 0.374000\n",
            "Best_acc: 0.638000\n",
            "epoch[42], Train\n",
            "Train Loss: 2.286232,  Acc: 0.437083\n",
            "Val Loss: 1.062533, Acc: 0.329000\n",
            "Test Loss: 1.074341, Acc: 0.377667\n",
            "Best_acc: 0.638000\n",
            "epoch[43], Train\n",
            "Train Loss: 2.283532,  Acc: 0.438083\n",
            "Val Loss: 1.064992, Acc: 0.326667\n",
            "Test Loss: 1.075970, Acc: 0.374000\n",
            "Best_acc: 0.638000\n",
            "epoch[44], Train\n",
            "Train Loss: 2.283895,  Acc: 0.439083\n",
            "Val Loss: 1.065462, Acc: 0.328667\n",
            "Test Loss: 1.082656, Acc: 0.371667\n",
            "Best_acc: 0.638000\n",
            "epoch[45], Train\n",
            "Train Loss: 2.283646,  Acc: 0.436667\n",
            "Val Loss: 1.065441, Acc: 0.331333\n",
            "Test Loss: 1.075982, Acc: 0.377333\n",
            "Best_acc: 0.638000\n",
            "epoch[46], Train\n",
            "Train Loss: 2.281426,  Acc: 0.437917\n",
            "Val Loss: 1.067095, Acc: 0.330667\n",
            "Test Loss: 1.078792, Acc: 0.376667\n",
            "Best_acc: 0.638000\n",
            "epoch[47], Train\n",
            "Train Loss: 2.281573,  Acc: 0.443167\n",
            "Val Loss: 1.064171, Acc: 0.335333\n",
            "Test Loss: 1.068830, Acc: 0.386667\n",
            "Best_acc: 0.638000\n",
            "epoch[48], Train\n",
            "Train Loss: 2.281628,  Acc: 0.441000\n",
            "Val Loss: 1.067134, Acc: 0.332000\n",
            "Test Loss: 1.078842, Acc: 0.376333\n",
            "Best_acc: 0.638000\n",
            "epoch[49], Train\n",
            "Train Loss: 2.280229,  Acc: 0.443583\n",
            "Val Loss: 1.065695, Acc: 0.333000\n",
            "Test Loss: 1.079643, Acc: 0.377333\n",
            "Best_acc: 0.638000\n",
            "epoch[50], Train\n",
            "Train Loss: 2.279081,  Acc: 0.443167\n",
            "Val Loss: 1.066757, Acc: 0.332333\n",
            "Test Loss: 1.075257, Acc: 0.380667\n",
            "Best_acc: 0.638000\n",
            "epoch[51], Train\n",
            "Train Loss: 2.279849,  Acc: 0.439000\n",
            "Val Loss: 1.068447, Acc: 0.334667\n",
            "Test Loss: 1.080662, Acc: 0.377667\n",
            "Best_acc: 0.638000\n",
            "epoch[52], Train\n",
            "Train Loss: 2.279308,  Acc: 0.440500\n",
            "Val Loss: 1.065818, Acc: 0.336333\n",
            "Test Loss: 1.074965, Acc: 0.383000\n",
            "Best_acc: 0.638000\n",
            " Early stopping at epoch 52 (best val_loss=1.050784, best acc=0.638000)\n",
            "Best_acc:  0.638\n",
            "epoch[1], Warmup\n",
            "Warmup Loss: 1.117793\n",
            "Val Loss: 1.099083, Acc: 0.361000\n",
            "Test Loss: 1.086200, Acc: 0.514000\n",
            "Best_acc: 0.514000\n",
            "epoch[2], Warmup\n",
            "Warmup Loss: 1.100286\n",
            "Val Loss: 1.095829, Acc: 0.370667\n",
            "Test Loss: 1.071853, Acc: 0.480000\n",
            "Best_acc: 0.514000\n",
            "epoch[3], Warmup\n",
            "Warmup Loss: 1.096534\n",
            "Val Loss: 1.094052, Acc: 0.376000\n",
            "Test Loss: 1.065782, Acc: 0.602667\n",
            "Best_acc: 0.602667\n",
            "epoch[4], Warmup\n",
            "Warmup Loss: 1.096145\n",
            "Val Loss: 1.091642, Acc: 0.380333\n",
            "Test Loss: 1.079375, Acc: 0.500333\n",
            "Best_acc: 0.602667\n",
            "epoch[5], Warmup\n",
            "Warmup Loss: 1.095310\n",
            "Val Loss: 1.091708, Acc: 0.382000\n",
            "Test Loss: 1.062776, Acc: 0.528667\n",
            "Best_acc: 0.602667\n",
            "epoch[1], Train\n",
            "Train Loss: 2.407164,  Acc: 0.370250\n",
            "Val Loss: 1.092345, Acc: 0.379000\n",
            "Test Loss: 1.039829, Acc: 0.667000\n",
            "Best_acc: 0.667000\n",
            "epoch[2], Train\n",
            "Train Loss: 2.404583,  Acc: 0.375500\n",
            "Val Loss: 1.093040, Acc: 0.367667\n",
            "Test Loss: 1.063629, Acc: 0.585667\n",
            "Best_acc: 0.667000\n",
            "epoch[3], Train\n",
            "Train Loss: 2.403905,  Acc: 0.371667\n",
            "Val Loss: 1.093208, Acc: 0.375333\n",
            "Test Loss: 1.048039, Acc: 0.591000\n",
            "Best_acc: 0.667000\n",
            "epoch[4], Train\n",
            "Train Loss: 2.402282,  Acc: 0.369917\n",
            "Val Loss: 1.091603, Acc: 0.375667\n",
            "Test Loss: 1.065597, Acc: 0.567667\n",
            "Best_acc: 0.667000\n",
            "epoch[5], Train\n",
            "Train Loss: 2.399197,  Acc: 0.379167\n",
            "Val Loss: 1.089876, Acc: 0.380000\n",
            "Test Loss: 1.057553, Acc: 0.573333\n",
            "Best_acc: 0.667000\n",
            "epoch[6], Train\n",
            "Train Loss: 2.393886,  Acc: 0.378833\n",
            "Val Loss: 1.087694, Acc: 0.364000\n",
            "Test Loss: 1.059437, Acc: 0.485000\n",
            "Best_acc: 0.667000\n",
            "epoch[7], Train\n",
            "Train Loss: 2.386061,  Acc: 0.373583\n",
            "Val Loss: 1.075245, Acc: 0.351667\n",
            "Test Loss: 1.053261, Acc: 0.434333\n",
            "Best_acc: 0.667000\n",
            "epoch[8], Train\n",
            "Train Loss: 2.375081,  Acc: 0.370583\n",
            "Val Loss: 1.066075, Acc: 0.342000\n",
            "Test Loss: 1.057938, Acc: 0.414000\n",
            "Best_acc: 0.667000\n",
            "epoch[9], Train\n",
            "Train Loss: 2.361693,  Acc: 0.372333\n",
            "Val Loss: 1.061591, Acc: 0.333000\n",
            "Test Loss: 1.079899, Acc: 0.369000\n",
            "Best_acc: 0.667000\n",
            "epoch[10], Train\n",
            "Train Loss: 2.353254,  Acc: 0.375917\n",
            "Val Loss: 1.062259, Acc: 0.324667\n",
            "Test Loss: 1.067547, Acc: 0.363333\n",
            "Best_acc: 0.667000\n",
            "epoch[11], Train\n",
            "Train Loss: 2.349184,  Acc: 0.373417\n",
            "Val Loss: 1.053947, Acc: 0.323333\n",
            "Test Loss: 1.078225, Acc: 0.346667\n",
            "Best_acc: 0.667000\n",
            "epoch[12], Train\n",
            "Train Loss: 2.345256,  Acc: 0.379333\n",
            "Val Loss: 1.057327, Acc: 0.323000\n",
            "Test Loss: 1.078207, Acc: 0.342667\n",
            "Best_acc: 0.667000\n",
            "epoch[13], Train\n",
            "Train Loss: 2.342031,  Acc: 0.382917\n",
            "Val Loss: 1.053621, Acc: 0.323000\n",
            "Test Loss: 1.067236, Acc: 0.363000\n",
            "Best_acc: 0.667000\n",
            "epoch[14], Train\n",
            "Train Loss: 2.342179,  Acc: 0.380583\n",
            "Val Loss: 1.058141, Acc: 0.321000\n",
            "Test Loss: 1.106255, Acc: 0.333667\n",
            "Best_acc: 0.667000\n",
            "epoch[15], Train\n",
            "Train Loss: 2.339970,  Acc: 0.379333\n",
            "Val Loss: 1.053057, Acc: 0.321000\n",
            "Test Loss: 1.096195, Acc: 0.334000\n",
            "Best_acc: 0.667000\n",
            "epoch[16], Train\n",
            "Train Loss: 2.335411,  Acc: 0.387333\n",
            "Val Loss: 1.054325, Acc: 0.321667\n",
            "Test Loss: 1.079441, Acc: 0.336667\n",
            "Best_acc: 0.667000\n",
            "epoch[17], Train\n",
            "Train Loss: 2.335756,  Acc: 0.383667\n",
            "Val Loss: 1.061156, Acc: 0.325667\n",
            "Test Loss: 1.140886, Acc: 0.351667\n",
            "Best_acc: 0.667000\n",
            "epoch[18], Train\n",
            "Train Loss: 2.333476,  Acc: 0.385167\n",
            "Val Loss: 1.049188, Acc: 0.321000\n",
            "Test Loss: 1.094995, Acc: 0.334333\n",
            "Best_acc: 0.667000\n",
            "epoch[19], Train\n",
            "Train Loss: 2.332233,  Acc: 0.383250\n",
            "Val Loss: 1.051332, Acc: 0.325000\n",
            "Test Loss: 1.070961, Acc: 0.348333\n",
            "Best_acc: 0.667000\n",
            "epoch[20], Train\n",
            "Train Loss: 2.331426,  Acc: 0.388583\n",
            "Val Loss: 1.051685, Acc: 0.321333\n",
            "Test Loss: 1.110754, Acc: 0.334667\n",
            "Best_acc: 0.667000\n",
            "epoch[21], Train\n",
            "Train Loss: 2.326077,  Acc: 0.396917\n",
            "Val Loss: 1.050631, Acc: 0.321333\n",
            "Test Loss: 1.096548, Acc: 0.338333\n",
            "Best_acc: 0.667000\n",
            "epoch[22], Train\n",
            "Train Loss: 2.323229,  Acc: 0.396417\n",
            "Val Loss: 1.051741, Acc: 0.328000\n",
            "Test Loss: 1.091034, Acc: 0.351000\n",
            "Best_acc: 0.667000\n",
            "epoch[23], Train\n",
            "Train Loss: 2.321999,  Acc: 0.399917\n",
            "Val Loss: 1.054648, Acc: 0.321333\n",
            "Test Loss: 1.101813, Acc: 0.341667\n",
            "Best_acc: 0.667000\n",
            "epoch[24], Train\n",
            "Train Loss: 2.320726,  Acc: 0.404500\n",
            "Val Loss: 1.052386, Acc: 0.332333\n",
            "Test Loss: 1.084357, Acc: 0.360667\n",
            "Best_acc: 0.667000\n",
            "epoch[25], Train\n",
            "Train Loss: 2.319953,  Acc: 0.402917\n",
            "Val Loss: 1.054060, Acc: 0.329667\n",
            "Test Loss: 1.085719, Acc: 0.361333\n",
            "Best_acc: 0.667000\n",
            "epoch[26], Train\n",
            "Train Loss: 2.318403,  Acc: 0.406000\n",
            "Val Loss: 1.054236, Acc: 0.333333\n",
            "Test Loss: 1.084224, Acc: 0.369667\n",
            "Best_acc: 0.667000\n",
            "epoch[27], Train\n",
            "Train Loss: 2.317426,  Acc: 0.406167\n",
            "Val Loss: 1.055326, Acc: 0.321667\n",
            "Test Loss: 1.105554, Acc: 0.343667\n",
            "Best_acc: 0.667000\n",
            "epoch[28], Train\n",
            "Train Loss: 2.316382,  Acc: 0.409833\n",
            "Val Loss: 1.055226, Acc: 0.333333\n",
            "Test Loss: 1.083646, Acc: 0.366000\n",
            "Best_acc: 0.667000\n",
            "epoch[29], Train\n",
            "Train Loss: 2.314050,  Acc: 0.411083\n",
            "Val Loss: 1.059116, Acc: 0.329333\n",
            "Test Loss: 1.091723, Acc: 0.355667\n",
            "Best_acc: 0.667000\n",
            "epoch[30], Train\n",
            "Train Loss: 2.313332,  Acc: 0.412917\n",
            "Val Loss: 1.059803, Acc: 0.331000\n",
            "Test Loss: 1.077780, Acc: 0.367000\n",
            "Best_acc: 0.667000\n",
            "epoch[31], Train\n",
            "Train Loss: 2.311887,  Acc: 0.412750\n",
            "Val Loss: 1.057098, Acc: 0.330333\n",
            "Test Loss: 1.096210, Acc: 0.360333\n",
            "Best_acc: 0.667000\n",
            "epoch[32], Train\n",
            "Train Loss: 2.310823,  Acc: 0.413500\n",
            "Val Loss: 1.058270, Acc: 0.340667\n",
            "Test Loss: 1.066910, Acc: 0.397000\n",
            "Best_acc: 0.667000\n",
            "epoch[33], Train\n",
            "Train Loss: 2.308739,  Acc: 0.419500\n",
            "Val Loss: 1.062045, Acc: 0.332333\n",
            "Test Loss: 1.111002, Acc: 0.360667\n",
            "Best_acc: 0.667000\n",
            "epoch[34], Train\n",
            "Train Loss: 2.307924,  Acc: 0.415833\n",
            "Val Loss: 1.059472, Acc: 0.337667\n",
            "Test Loss: 1.091834, Acc: 0.382333\n",
            "Best_acc: 0.667000\n",
            "epoch[35], Train\n",
            "Train Loss: 2.305200,  Acc: 0.425167\n",
            "Val Loss: 1.061522, Acc: 0.335333\n",
            "Test Loss: 1.128180, Acc: 0.361000\n",
            "Best_acc: 0.667000\n",
            "epoch[36], Train\n",
            "Train Loss: 2.304141,  Acc: 0.423500\n",
            "Val Loss: 1.061090, Acc: 0.326333\n",
            "Test Loss: 1.111678, Acc: 0.356333\n",
            "Best_acc: 0.667000\n",
            "epoch[37], Train\n",
            "Train Loss: 2.302638,  Acc: 0.422333\n",
            "Val Loss: 1.065610, Acc: 0.341333\n",
            "Test Loss: 1.079406, Acc: 0.381000\n",
            "Best_acc: 0.667000\n",
            "epoch[38], Train\n",
            "Train Loss: 2.300329,  Acc: 0.425917\n",
            "Val Loss: 1.061370, Acc: 0.335333\n",
            "Test Loss: 1.086674, Acc: 0.375000\n",
            "Best_acc: 0.667000\n",
            "epoch[39], Train\n",
            "Train Loss: 2.298893,  Acc: 0.427500\n",
            "Val Loss: 1.070096, Acc: 0.350667\n",
            "Test Loss: 1.102288, Acc: 0.416667\n",
            "Best_acc: 0.667000\n",
            "epoch[40], Train\n",
            "Train Loss: 2.295052,  Acc: 0.433583\n",
            "Val Loss: 1.069825, Acc: 0.347333\n",
            "Test Loss: 1.078376, Acc: 0.401000\n",
            "Best_acc: 0.667000\n",
            "epoch[41], Train\n",
            "Train Loss: 2.286675,  Acc: 0.433917\n",
            "Val Loss: 1.066782, Acc: 0.339667\n",
            "Test Loss: 1.093798, Acc: 0.391333\n",
            "Best_acc: 0.667000\n",
            "epoch[42], Train\n",
            "Train Loss: 2.284992,  Acc: 0.443250\n",
            "Val Loss: 1.066982, Acc: 0.341333\n",
            "Test Loss: 1.094014, Acc: 0.390000\n",
            "Best_acc: 0.667000\n",
            "epoch[43], Train\n",
            "Train Loss: 2.283494,  Acc: 0.443917\n",
            "Val Loss: 1.067509, Acc: 0.339667\n",
            "Test Loss: 1.099030, Acc: 0.384333\n",
            "Best_acc: 0.667000\n",
            "epoch[44], Train\n",
            "Train Loss: 2.283474,  Acc: 0.444667\n",
            "Val Loss: 1.068476, Acc: 0.338333\n",
            "Test Loss: 1.099396, Acc: 0.384000\n",
            "Best_acc: 0.667000\n",
            "epoch[45], Train\n",
            "Train Loss: 2.283013,  Acc: 0.445667\n",
            "Val Loss: 1.068831, Acc: 0.337333\n",
            "Test Loss: 1.101578, Acc: 0.383333\n",
            "Best_acc: 0.667000\n",
            "epoch[46], Train\n",
            "Train Loss: 2.280867,  Acc: 0.448500\n",
            "Val Loss: 1.069522, Acc: 0.337667\n",
            "Test Loss: 1.102547, Acc: 0.383333\n",
            "Best_acc: 0.667000\n",
            "epoch[47], Train\n",
            "Train Loss: 2.280218,  Acc: 0.447417\n",
            "Val Loss: 1.069881, Acc: 0.336667\n",
            "Test Loss: 1.106322, Acc: 0.384000\n",
            "Best_acc: 0.667000\n",
            "epoch[48], Train\n",
            "Train Loss: 2.280427,  Acc: 0.450833\n",
            "Val Loss: 1.070167, Acc: 0.336000\n",
            "Test Loss: 1.104322, Acc: 0.383667\n",
            "Best_acc: 0.667000\n",
            " Early stopping at epoch 48 (best val_loss=1.049188, best acc=0.667000)\n",
            "Best_acc:  0.667\n"
          ]
        }
      ],
      "execution_count": 73
    },
    {
      "cell_type": "code",
      "source": [
        "x_label = list(range(len(best_list)))\n",
        "plt.plot(x_label, best_list)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "tyex7-nTNOlx",
        "outputId": "7b67d9eb-3afe-4638-e03a-99c66739ac3b"
      },
      "id": "tyex7-nTNOlx",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "id": "ab69b8d668a12854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d487974b-7c49-4f71-c205-4598dba3b7fa"
      },
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Forward T (row-stoch) ===\")\n",
        "print(np.array_str(Tf, precision=3, suppress_small=True, max_line_width=180))\n",
        "print(\"row sums:\", np.round(Tf.sum(axis=1), 6))\n",
        "\n",
        "print(\"\\n=== Backward T (row-stoch) ===\")\n",
        "print(np.array_str(Tb, precision=3, suppress_small=True, max_line_width=180))\n",
        "print(\"row sums:\", np.round(Tb.sum(axis=1), 6))\n"
      ],
      "id": "ab69b8d668a12854",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Forward T (row-stoch) ===\n",
            "[[0.984 0.01  0.006]\n",
            " [0.126 0.866 0.008]\n",
            " [0.181 0.116 0.703]]\n",
            "row sums: [1. 1. 1.]\n",
            "\n",
            "=== Backward T (row-stoch) ===\n",
            "[[0.63  0.184 0.187]\n",
            " [0.    0.773 0.227]\n",
            " [0.    0.001 0.999]]\n",
            "row sums: [1. 1. 1.]\n"
          ]
        }
      ],
      "execution_count": 72
    },
    {
      "metadata": {
        "id": "489830f2d82f3ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ab5e2b8-1224-463a-9a0b-8b82b97a7535"
      },
      "cell_type": "code",
      "source": [
        "#  CSV\n",
        "with open('summary_results.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['name', 'mean_best', 'std_best', 'Tf_path', 'Tb_path'])\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(\"  summary_results.csv  saved/ \")"
      ],
      "id": "489830f2d82f3ce",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  summary_results.csv  saved/ \n"
          ]
        }
      ],
      "execution_count": 82
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3i-vU6v1p4S",
        "outputId": "c385f7e9-9f13-4186-fdd7-d9d9f56d6ab8"
      },
      "id": "p3i-vU6v1p4S",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['mnist03',\n",
              "  np.float64(0.9851666666666666),\n",
              "  np.float64(0.0017779513804126303),\n",
              "  'saved/mnist03_Tf.npy',\n",
              "  'saved/mnist03_Tb.npy'],\n",
              " ['mnist06',\n",
              "  np.float64(0.8217333333333334),\n",
              "  np.float64(0.14108144220035937),\n",
              "  'saved/mnist06_Tf.npy',\n",
              "  'saved/mnist06_Tb.npy'],\n",
              " ['cifar',\n",
              "  np.float64(0.6370666666666667),\n",
              "  np.float64(0.02978620113035192),\n",
              "  'saved/cifar_Tf.npy',\n",
              "  'saved/cifar_Tb.npy']]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hoKfRXqBLP5S"
      },
      "id": "hoKfRXqBLP5S",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}